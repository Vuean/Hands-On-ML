# 第五章 支持向量机

支持向量机（Support Vector Machine，SVM）是一个功能强大并且全面的机器学习模型，它能够执行线性或非线性分类、回归，甚至是异常值检测任务。它是机器学习领域最受欢迎的模型之一，任何对机器学习感兴趣的人都应该在工具箱中配备一个。SVM特别适用于中小型复杂数据集的分类。

本章将会介绍不同SVM的核心概念、如何使用它们以及它们的工作原理。

## 5.1 线性SVM分类

SVM的基本思想可以用一些图来说明。下图所示的数据集来自第4章末尾引用的鸢尾花数据集的一部分。两个类可以轻松地被一条直线（它们是线性可分离的）分开。左图显示了三种可能的线性分类器的决策边界。其中虚线所代表的模型表现非常糟糕，甚至都无法正确实现分类。其余两个模型在这个训练集上表现堪称完美，但是它们的决策边界与实例过于接近，导致在面对新实例时，表现可能不会太好。相比之下，右图中的实线代表SVM分类器的决策边界，这条线不仅分离了两个类，并且尽可能远离了最近的训练实例。你可以将SVM分类器视为**在类之间拟合可能的最宽的街道（平行的虚线所示）**。因此这也叫作**大间隔分类**。

![fig01_大间隔分类]()

请注意，在“街道以外”的地方增加更多训练实例不会对决策边界产生影响，也就是说它完全由位于街道边缘的实例所决定（或者称之为“支持”）。这些实例被称为**支持向量**（在上图中已圈出）。

SVM对特征的缩放非常敏感，如下图所示，在左图中，垂直刻度比水平刻度大得多，因此可能的最宽的街道接近于水平。在特征缩放（例如使用Scikit-Learn的`StandardScaler`）后，决策边界看起来好了很多（见右图）。

![fig02_特征缩放敏感性]()

### 5.1.1 软间隔分类

如果我们严格地让所有实例都不在街道上，并且位于正确的一边，这就是**硬间隔分类**。硬间隔分类有两个主要问题。**首先**，它只在数据是线性可分离的时候才有效；**其次**，它对异常值非常敏感。下图显示了有一个额外异常值的鸢尾花数据：左图的数据根本找不出硬间隔，而右图最终显示的决策边界与我们在图1中所看到的无异常值时的决策边界也大不相同，可能无法很好地泛化。

![fig03_硬间隔对异常值的敏感度]()

要避免这些问题，最好使用更灵活的模型。目标是尽可能在保持街道宽阔和限制间隔违例（即位于街道之上，甚至在错误的一边的实例）之间找到良好的平衡，这就是**软间隔分类**。

使用Scikit-Learn创建SVM模型时，我们可以指定许多超参数。*C* 是这些超参数之一。如果将其设置为较低的值，则最终得到图4左侧的模型。如果设置为较高的值，我们得到右边的模型。间隔冲突很糟糕，通常最好要少一些。但是，在这种情况下，左侧的模型存在很多间隔违例的情况，但泛化效果可能会更好。

![fig04_大间隔与更少的间隔冲突]()

如果你的SVM模型过拟合，可以尝试通过降低*C* 来对其进行正则化。

以下Scikit-Learn代码可加载鸢尾花数据集，缩放特征，然后训练线性SVM模型（使用`C=1`的`LinearSVC`类和稍后描述的hinge损失函数）来检测维吉尼亚鸢尾花：

```python

```

**与Logistic回归分类器不同，SVM分类器不会输出每个类的概率**。

我们可以将SVC类与线性内核一起使用，而不使用LinearSVC类。创建SVC模型时，我们可以编写`SVC(kernel="linear"，C=1)`。或者我们可以将`SGDClassifier`类与`SGDClassifier(loss="hinge"，alpha=1/(m*C))`一起使用。这将使用常规的随机梯度下降（见第4章）来训练线性SVM分类器。它的收敛速度不如LinearSVC类，但是对处理在线分类任务或不适合内存的庞大数据集（核外训练）很有用。

`LinearSVC`类会对偏置项进行正则化，所以你需要先减去平均值，使训练集居中。如果使用`StandardScaler`会自动进行这一步。此外，请确保超参数`loss`设置为`"hinge"`，因为它不是默认值。最后，为了获得更好的性能，还应该将超参数`dual`设置为`False`，除非特征数量比训练实例还多（本章后文将会讨论）。

## 5.2 非线性SVM分类

