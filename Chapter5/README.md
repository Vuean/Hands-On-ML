# 第五章 支持向量机

支持向量机（Support Vector Machine，SVM）是一个功能强大并且全面的机器学习模型，它能够执行线性或非线性分类、回归，甚至是异常值检测任务。它是机器学习领域最受欢迎的模型之一，任何对机器学习感兴趣的人都应该在工具箱中配备一个。SVM特别适用于中小型复杂数据集的分类。

本章将会介绍不同SVM的核心概念、如何使用它们以及它们的工作原理。

## 5.1 线性SVM分类

SVM的基本思想可以用一些图来说明。下图所示的数据集来自第4章末尾引用的鸢尾花数据集的一部分。两个类可以轻松地被一条直线（它们是线性可分离的）分开。左图显示了三种可能的线性分类器的决策边界。其中虚线所代表的模型表现非常糟糕，甚至都无法正确实现分类。其余两个模型在这个训练集上表现堪称完美，但是它们的决策边界与实例过于接近，导致在面对新实例时，表现可能不会太好。相比之下，右图中的实线代表SVM分类器的决策边界，这条线不仅分离了两个类，并且尽可能远离了最近的训练实例。你可以将SVM分类器视为**在类之间拟合可能的最宽的街道（平行的虚线所示）**。因此这也叫作**大间隔分类**。

![fig01_大间隔分类](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter5/fig01_%E5%A4%A7%E9%97%B4%E9%9A%94%E5%88%86%E7%B1%BB.jpg)

请注意，在“街道以外”的地方增加更多训练实例不会对决策边界产生影响，也就是说它完全由位于街道边缘的实例所决定（或者称之为“支持”）。这些实例被称为**支持向量**（在上图中已圈出）。

SVM对特征的缩放非常敏感，如下图所示，在左图中，垂直刻度比水平刻度大得多，因此可能的最宽的街道接近于水平。在特征缩放（例如使用Scikit-Learn的`StandardScaler`）后，决策边界看起来好了很多（见右图）。

![fig02_特征缩放敏感性](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter5/fig02_%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%E6%95%8F%E6%84%9F%E6%80%A7.jpg)

### 5.1.1 软间隔分类

如果我们严格地让所有实例都不在街道上，并且位于正确的一边，这就是**硬间隔分类**。硬间隔分类有两个主要问题。**首先**，它只在数据是线性可分离的时候才有效；**其次**，它对异常值非常敏感。下图显示了有一个额外异常值的鸢尾花数据：左图的数据根本找不出硬间隔，而右图最终显示的决策边界与我们在图1中所看到的无异常值时的决策边界也大不相同，可能无法很好地泛化。

![fig03_硬间隔对异常值的敏感度](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter5/fig03_%E7%A1%AC%E9%97%B4%E9%9A%94%E5%AF%B9%E5%BC%82%E5%B8%B8%E5%80%BC%E7%9A%84%E6%95%8F%E6%84%9F%E5%BA%A6.jpg)

要避免这些问题，最好使用更灵活的模型。目标是尽可能在保持街道宽阔和限制间隔违例（即位于街道之上，甚至在错误的一边的实例）之间找到良好的平衡，这就是**软间隔分类**。

使用Scikit-Learn创建SVM模型时，我们可以指定许多超参数。*C* 是这些超参数之一。如果将其设置为较低的值，则最终得到图4左侧的模型。如果设置为较高的值，我们得到右边的模型。间隔冲突很糟糕，通常最好要少一些。但是，在这种情况下，左侧的模型存在很多间隔违例的情况，但泛化效果可能会更好。

![fig04_大间隔与更少的间隔冲突](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter5/fig04_%E5%A4%A7%E9%97%B4%E9%9A%94%E4%B8%8E%E6%9B%B4%E5%B0%91%E7%9A%84%E9%97%B4%E9%9A%94%E5%86%B2%E7%AA%81.jpg)

如果你的SVM模型过拟合，可以尝试通过降低*C* 来对其进行正则化。

以下Scikit-Learn代码可加载鸢尾花数据集，缩放特征，然后训练线性SVM模型（使用`C=1`的`LinearSVC`类和稍后描述的hinge损失函数）来检测维吉尼亚鸢尾花：

```python
    import numpy as np
    from sklearn import datasets
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.svm import LinearSVC

    iris = datasets.load_iris()
    X = iris["data"][:, (2, 3)]  # petal length, petal width
    y = (iris["target"] == 2).astype(np.float64)  # Iris virginica

    svm_clf = Pipeline([
            ("scaler", StandardScaler()),
            ("linear_svc", LinearSVC(C=1, loss="hinge", random_state=42)),
        ])

    svm_clf.fit(X, y)
```

**与Logistic回归分类器不同，SVM分类器不会输出每个类的概率**。

我们可以将SVC类与线性内核一起使用，而不使用LinearSVC类。创建SVC模型时，我们可以编写`SVC(kernel="linear"，C=1)`。或者我们可以将`SGDClassifier`类与`SGDClassifier(loss="hinge"，alpha=1/(m*C))`一起使用。这将使用常规的随机梯度下降（见第4章）来训练线性SVM分类器。它的收敛速度不如LinearSVC类，但是对处理在线分类任务或不适合内存的庞大数据集（核外训练）很有用。

`LinearSVC`类会对偏置项进行正则化，所以你需要先减去平均值，使训练集居中。如果使用`StandardScaler`会自动进行这一步。此外，请确保超参数`loss`设置为`"hinge"`，因为它不是默认值。最后，为了获得更好的性能，还应该将超参数`dual`设置为`False`，除非特征数量比训练实例还多（本章后文将会讨论）。

## 5.2 非线性SVM分类

虽然在许多情况下，线性SVM分类器是有效的，并且通常出人意料的好，但是，有很多数据集远不是线性可分离的。处理非线性数据集的方法之一是添加更多特征，比如多项式特征（如第4章所述）。某些情况下，这可能导致数据集变得线性可分离。参见图5的左图：这是一个简单的数据集，只有一个特征x1。可以看出，数据集线性不可分。但是如果添加第二个特征x2=(x1)^2，生成的2D数据集则完全线性可分离。

![fig05_通过添加特征使数据集线性可分离](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter5/fig05_%E9%80%9A%E8%BF%87%E6%B7%BB%E5%8A%A0%E7%89%B9%E5%BE%81%E4%BD%BF%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E7%A6%BB.jpg)

为了使用Scikit-Learn来实现这个想法，创建一个包含`PolynomialFeatures`转换器（见4.3节）的`Pipeline`，然后是`StandardScaler`和`LinearSVC`。让我们在卫星数据集上进行测试：这是一个用于二元分类的小数据集，其中数据点的形状为两个交织的半圆（见图6）。你可以使用`make_moons()`函数生成此数据集：

```python
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import PolynomialFeatures

    polynomial_svm_clf = Pipeline([
        ("poly_features", PolynomialFeatures(degree=3)),
        ("scaler", StandardScaler()),
        ("svm_clf", LinearSVC(C=10, loss='hinge', random_state=42))
    ])

    polynomial_svm_clf.fit(X, y)
```

![fig06_使用多项式特征的线性SVM分类器](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter5/fig06_%E4%BD%BF%E7%94%A8%E5%A4%9A%E9%A1%B9%E5%BC%8F%E7%89%B9%E5%BE%81%E7%9A%84%E7%BA%BF%E6%80%A7SVM%E5%88%86%E7%B1%BB%E5%99%A8.jpg)

### 5.2.1 多项式内核

添加多项式特征实现起来非常简单，并且对所有的机器学习算法（不只是SVM）都非常有效。但问题是，如果多项式太低阶，则处理不了非常复杂的数据集。而高阶则会创造出大量的特征，导致模型变得太慢。

幸运的是，使用SVM时，有一个魔术般的数学技巧可以应用，这就是**核技巧**（稍后解释）。它产生的结果就跟添加了许多多项式特征（甚至是非常高阶的多项式特征）一样，但实际上并不需要真的添加。因为实际没有添加任何特征，所以也就不存在数量爆炸的组合特征了。这个技巧由SVC类来实现，我们看看在卫星数据集上的测试：

```python
    from sklearn.svm import SVC
    poly_kernel_svm_clf = Pipeline([
        ("scaler", StandardScaler()),
        ("svm_clf", SVC(kernel="poly", degree=3, coef0=1, C=5))
    ])
    poly_kernel_svm_clf.fit(X, y)
```

这段代码使用了一个3阶多项式内核训练SVM分类器。如图7的左图所示。而右图是另一个使用了10阶多项式核的SVM分类器。显然，如果模型过拟合，你应该降低多项式阶数；反过来，如果欠拟合，则可以尝试使之提升。超参数`coef0`控制的是**模型受高阶多项式还是低阶多项式影响的程度**。

寻找正确的超参数值的常用方法是网格搜索（见第2章）。先进行一次粗略的网格搜索，然后在最好的值附近展开一轮更精细的网格搜索，这样通常会快一些。多了解每个超参数实际上是用来做什么的，有助于你在超参数空间层正确搜索。

![fig07_多项式核的SVM分类器](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter5/fig07_%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%A0%B8%E7%9A%84SVM%E5%88%86%E7%B1%BB%E5%99%A8.jpg)

### 5.2.2 相似特征

解决非线性问题的另一种技术是**添加相似特征**，这些特征经过相似函数计算得出，**相似函数可以测量每个实例与一个特定地标之间的相似度**。以前面提到过的一维数据集为例，在x1=-2和x1=1处添加两个地标（见图9中的左图）。接下来，我们采用**高斯径向基函数（RBF）**作为相似函数，γ=0.3（见等式5-1）:

![fig08_高斯RBF](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter5/fig08_%E9%AB%98%E6%96%AFRBF.png)

这是一个从0（离地标差得非常远）到1（跟地标一样）变化的钟形函数。现在我们准备计算新特征。例如，我们看实例x1=-1：它与第一个地标的距离为1，与第二个地标的距离为2。因此它的新特征为x2=eps(-0.3×1^2)≈0.74，x3=eps(-0.3×2^2)≈0.30。图9的右图显示了转换后的数据集（去除了原始特征），现在你可以看出，数据呈线性可分离了。

![fig09_使用高斯RBF的相似特征](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter5/fig09_%E4%BD%BF%E7%94%A8%E9%AB%98%E6%96%AFRBF%E7%9A%84%E7%9B%B8%E4%BC%BC%E7%89%B9%E5%BE%81.jpg)

你可能想知道怎么选择地标。最简单的方法是在数据集里每一个实例的位置上创建一个地标。这会创造出许多维度，因而也增加了转换后的训练集线性可分离的机会。缺点是一个有m个实例n个特征的训练集会被转换成一个m个实例m个特征的训练集（假设抛弃了原始特征）。如果训练集非常大，那就会得到同样大数量的特征。

### 5.2.3 高斯RBF内核

与多项式特征方法一样，相似特征法也可以用任意机器学习算法，但是要计算出所有附加特征，其计算代价可能非常昂贵，尤其是对大型训练集来说。然而，核技巧再一次施展了它的SVM魔术：它能够产生的结果就跟添加了许多相似特征一样（但实际上也并不需要添加）。我们来使用SVC类试试高斯RBF核：

```python
rbf_kernel_svm_clf = Pipeline([
        ("scaler", StandardScaler()),
        ("svm_clf", SVC(kernel="rbf", gamma=5, C=0.001))
    ])
rbf_kernel_svm_clf.fit(X, y)
```

图10的左下方显示了这个模型。其他图显示了超参数`gamma(γ)`和C使用不同值时的模型。增加gamma值会使钟形曲线变得更窄（见图9的左图），因此每个实例的影响范围随之变小：决策边界变得更不规则，开始围着单个实例绕弯。反过来，减小gamma值使钟形曲线变得更宽，因而每个实例的影响范围增大，决策边界变得更平坦。所以γ就像是一个正则化的超参数：模型过拟合，就降低它的值，如果欠拟合则提升它的值（类似超
参数C）。

还有一些其他较少用到的核函数，例如专门针对特定数据结构的核函数。字符串核常用于文本文档或是DNA序列（如使用字符串子序列核或是基于莱文斯坦距离的核函数）的分类。

![fig10_使用RBF核的SVM分类器](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter5/fig10_%E4%BD%BF%E7%94%A8RBF%E6%A0%B8%E7%9A%84SVM%E5%88%86%E7%B1%BB%E5%99%A8.jpg)

有这么多的核函数，该如何决定使用哪一个呢？有一个经验法则是，永远先从线性核函数开始尝试（要记住，`LinearSVC`比`SVC(kernel="linear")`快得多），特别是训练集非常大或特征非常多的时候。如果训练集不太大，你可以试试高斯RBF核，大多数情况下它都非常好用。如果你还有多余的时间和计算能力，可以使用交叉验证和网格搜索来尝试一些其他的核函数，特别是那些专门针对你的数据集数据结构的核函数。

### 5.2.4 计算复杂度

`liblinear`库为线性SVM实现了一个优化算法，`LinearSVC`正是基于该库的。该算法不支持核技巧，不过它与训练实例的数量和特征数量几乎呈线性相关：其训练时间复杂度大致为O(m×n)。

如果你想要非常高的精度，算法需要的时间更长。它由容差超参数ε（在Scikit-Learn中为tol）来控制。大多数分类任务中，默认的容差就够了。

SVC则是基于`libsvm`库的，这个库的算法支持核技巧。训练时间复杂度通常在O(m^2×n)和O(m^3×n)之间。很不幸，这意味着如果训练实例的数量变大（例如成千上万的实例），它将会慢得可怕，所以这个算法完美适用于复杂但是中小型的训练集。但是，它还是可以良好地适应特征数量的增加，特别是应对稀疏特征（即每个实例仅有少量的非零特征）。在这种情况下，算法复杂度大致与实例的平均非零特征数成比例。表1比较了Scikit-Learn的SVM分类器类。

[表1：用于SVM分类的Scikit-Learn类的比较]

|         类    |    时间复杂度    |    核外支持    |    需要缩放    |    核技巧   |
|    ------: |    :-------:    |    :---------   |    ------    |:------|
|    LinearSVC    |    *O*(*m*×*n*)    |    否    |    是    |   是   |
|    SGDClassfier    |    *O*(*m*×*n*)    |    是    |  是  |  是  |
|    SVC    |    *O*(*m*<sup>2</sup>×*n*)  到  *O*(*m*<sup>3</sup>×*n*) |    否    |  是  |  是   |


## 5.3 SVM回归

正如前面提到的，SVM算法非常全面：它不仅支持线性和非线性分类，而且还支持线性和非线性回归。诀窍在于将目标反转一下：不再尝试拟合两个类之间可能的最宽街道的同时限制间隔违例，**SVM回归要做的是让尽可能多的实例位于街道上**，同时限制间隔违例（也就是不在街道上的实例）。街道的宽度由超参数ε控制。图11显示了用随机线性数据训练的两个线性SVM回归模型，一个间隔较大（ε=1.5），另一个间隔较小（ε=0.5）。

![fig11_SVM回归](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter5/fig11_SVM%E5%9B%9E%E5%BD%92.jpg)

在间隔内添加更多的实例不会影响模型的预测，所以这个模型被称为**ε不敏感**。

你可以使用Scikit-Learn的`LinearSVR`类来执行线性SVM回归。以下代码生成如图11左图所示的模型（训练数据需要先缩放并居中）：

```python
    from sklearn.svm import LinearSVR
    svm_reg = LinearSVR(epsilon=1.5)
    svm_reg.fit(X, y)
```

要解决非线性回归任务，可以使用核化的SVM模型。例如，图12显示了在一个随机二次训练集上使用二阶多项式核的SVM回归。左图几乎没有正则化（C值很大），右图则过度正则化（C值很小）。

![fig12_使用二阶多项式核的SVM回归](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter5/fig12_%E4%BD%BF%E7%94%A8%E4%BA%8C%E9%98%B6%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%A0%B8%E7%9A%84SVM%E5%9B%9E%E5%BD%92.jpg)

以下代码使用Scikit-Learn的SVR类（支持核技巧）生成如图12左图所示的模型：

```python
    from sklearn.svm import SVR
    svm_poly_reg = SVR(kernel="poly", degree=2, C=100, epsilon=0.1)
    svm_poly_reg.fit(X, y)
```

SVR类是SVC类的回归等价物，`LinearSVR`类也是`LinearSVC`类的回归等价物。`LinearSVR`与训练集的大小线性相关（与`LinearSVC`一样），而SVR则在训练集变大时，变得很慢（SVC也一样）

## 5.4 工作原理

本节将会介绍SVM如何进行预测，以及它们的训练算法是如何工作的，从线性SVM分类器开始。如果你刚刚开始接触机器学习，可以安全地跳过本节，直接进入本章末尾的练习，等到想要更深入地了解SVM时再回来也不迟。

首先，说明一下符号。在第4章里，我们使用过一个约定，将所有模型参数放在一个向量θ中，包括偏置项θ<sub>0</sub>，以及输入特征的权重θ<sub>1</sub>到θ<sub>n</sub>，同时在所有实例中添加偏置项x<sub>0</sub>=1。在本章中，我们将会使用另一个约定，在处理SVM时它更为方便（也更常见）：**偏置项**表示为*b*，**特征权重向量**表示为***w***，同时输入特征向量中不添加偏置特征。

### 5.4.1 决策函数和预测

线性SVM分类器通过简单地计算决策函数来预测新实例x的分类。如果结果为正，则预测类别是正类(1)，否则预测其为负类(0)。见公式5-2。

决策函数：

![fig13_决策函数](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter5/fig13_%E5%86%B3%E7%AD%96%E5%87%BD%E6%95%B0.png)

公式5-2：线性SVM分类器预测

![fig14_线性SVM分类器预测](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter5/fig14_%E7%BA%BF%E6%80%A7SVM%E5%88%86%E7%B1%BB%E5%99%A8%E9%A2%84%E6%B5%8B.png)

图15显示了图5-4右侧的模型所对应的决策函数：数据集包含两个特征（花瓣宽度和长度），所以是一个二维平面。决策边界是决策函数等于0的点的集合：它是两个平面的交集，也就是一条直线（加粗实线所示）。

![fig15_鸢尾花数据集的决策函数](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter5/fig15_%E9%B8%A2%E5%B0%BE%E8%8A%B1%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%86%B3%E7%AD%96%E5%87%BD%E6%95%B0.jpg)

虚线表示决策函数等于1或-1的点：它们互相平行，并且与决策边界的距离相等，从而形成了一个间隔。训练线性SVM分类器意味着找到w和b的值，从而使这个间隔尽可能宽的同时，避免（硬间隔）或限制（软间隔）间隔违例。

### 5.4.2 训练目标

思考一下决策函数的斜率：它等于权重向量的范数，即‖w‖。如果我们将斜率除以2，那么决策函数等于±1的点也将变得离决策函数两倍远。也就是说，将斜率除以2，将会使间隔乘以2。也许2D图更容易将其可视化，见图16。权重向量w越小，间隔越大。

我们要最小化‖w‖来得到尽可能大的间隔。但是，如果我们想避免任何间隔违例（硬间隔），那么就要使所有正类训练集的决策函数大于1，负类训练集的决策函数小于-1。如果我们定义实例为负类（如果y<sup>(i)</sup>=0）时，t<sup>(i)</sup>=-1；实例为正类（如果y<sup>(i)</sup>=1）时，t<sup>(i)</sup>=1。那么就可以将这个约束条件表示为：对所有实例来说，

![fig16_约束条件](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter5/fig16_%E7%BA%A6%E6%9D%9F%E6%9D%A1%E4%BB%B6.png)

因此，我们可以将硬间隔线性SVM分类器的目标看作一个约束优化问题，如公式5-3所示：

![fig17_硬间隔线性SVM分类器的目标](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter5/fig17_%E7%A1%AC%E9%97%B4%E9%9A%94%E7%BA%BF%E6%80%A7SVM%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%E7%9B%AE%E6%A0%87.png)

达到软间隔的目标，我们需要为每个实例引入一个**松弛变量**ζ<sup>(i)</sup>≥0，ζ<sup>(i)</sup>衡量的是第i个实例多大程度上允许间隔违例。那么现在我们有了两个互相冲突的目标：**使松弛变量越小越好从而减少间隔违例**，同时还要使w<sup>T</sup>·w/2最小化以增大间隔。这正是超参数C的用武之地：**允许我们在两个目标之间权衡**。公式5-4给出了这个约束优化问题。

![fig18_软间隔线性SVM分类器目标](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter5/fig18_%E8%BD%AF%E9%97%B4%E9%9A%94%E7%BA%BF%E6%80%A7SVM%E5%88%86%E7%B1%BB%E5%99%A8%E7%9B%AE%E6%A0%87.png)

### 5.4.3 二次规划

硬间隔和软间隔问题都属于线性约束的凸二次优化问题。这类问题被称为二次规划（QP）问题。要解决二次规划问题有很多现成的求解器，使用到的技术各不相同，这些不在本书的讨论范围之内。公式5-5给出的是问题的一般形式：

![fig19_二次规划问题](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter5/fig19_%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92%E9%97%AE%E9%A2%98.png)

其中：

- p是一个n<sub>p</sub>维向量（n<sub>p</sub>是参数数量）

- H是一个n<sub>p</sub>×n<sub>p</sub>的矩阵

- f是一个n<sub>p</sub>维的向量

- A是一个n<sub>c</sub>×n<sub>p</sub>的矩阵（n<sub>c</sub>是约束的数量）

- b是一个n<sub>c</sub>维的向量

请注意，表达式Ap≤b定义了n<sub>c</sub>个约束：p<sup>T</sup>a<sup>(i)</sup>≤b<sup>(i)</sup>，其中i=1,2,...,n<sub>c</sub>，其中a<sup>(i)</sup>是包含A的第i行元素的向量，b<sup>(i)</sup>是b的第i个元素。

你可以轻松地验证，如果用以下方式设置QP参数，就可以得到硬间隔线性SVM分类器的目标：

- n<sub>p</sub>=n+1，其中n是特征数量（+1是偏置项）。

- n<sub>c</sub>=m，其中m是训练实例的数量。

- H是一个n<sub>p</sub>×n<sub>p</sub>的矩阵，除了在左上方的元素为零（忽略偏置项）。

- f=0，一个全零的n<sub>p</sub>维向量。

- b=-1，一个全是-1的n<sub>c</sub>维向量。

- $a^{(i)}=-t^{(i)}\dot x^{(i)}$ ，其中 $\dot x^{(i)}$  等于 $x^{(i)}$ ，并且具有额外的偏差特征 $\dot x _0= 1$ 。

所以，要训练硬间隔线性SVM分类器，有一种办法是直接将上面的参数用在一个现成的二次规划求解器上。得到的向量p将会包括偏置项b=p<sub>0</sub>，以及特征权重w<sub>i</sub>=p<sub>i</sub>，i=1,2,...,m。类似地，你也可以用二次规划求解器来解决软间隔问题（见本章末尾练习）。

但是，为了运用核技巧，接下来我们将要看一个不同的约束优化问题。

### 5.4.4 对偶问题

针对一个给定的约束优化问题，称之为**原始问题**，我们常常可以用另一个不同的，但是与之密切相关的问题来表达，这个问题我们称之为**对偶问题**。通常来说，对偶问题的解只能算是原始问题的解的下限，但是在某些情况下，它也可能跟原始问题的解完全相同。幸运的是，SVM问题刚好就满足这些条件，所以你可以选择是解决原始问题还是对偶问题，二者解相同。公式5-6给出了线性SVM目标的对偶形式（如果你对如何从原始问题导出对偶问题感兴趣，请参阅附录C）

![fig20_线性SVM目标的对偶形式]()

一旦得到使得该等式最小化（使用二次规划求解器）的向量$\hat \alpha$，就可以用公式5-7来计算使原始问题最小化的$\hat w$和$\hat b$。

![fig21_从对偶问题到原始问题]()

当训练实例的数量小于特征数量时，解决对偶问题比原始问题更快速。更重要的是，它能够实现核技巧，而原始问题不可能实现。这个核技巧到底是什么呢？

### 5.4.5 内核化SVM

内核化SVM假设你想要将一个二阶多项式转换为一个二维训练集（例如卫星训练集），然后在转换训练集上训练线性SVM分类器。这个二阶多项式的映射函数φ如公式5-8所示：

![fig22_二阶多项式映射]()

注意转换后的向量是三维的而不是二维的。现在我们来看看，如果应用这个二阶多项式映射，两个二维向量a和b会发生什么变化，然后计算转换后两个向量的点积（注：如第4章所述，两个向量a和b的点积通常记为a·b。但是，在机器学习中，向量经常表示为列向量（即单列矩阵），因此点积可通过计算aTb获得。为了与本书的其余部分保持一致，我们将在此处使用此表示法，而忽略了这一事实，即从技术上讲会导致一个单元矩阵而不是标量值。）（参见公式5-9）。

![fig23_二阶多项式映射的核技巧]()

关键点：如果将转换映射φ应用于所有训练实例，那么对偶问题（见公式5-6）将包含点积 $\phi(x^{(i)})^T\phi(x^{(j)})$ 的计算。如果φ是公式5-8所定义的二阶多项式转换，那么可以直接 $(x^{(i)T}x^{(j)})^2$ 用来替代这个转换向量的点积。所以你根本不需要转换训练实例，只需将公式5-6里的点积换成点积的平方即可。如果你不嫌麻烦，可以动手将训练集进行转换，然后拟合线性SVM算法，你会发现，结果一模一样。但是这个技巧大大提高了整个过程的计算效率，这就是核技巧的本质。

函数 $K(a, b) =(a^T·b)^2$ 被称为二阶多项式核。在机器学习里，核是能够仅基于原始向量a和b来计算点积 $φ(a)^Tφ(b)$ 的函数，它不需要计算（甚至不需要知道）转换函数。公式5-10列出了一些最常用的核函数。

![fig24_常用核函数]()

根据Mercer定理，如果函数K(a, b)符合几个数学条件——也就是Mercer条件（K必须是连续的，并且在其参数上对称，所以K(a, b) = K(b,a)，等等），则存在函数φ将a和b映射到另一空间（可能是更高维度的空间），使得$K(a, b) =φ(a)^Tφ(b)$。所以你可以将K用作核函数，因为你知道φ是存在的，即使你不知道它是什么。对于高斯RBF核函数，可以看出，φ实际上将每个训练实例映射到了一个无限维空间，幸好不用执行这个映射。

注意，也有一些常用的核函数（如S型核函数）不符合Mercer条件的所有条件，但是它们在实践中通常也表现不错。

还有一个未了结的问题需要说明。公式5-7显示了用线性SVM分类器如何从对偶解走到原始解，但是如果你应用了核技巧，最终得到的是包含 $φ(x^{(i)})$ 的方程。而 $\hat w$ 的维度数量必须与 $φ(x^{(i)})$ 相同，后者很有可能是巨大甚至是无穷大的，所以你根本没法计算。可是不知道 $\hat w$ 该如何做出预测呢？你可以将公式5-7中 $\hat w$ 的公式插入新实例x<sup>(n)</sup>的决策函数中，这样就得到了一个只包含输入向量之间点积的公式。这时你就可以再次运用核技巧了（见公式5-11）。

![fig25_使用核化SVM做出预测]()

注意，因为仅对于支持向量才有 α<sup>(i)</sup>≠0，所以预测时，计算新输入向量x(n)的点积，使用的仅仅是支持向量而不是全部训练实例。当然，你还需要使用同样的技巧来计算偏置项 $\hat b$  （见公式5-12）。

![fig26_使用核技巧来计算偏置项]()

### 5.4.6 在线SVM

在结束本章之前，让我们快速看一下在线SVM分类器（回想一下，在线学习意味着增量学习，通常是随着新实例的到来而学习）。

对于线性SVM分类器，一种实现在线SVM分类器的方法是使用梯度下降（例如，使用`SGDClassifier`）来最小化源自原始问题的公式5-13中的成本函数。不幸的是，梯度下降比基于QP的方法收敛慢得多。

![fig27_线性SVM分类器成本函数]()

成本函数中的第一项会推动模型得到一个较小的权重向量w，从而使间隔更大。第二项则计算全部的间隔违例。如果没有一个示例位于街道之上，并且都在街道正确的一边，那么这个实例的间隔违例为0；否则，该实例的违例大小与其到街道正确一边的距离成正比。所以将这个项最小化，能够保证模型使间隔违例尽可能小，也尽可能少。

Hinge损失函数：

函数max(0，1-t)被称为hinge损失函数（如图所示）。当t≥1时，函数等于0。如果t<1，其导数（斜率）等于-1；如果t>1，则导数（斜率）为0；t=1时，函数不可导。但是，在t=0处可以使用任意次导数（即-1到0之间的任意值），你还是可以使用梯度下降，就跟Lasso回归一样。

![fig28_hinge损失函数]()
