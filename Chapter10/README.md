# 第十章 Keras人工神经网络简介

人工神经网络是深度学习的核心。它们用途广泛、功能强大且可扩展，使其**非常适合处理大型和高度复杂的机器学习任务**，例如对数十亿张图像进行分类（例如Google Images），为语音识别服务（例如Apple的Siri）提供支持，每天向成千上万的用户推荐（例如YouTube）观看的最佳视频，或学习在围棋游戏（DeepMind的AlphaGo）中击败世界冠军。

本章的第一部分介绍了人工神经网络，首先是对第一个ANN架构的快速浏览，然后是今天广泛使用的多层感知机（MLP）（其他架构将在第11章中进行探讨）。在第二部分中，我们将研究如何使用流行的Keras API实现神经网络。这是设计精巧、简单易用的用于构建、训练、评估和运行神经网络的API。但是，不要被它的简单性所迷惑：它的表现力和灵活性足以让你构建各种各样的神经网络架构。实际上，对于大多数示例而言，这可能就足够了。如果你需要额外的灵活性，可以随时使用其较低级的API编写自定义的Keras组件，这将在第12章中讨论。

## 10.1 从生物神经元到人工神经元

McCulloch和Pitts在其具有里程碑意义的论文“*A Logical Calculus of Ideas Immanent in Nervous Activity*”中，提出了一种简化的计算模型，该模型计算了生物神经元如何在动物大脑中协同工作，利用命题逻辑进行复杂的计算。这是第一个人工神经网络架构。

人们对人工神经网络重新充满兴趣将对我们的生活产生更深远的影响：

- 现在有大量数据可用于训练神经网络，并且在非常大和复杂的问题上，人工神经网络通常优于其他机器学习技术。

- 自20世纪90年代以来，计算能力的飞速增长使得现在有可能在合理的时间内训练大型神经网络。

- 训练算法已得到改进。

- 在实践中，人工神经网络的一些理论局限性被证明是良性的。例如，许多人认为ANN训练算法注定要失败，因为它们可能会陷入局部最优解，但事实证明，这在实践中相当罕见。

- 人工神经网络似乎已经进入了资金和发展的良性循环。

### 10.1.1 生物神经元

生物神经元产生短的电脉冲称为动作电位（AP，或只是信号），它们沿着轴突传播，使突触释放称为神经递质的化学信号。当神经元在几毫秒内接收到足够数量的这些神经递质时，它会激发自己的电脉冲。

![fig01_生物神经元](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/fig01_%E7%94%9F%E7%89%A9%E7%A5%9E%E7%BB%8F%E5%85%83.jpg)

因此，单个生物神经元的行为似乎很简单，但是它们组成了数十亿个庞大的网络，每个神经元都与数千个其他神经元相连。高度复杂的计算可以通过相当简单的神经元网络来执行，就像复杂的蚁丘可以通过简单蚂蚁的共同努力而出现一样。生物神经网络（BNN）的架构仍是活跃的研究主题，但大脑的某些部分已被绘制成图，似乎神经元通常组织成连续的层，尤其是在大脑皮层中（大脑的外层），如图2所示。

![fig02_生物神经网络](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/fig02_%E7%94%9F%E7%89%A9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.jpg)

### 10.1.2 神经元的逻辑计算

McCulloch和Pitts提出了一个非常简单的生物神经元模型，该模型后来被称为**神经元(artificial neuron)**。它具有*一个或多个二进制（开/关）输入和一个二进制输出*。当超过一定数量的输入处于激活状态时，人工神经元将激活其输出。他们的论文表明即使使用这样的简化模型，也可以构建一个人工神经元网络来计算所需的任何逻辑命题。为了了解这种网络的工作原
理，让我们构建一些执行各种逻辑计算的ANN（见图3），假设神经元至少两个输入处于激活状态时，神经元就会被激活。

![fig03_ANN执行简单的逻辑运算](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/fig03_ANN%E6%89%A7%E8%A1%8C%E7%AE%80%E5%8D%95%E7%9A%84%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97.jpg)

网络的作用如下所述：

- 左边的第一个网络是恒等函数：如果神经元A被激活，那么神经元C也被激活（因为它从神经元A接收到两个输入信号）；如果神经元A关闭，那么神经元C也关闭。

- 第二个网络执行逻辑AND：仅当神经元A和B都被激活（单个输入信号不足以激活神经元C）时，神经元C才被激活。

- 第三个网络执行逻辑OR：如果神经元A或神经元B被激活（或两者都激活），则神经元C被激活。

- 最后，如果我们假设输入连接可以抑制神经元的活动（生物神经元就是这种情况），则第四个网络计算出一个稍微复杂的逻辑命题：只有在神经元A处于活动状态和神经元B关闭时，神经元C才被激活。如果神经元A一直处于活动状态，那么你会得到逻辑NOT：神经元B关闭时神经元C处于活动状态，反之亦然。

### 10.1.3 感知器

**感知器(Perceptron)**是最简单的ANN架构之一，由Frank Rosenblatt于1957年发明。它基于稍微不同的人工神经元（见图4），称为**阈值逻辑单元（TLU）**，有时也称为**线性阈值单元（LTU）**。输入和输出是数字（而不是二进制开/关值），并且每个输入连接都与权重相关联。TLU计算其输入的加权和（ $z=w_1x_1+w_2x_2+...+w_nx_n=x^Tw$），然后将**阶跃函数**应用于该和并输出结果：$h_w(x)=step(z)$，其中 $z=x^Tw$。

![fig04_阈值逻辑单元](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/fig04_%E9%98%88%E5%80%BC%E9%80%BB%E8%BE%91%E5%8D%95%E5%85%83.jpg)

感知器中最常用的阶跃函数是**Heaviside阶跃函数**（见公式10-1）。有时使用符号函数代替。

![fig05_公式10-1_Heaviside阶跃函数](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/fig05_%E5%85%AC%E5%BC%8F10-1_Heaviside%E9%98%B6%E8%B7%83%E5%87%BD%E6%95%B0.jpg)

单个TLU可用于简单的线性二进制分类。它计算输入的线性组合，如果结果超过阈值，则输出正类；否则，它将输出负类（就像逻辑回归或线性SVM分类器一样）。例如，你可以使用单个TLU根据花瓣的长度和宽度对鸢尾花进行分类（就像我们在前面的章节中所做的那样，还添加了额外的偏置特征 $x_0=1$）。在这种情况下，训练TLU意味着找到$w_0$、$w_1$和$w_2$的正确值（稍后将讨论训练算法）。

**感知器仅由单层TLU组成**，每个TLU连接到所有的输入。当一层中的所有神经元都连接到上一层中的每个神经元（即其输入神经元）时，该层称为**全连接层或密集层**。感知器的输入被送到称为**输入神经元**的特殊直通神经元：它们输出被送入的任何输入。**所有输入神经元形成输入层**。此外，通常会添加一个额外的偏置特征（$x_0=1$）：通常使用一种称为**偏置神经元**的特殊类型的神经元来表示该特征，该神经元始终输出1。具有两个输入和三个输出的感知器如图5所示。**该感知器可以将实例同时分为三个不同的二进制类，这使其成为多输出分类器**。

![fig06_感知器架构](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/fig06_%E6%84%9F%E7%9F%A5%E5%99%A8%E6%9E%B6%E6%9E%84.jpg)

借助线性代数的魔力，公式10-2使得可以同时为多个实例高效地计算出一层人工神经元的输出。

![fig07_计算全连接层的输出](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/fig07_%E8%AE%A1%E7%AE%97%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E7%9A%84%E8%BE%93%E5%87%BA.jpg)

其中，

- X代表输入特征的矩阵。每个实例一行，每个特征一列。

- 权重矩阵W包含除偏置神经元外的所有连接权重。在该层中，每个输入神经元一行，每个人工神经元一列。

- 偏置向量b包含偏置神经元和人工神经元之间的所有连接权重。每个人工神经元有一个偏置项。

- 函数φ称为**激活函数**：当人工神经元是TLU时，它是阶跃函数（但我们在后面会讨论其他激活函数）。

那么，感知器如何训练？Rosenblatt提出的感知器训练算法在很大程度上受Hebb规则启发。Donald Hebb在其1949年的The Organization of Behavior（Wiley）中提出，当一个生物神经元经常触发另一个神经元时，这两个神经元之间的联系就会增强。后来，Siegrid Löwel用有名的措辞概括了Hebb的思想，即“触发的细胞，连接在一起”。也就是说，**两个神经元同时触发时，它们之间的连接权重会增加**。该规则后来被称为Hebb规则（或Hebb学习）。使用此规则的变体训练感知器，该变体考虑了网络进行预测时所犯的错误。感知器学习规则加强了有助于减少错误的连接。更具体地说，感知器一次被送入一个训练实例，并且针对每个实例进行预测。对于产生错误预测的每个输出神经元，它会增强来自输入的连接权重，这些权重将有助于正确的预测。该规则如公式3所示。

![fig08_公式10-3_感知器学习规则](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/fig08_%E5%85%AC%E5%BC%8F10-3_%E6%84%9F%E7%9F%A5%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%A7%84%E5%88%99.jpg)

其中：

- $w_{i,j}$是第i个输入神经元和第j个输出神经元之间的连接权重；

- $x_i$是当前训练实例的第i个输入值；

- $\haty_j$是当前训练实例的第j个输出神经元的输出。

- $y_j$是当前训练实例的第j个输出神经元的目标输出。

- $η$是学习率。

每个**输出神经元的决策边界都是线性的**，因此感知器无法学习复杂的模式（就像逻辑回归分类器一样）。但是，如果训练实例是线性可分的，osenblatt证明了该算法将收敛到一个解。这被称为**感知器收敛定理**。

Scikit-Learn提供了一个Perceptron类，该类实现了单个TLU网络。它可以像你期望的那样使用，例如，在鸢尾植物数据集上：

```python
    import numpy as np
    from sklearn.datasets import load_iris
    from sklearn.linear_model import Perceptron

    iris = load_iris()
    X = iris.data[:, (2, 3)]  # petal length, petal width
    y = (iris.target == 0).astype(np.int)

    per_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=42)
    per_clf.fit(X, y)

    y_pred = per_clf.predict([[2, 0.5]])
```
你可能已经注意到，感知器学习算法非常类似于随机梯度下降。实际上，Scikit-Learn的Perceptron类等效于使用具有以下超参数的 `SGDClassifier: loss="perceptron"`，`learning_rate="constant"`，`eta0=1`（学习率）和`penalty=None`（无正则化））。请注意，与逻辑回归分类器相反，感知器不输出分类概率；相反，它们基于硬阈值进行预测。这是逻辑回归胜过感知器的原因。

Marvin Minsky和Seymour Papert在1969年的专著Perceptron中，特别指出了感知器的一些严重缺陷，即它们无法解决一些琐碎的问题（例如，异或（XOR）分类问题，参见图9的左侧）。任何其他线性分类模型（例如逻辑回归分类器）都是如此，但是研究人员对感知器的期望更高，有些人感到失望，他们完全放弃了神经网络，转而支持更高层次的问题，例如逻辑、问题求解和搜索。

![fig09_XOR分类问题和解决该问题的MLP](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/fig09_XOR%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E8%AF%A5%E9%97%AE%E9%A2%98%E7%9A%84MLP.jpg)

事实证明，可以通过堆叠多个感知器来消除感知器的某些局限性。所得的ANN称为**多层感知器（MLP）**。MLP可以解决XOR问题，你可以通过计算图9右侧所示的MLP的输出来验证：输入(0,0)或(1,1)，网络输出0，输入(0,1)或(1,0)则输出1。所有连接的权重等于1，但显示权重的四个连接除外。尝试验证该网络确实解决了XOR问题！

### 10.1.4 多层感知器和反向传播

MLP由一层（直通）输入层、一层或多层TLU（称为隐藏层）和一个TLU的最后一层（称为输出层）组成（见图10）。靠近输入层的层通常称为**较低层**，靠近输出层的层通常称为**较高层**。除输出层外的每一层都包含一个偏置神经元，并完全连接到下一层。

信号仅沿一个方向（从输入到输出）流动，因此该架构是**前馈神经网络（FNN）**的示例。

![fig10_多层感知器架构](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/fig10_%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%E6%9E%B6%E6%9E%84.jpg)

当一个ANN包含一个深层的隐藏层时，它称为**深度神经网络（DNN）**。深度学习领域研究DNN，更广泛地讲包含深度计算堆栈的模型。即便如此，只要涉及神经网络（甚至是浅层的神经网络），许多人就会谈论深度学习。

1986年，David Rumelhart、Geoffrey Hinton和Ronald Williams发表的开创性论文介绍了**反向传播训练算法**，该算法至今仍在使用。简而言之，它是使用有效的技术自动计算梯度下降（在第4章中介绍）：在仅两次通过网络的过程中（一次前向，一次反向），反向传播算法能够针对每个模型参数计算网络误差的梯度。换句话说，它可以找出应如何调整每个连接权重和每个偏置项以减少误差。一旦获得了这些梯度，它便会执行常规的梯度下降步骤，然后重复整个过程，直到网络收敛到解。

自动计算梯度称为自动微分或者autodiff。有各种autodiff技术，各有优缺点。反向传播使用的一种称为反向模式autodiff。它快速而精确，并且非常适用于微分函数具有多个变量（例如，连接权重）和少量输出（例如，一个损失）的情况。

让我们更详细地介绍一下该算法：

- 它一次处理一个小批量（例如，每次包含32个实例），并且多次遍历整个训练集。每次遍历都称为一个**轮次**。

- 每个小批量都传递到网络的输入层，然后将其送到第一个隐藏层。然后该算法将计算该层中所有神经元的输出（对于小批量中的每个实例）。结果传递到下一层，计算其输出并传递到下一层，以此类推，直到获得最后一层（即输出层）的输出。这就是**前向通路**：就像进行预测一样，只是保留了所有中间结果，因为反向遍历需要它们。

- 接下来，该算法测量网络的输出误差（该算法使用一种损失函数，该函数将网络的期望输出与实际输出进行比较，并返回一些误差测量值）。

- 然后，它计算每个输出连接对错误的贡献程度。通过应用链式法则（可能是微积分中最基本的规则）来进行分析，从而使此步骤变得快速而精确。

- 然后，算法再次使用链式法则来测量这些错误贡献中有多少是来自下面层中每个连接的错误贡献，算法一直进行，到达输入层为止。如前所述，这种反向传递通过在网络中向后传播误差梯度，从而有效地测量了网络中所有连接权重上的误差梯度（因此称为算法）。

- 最终，该算法执行梯度下降步骤，使用刚刚计算出的误差梯度来调整网络中的所有连接权重。

该算法非常重要，值得再次总结：对于每个训练实例，反向传播算法首先进行预测（正向传递）并测量误差，然后反向经过每个层以测量来自每个连接的误差贡献（反向传递），最后调整连接权重以减少错误（梯度下降步骤）。

随机初始化所有隐藏层的连接权重很重要，否则训练将失败。例如，如果将所有权重和偏置初始化为零，则给定层中的所有神经元将完全相同，从而反向传播将以完全相同的方式影响它们，因此它们将保持相同。换句话说，尽管每层有数百个神经元，但是模型会像每层只有一个神经元一样工作：不会太聪明。相反，如果随机初始化权重，则会破坏对称性，并允许反向传播来训练各种各样的神经元。

为了使该算法正常工作，作者对MLP的架构进行了重要更改，将阶跃函数替换为逻辑（s型）函数：σ(z)=1/(1+exp(-z))。这一点很重要，因为阶跃函数仅包含平坦段，所以没有梯度可使用（梯度下降不能在平面上移动），而逻辑函数在各处均具有定义明确的非零导数，从而使梯度下降在每一步都可以有所进展。实际上，反向传播算法可以与许多其他激活函数（不仅是逻辑函数）一起很好地工作。这是另外两个受欢迎的选择：

双曲正切函数：tanh(z)=2σ(2z)-1

与逻辑函数一样，该激活函数为S形、连续且可微，但其输出值范围为-1到1（而不是逻辑函数的从0到1）。在训练开始时，该范围倾向于使每一层的输出或多或少地以0为中心，这通常有助于加快收敛速度。

线性整流单位函数：ReLU(z)=max(0，z)

ReLU函数是连续的，但不幸的是，在z=0时，该函数不可微分（斜率会突然变化，这可能使梯度下降反弹），如果z<0则其导数为0。但是，实际上它运行良好并且具有计算快速的优点，因此它已成为默认值。最重要的是，它没有最大输出值这一事实有助于减少梯度下降期间的某些问题（我们将在第11章中对此进行讨论）。

这些流行的激活函数及其派生函数如图11所示。为什么我们首先需要激活函数？如果连接多个线性变换，那么得到的只是一个线性变换。例如，如果f(x)=2x+3且g(x)=5x-1，则连接这两个线性函数可以得到另一个线性函数：f(g(x))=2(5x-1）+3=10x+1。因此，如果层之间没有非线性，那么即使是很深的层堆叠也等同于单个层，这样你无法解决非常复杂的问题。相反，具有非线性激活函数的足够大的DNN理论上可以近似任何连续函数。

![fig11_激活函数及其派生](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/fig11_%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%8F%8A%E5%85%B6%E6%B4%BE%E7%94%9F.jpg)

好！你知道神经网络来自何处、其结构是什么以及如何计算其输出。你还了解了反向传播算法。但是，你到底可以使用它们做什么呢？

### 10.1.5 回归MLP

首先，MLP可用于回归任务。如果要预测单个值（房屋的价格，给定其许多特征），则只需要单个输出神经元：其输出就是预测值。对于多元回归（即一次预测多个值），每个输出维度需要一个输出神经元。例如，要在图像中定位物体的中心，你需要预测2D坐标，因此需要两个输出神经元。如果你还想在物体周围放置边框，则还需要两个数字：

物体的宽度和高度。因此，你得到了四个输出神经元。

通常，在构建用于回归的MLP时，你不想对输出神经元使用任何激活函数，因此它们可以输出任何范围的值。如果要保证输出始终为正，则可以在输出层中使用ReLU激活函数。另外，你可以使用`softplus`激活函数，它是ReLU的平滑变体：softplus(z)=log(1+exp(z))。当z为负时，它接近于0，而当z为正时，它接近于z。最后，如果要保证预测值落在给定的值范围内，则可以使用逻辑函数或双曲正切，然后将标签缩放到适当的范围：逻辑函数的范围为0到1，双曲正切为-1到1。

训练期间要使用的损失函数通常是均方误差，但是如果训练集中有很多离群值，则你可能更愿意使用平均绝对误差。或者，你可以使用Huber损失，这是两者的组合。

当误差小于阈值δ（通常为1）时，Huber损失为二次方，而当误差大于δ时，Huber损失为线性。线性部分使它对离群值的敏感性低于均方误差，而二次方部分使它比平均绝对误差更收敛并且更精确。

表10-1总结了回归MLP的典型架构。

|超参数|典型值|
|:---:|:---:|
|输入神经元数量|每个输入特征一个|
|隐藏层数量|取决于问题，但通常为1到5|
|每个隐藏层的神经元数量|取决于问题，但通常为10到100|
|输出神经元数量|每个预测维度输出1个神经元|
|隐藏的激活|ReLU（或SELU，见第11章）|
|输出激活|无，或ReLU/softplus（如果为正输出）或逻辑/tanh（如果为有界输出）|
|损失函数|MSE或MAE/Huber（如果存在离群值）|

### 10.1.6 分类MLP

MLP也可以用于分类任务。对于二进制分类问题，你只需要使用逻辑激活函数的单个输出神经元：输出将是0到1之间的数字，你可以将其解释为正类的估计概率。负类别的估计概率等于一减去该数字。

MLP还可以轻松处理多标签二进制分类任务（第3章）。例如，你可能有一个电子邮件分类系统，该系统可以预测每个收到的电子邮件是正常邮件还是垃圾邮件，并同时预测它是紧急电子邮件还是非紧急电子邮件。在这种情况下，你需要两个输出神经元，两个都使用逻辑激活函数：第一个输出电子邮件为垃圾邮件的可能性，第二个输出紧急邮件的可能性。更一般地，你为每个正类别用一个输出神经元。请注意，输出概率不一定要加起来为1。这可以使模型输出任何组合的标签：你可以包含非紧急正常邮件、紧急正常邮件、非紧急垃圾邮件，甚至可能是紧急垃圾邮件（尽管可能是一个错误）。

如果每个实例只能属于三个或更多可能的类中的一个类（例如，用于数字图像分类的类0到9），则每个类需要一个输出神经元，并且应该使用softmax激活函数整个输出层（见图12）。softmax函数（在第4章中介绍）将确保所有估计的概率在0到1之间，并且它们加起来等于1（如果类是互斥的，则是必需的）。这称为多类分类。

![图fig12_用于分类的现代MLP](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/%E5%9B%BEfig12_%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%E7%9A%84%E7%8E%B0%E4%BB%A3MLP.jpg)

关于损失函数，由于我们正在预测概率分布，因此交叉熵损失（也称为对数损失，见第4章）通常是一个不错的选择。表10-2总结了分类MLP的典型架构：

表10-2：典型的MLP架构

|超参数|二进制分类|多标签二进制分类|多类分类|
|:---:|:---:|:---:|:---:|
|输入层和隐藏层|与回归相同|与回归相同|与回归相同|
|输出神经元数量|1|每个标签1|每个类1|
|输出层激活|逻辑|逻辑|softmax|
|损失函数|交叉熵|交叉熵|交叉熵|

## 10.2 使用Keras实现MLP

Keras是高级深度学习API，可让你轻松构建、训练、评估和执行各种神经网络。其文档（或规范）可从https://keras.io/获得。这个参考实现也称为Keras，由Fransois Chollet开发，是一个研究项目的一部分，并于2015年3月作为开源项目发布。由于其易用性、灵活性和精巧设计，它迅速流行。为了执行神经网络所需的繁重计算，此参考实现依赖于计算后端。目前你可以从三种流行的开源深度学习库中进行选择：TensorFlow、微软的Cognitive Toolkit（CNTK）和Theano。因此，为避免混淆，我们将此参考实现称为多后端Keras。

自2016年底以来发布了其他的实现。现在，你可以在Apache MXNet、苹果的Core ML、avaScript或TypeScript（可以在网络浏览器中运行Keras代码）和PlaidML（可以在各种GPU设备上运行，而不仅仅是Nvidia）上运行Keras。而且，TensorFlow本身现在与自己的Keras实现程序tf.keras捆绑在一起。它仅支持TensorFlow作为后端，但具有提供一些有用的额外功能的优势（见图13）：例如，它支持TensorFlow的数据API，可轻松高效地加载和预处理数据。因此，我们将在本书中使用tf.keras。但是，在本章中，我们将不会使用任何特定于TensorFlow的功能，因此该代码也应该可以在其他Keras实现上很好地运行（至少在Python中），并且只需进行少量修改即可，例如更改导入。

![fig13_Keras API的两种实现]()

在Keras和TensorFlow之后，最受欢迎的深度学习库是Facebook的PyTorch库。好消息是它的API与Keras的API十分相似（部分原因是这两个API均受Scikit-Learn和Chainer的启发），因此一旦你了解Keras，便可以轻松切换到PyTorch（如果你想的话）。PyTorch的受欢迎程度在2018年呈指数增长，这主要归功于它的简单性和出色的文档，而这并不是TensorFlow 1.x的主要优势。但是TensorFlow 2可以说与PyTorch一样简单，因为它采用Keras作为其官方高级API，并且其开发人员简化和清理了其余的API。该文档也已被完全重新组织，现在更容易找到所需的内容。同样，PyTorch1.0的主要缺点（例如，有限的可移植性和无计算图分析）已得到解决。健康的竞争对所有人都有利。

好了，该写代码了！由于tf.keras与TensorFlow捆绑在一起，让我们从安装TensorFlow开始。

### 10.2.1 安装Tensorflow2

### 10.2.2 使用顺序API构建图像分类器

