# 第十章 Keras人工神经网络简介

人工神经网络是深度学习的核心。它们用途广泛、功能强大且可扩展，使其**非常适合处理大型和高度复杂的机器学习任务**，例如对数十亿张图像进行分类（例如Google Images），为语音识别服务（例如Apple的Siri）提供支持，每天向成千上万的用户推荐（例如YouTube）观看的最佳视频，或学习在围棋游戏（DeepMind的AlphaGo）中击败世界冠军。

本章的第一部分介绍了人工神经网络，首先是对第一个ANN架构的快速浏览，然后是今天广泛使用的多层感知机（MLP）（其他架构将在第11章中进行探讨）。在第二部分中，我们将研究如何使用流行的Keras API实现神经网络。这是设计精巧、简单易用的用于构建、训练、评估和运行神经网络的API。但是，不要被它的简单性所迷惑：它的表现力和灵活性足以让你构建各种各样的神经网络架构。实际上，对于大多数示例而言，这可能就足够了。如果你需要额外的灵活性，可以随时使用其较低级的API编写自定义的Keras组件，这将在第12章中讨论。

## 10.1 从生物神经元到人工神经元

McCulloch和Pitts在其具有里程碑意义的论文“*A Logical Calculus of Ideas Immanent in Nervous Activity*”中，提出了一种简化的计算模型，该模型计算了生物神经元如何在动物大脑中协同工作，利用命题逻辑进行复杂的计算。这是第一个人工神经网络架构。

人们对人工神经网络重新充满兴趣将对我们的生活产生更深远的影响：

- 现在有大量数据可用于训练神经网络，并且在非常大和复杂的问题上，人工神经网络通常优于其他机器学习技术。

- 自20世纪90年代以来，计算能力的飞速增长使得现在有可能在合理的时间内训练大型神经网络。

- 训练算法已得到改进。

- 在实践中，人工神经网络的一些理论局限性被证明是良性的。例如，许多人认为ANN训练算法注定要失败，因为它们可能会陷入局部最优解，但事实证明，这在实践中相当罕见。

- 人工神经网络似乎已经进入了资金和发展的良性循环。

### 10.1.1 生物神经元

生物神经元产生短的电脉冲称为动作电位（AP，或只是信号），它们沿着轴突传播，使突触释放称为神经递质的化学信号。当神经元在几毫秒内接收到足够数量的这些神经递质时，它会激发自己的电脉冲。

![fig01_生物神经元](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/fig01_%E7%94%9F%E7%89%A9%E7%A5%9E%E7%BB%8F%E5%85%83.jpg)

因此，单个生物神经元的行为似乎很简单，但是它们组成了数十亿个庞大的网络，每个神经元都与数千个其他神经元相连。高度复杂的计算可以通过相当简单的神经元网络来执行，就像复杂的蚁丘可以通过简单蚂蚁的共同努力而出现一样。生物神经网络（BNN）的架构仍是活跃的研究主题，但大脑的某些部分已被绘制成图，似乎神经元通常组织成连续的层，尤其是在大脑皮层中（大脑的外层），如图2所示。

![fig02_生物神经网络](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/fig02_%E7%94%9F%E7%89%A9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.jpg)

### 10.1.2 神经元的逻辑计算

McCulloch和Pitts提出了一个非常简单的生物神经元模型，该模型后来被称为**神经元(artificial neuron)**。它具有*一个或多个二进制（开/关）输入和一个二进制输出*。当超过一定数量的输入处于激活状态时，人工神经元将激活其输出。他们的论文表明即使使用这样的简化模型，也可以构建一个人工神经元网络来计算所需的任何逻辑命题。为了了解这种网络的工作原
理，让我们构建一些执行各种逻辑计算的ANN（见图3），假设神经元至少两个输入处于激活状态时，神经元就会被激活。

![fig03_ANN执行简单的逻辑运算](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/fig03_ANN%E6%89%A7%E8%A1%8C%E7%AE%80%E5%8D%95%E7%9A%84%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97.jpg)

网络的作用如下所述：

- 左边的第一个网络是恒等函数：如果神经元A被激活，那么神经元C也被激活（因为它从神经元A接收到两个输入信号）；如果神经元A关闭，那么神经元C也关闭。

- 第二个网络执行逻辑AND：仅当神经元A和B都被激活（单个输入信号不足以激活神经元C）时，神经元C才被激活。

- 第三个网络执行逻辑OR：如果神经元A或神经元B被激活（或两者都激活），则神经元C被激活。

- 最后，如果我们假设输入连接可以抑制神经元的活动（生物神经元就是这种情况），则第四个网络计算出一个稍微复杂的逻辑命题：只有在神经元A处于活动状态和神经元B关闭时，神经元C才被激活。如果神经元A一直处于活动状态，那么你会得到逻辑NOT：神经元B关闭时神经元C处于活动状态，反之亦然。

### 10.1.3 感知器

**感知器(Perceptron)**是最简单的ANN架构之一，由Frank Rosenblatt于1957年发明。它基于稍微不同的人工神经元（见图4），称为**阈值逻辑单元（TLU）**，有时也称为**线性阈值单元（LTU）**。输入和输出是数字（而不是二进制开/关值），并且每个输入连接都与权重相关联。TLU计算其输入的加权和（ $z=w_1x_1+w_2x_2+...+w_nx_n=x^Tw$），然后将**阶跃函数**应用于该和并输出结果：$h_w(x)=step(z)$，其中 $z=x^Tw$。

![fig04_阈值逻辑单元](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/fig04_%E9%98%88%E5%80%BC%E9%80%BB%E8%BE%91%E5%8D%95%E5%85%83.jpg)

感知器中最常用的阶跃函数是**Heaviside阶跃函数**（见公式10-1）。有时使用符号函数代替。

![fig05_公式10-1_Heaviside阶跃函数](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/fig05_%E5%85%AC%E5%BC%8F10-1_Heaviside%E9%98%B6%E8%B7%83%E5%87%BD%E6%95%B0.jpg)

单个TLU可用于简单的线性二进制分类。它计算输入的线性组合，如果结果超过阈值，则输出正类；否则，它将输出负类（就像逻辑回归或线性SVM分类器一样）。例如，你可以使用单个TLU根据花瓣的长度和宽度对鸢尾花进行分类（就像我们在前面的章节中所做的那样，还添加了额外的偏置特征 $x_0=1$）。在这种情况下，训练TLU意味着找到$w_0$、$w_1$和$w_2$的正确值（稍后将讨论训练算法）。

**感知器仅由单层TLU组成**，每个TLU连接到所有的输入。当一层中的所有神经元都连接到上一层中的每个神经元（即其输入神经元）时，该层称为**全连接层或密集层**。感知器的输入被送到称为**输入神经元**的特殊直通神经元：它们输出被送入的任何输入。**所有输入神经元形成输入层**。此外，通常会添加一个额外的偏置特征（$x_0=1$）：通常使用一种称为**偏置神经元**的特殊类型的神经元来表示该特征，该神经元始终输出1。具有两个输入和三个输出的感知器如图5所示。**该感知器可以将实例同时分为三个不同的二进制类，这使其成为多输出分类器**。

![fig06_感知器架构](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/fig06_%E6%84%9F%E7%9F%A5%E5%99%A8%E6%9E%B6%E6%9E%84.jpg)

借助线性代数的魔力，公式10-2使得可以同时为多个实例高效地计算出一层人工神经元的输出。

![fig07_计算全连接层的输出](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/fig07_%E8%AE%A1%E7%AE%97%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E7%9A%84%E8%BE%93%E5%87%BA.jpg)

其中，

- X代表输入特征的矩阵。每个实例一行，每个特征一列。

- 权重矩阵W包含除偏置神经元外的所有连接权重。在该层中，每个输入神经元一行，每个人工神经元一列。

- 偏置向量b包含偏置神经元和人工神经元之间的所有连接权重。每个人工神经元有一个偏置项。

- 函数φ称为**激活函数**：当人工神经元是TLU时，它是阶跃函数（但我们在后面会讨论其他激活函数）。

那么，感知器如何训练？Rosenblatt提出的感知器训练算法在很大程度上受Hebb规则启发。Donald Hebb在其1949年的The Organization of Behavior（Wiley）中提出，当一个生物神经元经常触发另一个神经元时，这两个神经元之间的联系就会增强。后来，Siegrid Löwel用有名的措辞概括了Hebb的思想，即“触发的细胞，连接在一起”。也就是说，**两个神经元同时触发时，它们之间的连接权重会增加**。该规则后来被称为Hebb规则（或Hebb学习）。使用此规则的变体训练感知器，该变体考虑了网络进行预测时所犯的错误。感知器学习规则加强了有助于减少错误的连接。更具体地说，感知器一次被送入一个训练实例，并且针对每个实例进行预测。对于产生错误预测的每个输出神经元，它会增强来自输入的连接权重，这些权重将有助于正确的预测。该规则如公式3所示。

![fig08_公式10-3_感知器学习规则](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter10/figures/fig08_%E5%85%AC%E5%BC%8F10-3_%E6%84%9F%E7%9F%A5%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%A7%84%E5%88%99.jpg)

其中：

- $w_{i,j}$是第i个输入神经元和第j个输出神经元之间的连接权重；

- $x_i$是当前训练实例的第i个输入值；

- $\haty_j$是当前训练实例的第j个输出神经元的输出。

- $y_j$是当前训练实例的第j个输出神经元的目标输出。

- $η$是学习率。

每个**输出神经元的决策边界都是线性的**，因此感知器无法学习复杂的模式（就像逻辑回归分类器一样）。但是，如果训练实例是线性可分的，osenblatt证明了该算法将收敛到一个解。这被称为**感知器收敛定理**。

Scikit-Learn提供了一个Perceptron类，该类实现了单个TLU网络。它可以像你期望的那样使用，例如，在鸢尾植物数据集上：

```python
    import numpy as np
    from sklearn.datasets import load_iris
    from sklearn.linear_model import Perceptron

    iris = load_iris()
    X = iris.data[:, (2, 3)]  # petal length, petal width
    y = (iris.target == 0).astype(np.int)

    per_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=42)
    per_clf.fit(X, y)

    y_pred = per_clf.predict([[2, 0.5]])
```
你可能已经注意到，感知器学习算法非常类似于随机梯度下降。实际上，Scikit-Learn的Perceptron类等效于使用具有以下超参数的 `SGDClassifier: loss="perceptron"`，`learning_rate="constant"`，`eta0=1`（学习率）和`penalty=None`（无正则化））。请注意，与逻辑回归分类器相反，感知器不输出分类概率；相反，它们基于硬阈值进行预测。这是逻辑回归胜过感知器的原因。

Marvin Minsky和Seymour Papert在1969年的专著Perceptron中，特别指出了感知器的一些严重缺陷，即它们无法解决一些琐碎的问题（例如，异或（XOR）分类问题，参见图9的左侧）。任何其他线性分类模型（例如逻辑回归分类器）都是如此，但是研究人员对感知器的期望更高，有些人感到失望，他们完全放弃了神经网络，转而支持更高层次的问题，例如逻辑、问题求解和搜索。

![fig09_XOR分类问题和解决该问题的MLP]()

事实证明，可以通过堆叠多个感知器来消除感知器的某些局限性。所得的ANN称为**多层感知器（MLP）**。MLP可以解决XOR问题，你可以通过计算图9右侧所示的MLP的输出来验证：输入(0,0)或(1,1)，网络输出0，输入(0,1)或(1,0)则输出1。所有连接的权重等于1，但显示权重的四个连接除外。尝试验证该网络确实解决了XOR问题！

### 10.1.4 多层感知器和反向传播

MLP由一层（直通）输入层、一层或多层TLU（称为隐藏层）和一个TLU的最后一层（称为输出层）组成（见图10）。靠近输入层的层通常称为**较低层**，靠近输出层的层通常称为**较高层**。除输出层外的每一层都包含一个偏置神经元，并完全连接到下一层。

信号仅沿一个方向（从输入到输出）流动，因此该架构是**前馈神经网络（FNN）**的示例。

![fig10_多层感知器架构]()

