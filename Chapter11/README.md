# 第十一章 训练深度神经网络

在第10章中，我们介绍了人工神经网络并训练了第一个深度神经网络。但是它们是浅层网络，只有几个隐藏层。如果你需要解决一个复杂的问题，例如检测高分辨率图像中的数百种物体，该怎么办？你可能需要训练更深的DNN，也许10层或更多层，每层包含数百个神经元，有成千上万个连接。训练深度DNN并不是在公园里散步。以下是你可能会遇到的一些问题：

- 你可能会遇到棘手的梯度消失问题或相关的梯度爆炸问题。这是在训练过程中通过DNN反向传播时，梯度变得越来越小或越来越大时发生的。这两个问题都使得较低的层很难训练。

- 对于如此大的网络，你可能没有足够的训练数据，或者做标签的成本太高。

- 训练可能会非常缓慢。

- 具有数百万个参数的模型会有严重过拟合训练集的风险，尤其是在没有足够的训练实例或噪声太大的情况下。

在本章中，我们将研究所有这些问题，并介绍解决这些问题的技术。我们从探索梯度消失和梯度爆炸问题及其一些受欢迎的解决方法开始。接下来，我们将研究迁移学习和无监督预训练，即使在标签数据很少的情况下，它们也可以帮助你解决复杂的任务。然后我们将讨论可以极大加速训练大型模型的各种优化器。最后我们将介绍一些流行的针对大型神经网络的正则化技术。

使用这些工具，你就能够训练非常深的网络。欢迎使用深度学习！

## 11.1 梯度消失与梯度爆炸问题

正如我们在第10章中讨论的那样，反向传播算法的工作原理是**从输出层到输入层，并在此过程中传播误差梯度**。一旦算法计算出成本函数相对于网络中每个参数的梯度，就可以使用这些梯度以梯度下降步骤来更新每个参数。

不幸的是，随着算法向下传播到较低层，梯度通常会越来越小。结果梯度下降更新使较低层的连接权重保持不变，训练不能收敛到一个良好的解。我们称其为**梯度消失问题**。在某些情况下，可能会出现相反的情况：梯度可能会越来越大，各层需要更新很大的权重直到算法发散为止。这是**梯度爆炸问题**，它出现在递归神经网络中（见第15章）。更笼统地说，深度神经网络很受梯度不稳定的影响，不同的层可能以不同的速度学习。

这些问题很久以前就凭经验观察到了，这也是深度神经网络在2000年代初期被大量抛弃的原因之一。目前尚不清楚是什么原因导致在训练DNN时使梯度如此不稳定，但是Xavier Glorot和Yoshua Bengio在2010年的一篇论文中阐明了一些观点。作者发现了一些疑点，包括流行的逻辑sigmoid激活函数和当时最流行的权重初始化技术（即平均值为0且标准差为1的正态分布）。简而言之，它们表明使用此激活函数和此初始化方案，每层输出的方差远大于其输入的方差。随着网络的延伸，方差在每一层之后都会增加，直到激活函数在顶层达到饱和为止。实际上，由于逻辑函数的平均值为0.5，而不是0（双曲线正切函数的平均值为0，在深度网络中的表现比逻辑函数稍微好一些），因此饱和度实际上变得更差。

查看逻辑激活函数（见图1），你可以看到，当输入变大（负数或正数）时，该函数会以0或1饱和，并且导数非常接近0。因此反向传播开始时它几乎没有梯度可以通过网络传播回去。当反向传播通过顶层向下传播时，存在的小梯度不断被稀释，因此对于底层来说，实际上什么也没有留下。

![fig01_逻辑激活函数的饱和](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter11/figures/fig01_%E9%80%BB%E8%BE%91%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E9%A5%B1%E5%92%8C.jpg)

### 11.1.1 Glorot和He初始化

Glorot和Bengio在它们的论文中提出了一种能显著缓解不稳定梯度问题的方法。他们指出，我们需要信号在两个方向上正确流动：**进行预测时，信号为正向；在反向传播梯度时，信号为反向**。我们既不希望信号消失，也不希望它爆炸并饱和。为了使信号正确流动，作者认为，我们需要每层输出的方差等于其输入的方差，并且我们需要在反方向时流过某层之前和之后的梯度具有相同的方差（如果你对数学细节感兴趣，请查看本论文）。除非该层具有相等数量的输入和神经元（这些数字称为该层的扇入和扇出），否则实际上不可能同时保证两者，但是Glorot和Bengio提出了一个很好的折中方案，在实践中证明很好地发挥作用：必须按照公式11-1中所述的随机初始化每层的连接权重，其中$fan_avg=(fan_in+fan_out)/2$。这种初始化策略称为Xavier初始化或者Glorot初始化，以论文的第一作者命名。

![fig02_公式11-1_Glorot初始化](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter11/figures/fig02_%E5%85%AC%E5%BC%8F11-1_Glorot%E5%88%9D%E5%A7%8B%E5%8C%96.jpg)

如果在公式11-1中你用$fan_in$替换$fan_avg$，则会得到Yann LeCun在20世纪90年代提出的初始化策略。他称其为LeCun初始化。Genevieve Orr和laus-Robert Müller甚至在其1998年出版的Neural Networks：Tricks of the Trade（Springer）一书中进行了推荐。当fanin=fanout时，LeCun初始化等效于Glorot初始化。研究人员花了十多年的时间才意识到这一技巧的重要性。使用Glorot初始化可以大大加快训练速度，这是导致深度学习成功的诀窍之一。

一些论文为不同的激活函数提供了类似的策略。这些策略的差异仅在于方差的大小以及它们使用的是$fan_avg$还是$fan_in$，如表11-1所示（对于均匀分布，只需计算$r=\sqrt{3\sigma^2}$）。ReLU激活函数的初始化策略（及其变体，包括ELU激活函数）有时简称为He初始化。本章稍后将解释SELU激活函数。它应该与LeCun初始化一起使用（最好与正态分布一起使用，如我们所见）

表11-1：每种激活函数的初始化参数

|初始化|激活函数|$\sigma^2$(正常)|
|:---:|:---:|:---:|
|Glorot|None、tanh、logistic、softmax|$1/fan_{avg}$|
|He|ReLU和变体|$2/fan_{in}$|
|LeCun|SELU|$1/fan_{in}$|

默认情况下，Keras使用具有均匀分布的Glorot初始化。创建层时，可以通过设置`kernel_initializer="he_uniform"`或`kernel_initializer="he_normal"`来将其更改为He初始化：

```python
    keras.layers.Dense(10, activation="relu", kernel_initializer="he_normal")
```

如果你要使用均匀分布但基于fanavg而不是fanin进行He初始化，则可以使用Variance Scaling初始化，如下所示：

```python
    init = keras.initializers.VarianceScaling(scale=2.,      mode='fan_avg', distribution='uniform')
    keras.layers.Dense(10, activation="relu", kernel_initializer=init)
```

### 11.1.2 非饱和激活函数

Glorot和Bengio在2010年的论文中提出的一项见解是，梯度不稳定的问题部分是**由于激活函数选择不当所致**。在此之前，大多数人都认为，如果大自然母亲选择在生物神经元中使用类似sigmoid的激活函数，那么它们必定是一个好选择。但是事实证明，其他激活函数在深度神经网络中的表现要好得多，尤其是ReLU激活函数，这主要是因为它对正值不饱和（并且计算速度很快）。

不幸的是，ReLU激活函数并不完美。它有一个被称为“濒死的ReLU”的问题：在训练过程中，某些神经元实际上“死亡”了，这意味着它们停止输出除0以外的任何值。在某些情况下，你可能会发现网络中一半的神经元都死了，特别是如果你使用较大的学习率。当神经元的权重进行调整时，其输入的加权和对于训练集中的所有实例均为负数，神经元会死亡。发生这种情况时，它只会继续输出零，梯度下降不会再影响它，因为**ReLU函数的输入为负时其梯度为零**。

要解决此问题，你可能需要使用ReLU函数的变体，例如leaky ReLU。该函数定义为$LeakyReLU_α(z)=max(αz,z)$（见图3）。超参数α定义函数“泄漏”的程度：它是z<0时函数的斜率，通常设置为0.01。这个小的斜率确保了leaky ReLU永远不会死亡。

它们可能会陷入长时间的昏迷，但是有机会最后醒来。2015年的一篇论文比较了ReLU激活函数的几种变体，其结论之一是泄漏的变体要好于严格的ReLU激活函数。实际上，设置α=0.2（大泄漏）似乎比α=0.01（小泄漏）会产生更好的性能。本论文还对随机的Leaky ReLU（RReLU）进行了评估，在训练过程中在给定范围内随机选择α，在测试过程中将其固定为平均值。RReLU的表现也相当不错，似乎可以充当正则化函数（减少了过拟合训练集的风险）。最后，本文评估了参数化leaky ReLU（PReLU），其中α可以在训练期间学习（不是超参数，它像其他任何参数一样，可以通过反向传播进行修改）。据报道，PReLU在大型图像数据集上的性能明显优于ReLU，但是在较小的数据集上，它存在过拟合训练集的风险。

![fig03_Leaky ReLU](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter11/figures/fig03_Leaky%20ReLU.jpg)

最后但并非最不重要的一点是，Djork-ArnéClevert等人在2015年发表的论文提出了一种新的激活函数，称为指数线性单位（Exponential Linear Unit，ELU），该函数在作者的实验中胜过所有ReLU变体：减少训练时间，神经网络在测试集上表现更好。图5绘制了函数图，公式11-2给出了其定义。

![fig04_公式11-2_ELU激活函数](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter11/figures/fig04_%E5%85%AC%E5%BC%8F11-2_ELU%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.jpg)

![fig05_ELU激活函数](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter11/figures/fig05_ELU%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.jpg)

ELU激活函数与ReLU函数非常相似，但有一些主要区别：

- 当z<0时，它取负值，这使该单元的平均输出接近于0，有助于缓解梯度消失的问题。超参数α定义一个值，该值为当z为较大负数时ELU函数逼近的值。通常将其设置为1，但是你可以像其他任何超参数一样对其进行调整。

- 对于z<0，它具有非零梯度，从而避免了神经元死亡的问题。

- 如果α等于1，则该函数在所有位置（包括z=0左右）都是平滑的，这有助于加速梯度下降，因为它在z=0的左右两侧弹跳不大。

ELU激活函数的主要缺点是它的计算比ReLU函数及其变体要慢（由于使用了指数函
数）。它在训练过程中更快的收敛速度弥补了这种缓慢的计算，但是在测试时，ELU网络将比ReLU网络慢。

然后，Günter Klambauer等人在2017年发表的论文提出了可扩展的ELU(Scaled ELU，SEIU）激活函数：顾名思义，它是ELU激活函数的可扩展变体。作者表明，如果你构建一个仅由密集层堆叠组成的神经网络，并且如果所有隐藏层都使用SELU激活函数，则该网络是自归一化的：每层的输出倾向于在训练过程中保留平均值0和标准差1，从而解决了梯度消失/梯度爆炸的问题。结果，SELU激活函数通常大大优于这些神经网络（尤其是深层神经网络）的其他激活函数。但是，有一些产生自归一化的条件（有关数学证明，请参见论文）：

- 输入特征必须是标准化的（平均值为0，标准差为1）。

- 每个隐藏层的权重必须使用LeCun正态初始化。在Keras中，这意味着设置`kernel_initializer="lecun_normal"`。

- 网络的架构必须是顺序的。不幸的是，如果你尝试在非顺序架构（例如循环网络）中使用SELU（见第15章）或具有跳过连接的网络（即在Wide&Deep网络中跳过层的连接），将无法保证自归一化，因此SELU不一定会胜过其他激活函数。

- 本论文仅在所有层都是密集层的情况下保证自归一化，但一些研究人员指出SELU激活函数也可以改善卷积神经网络的性能（见第14章）

那么，你应该对深度神经网络的隐藏层使用哪个激活函数呢？尽管你的目标会
有所不同，但通常SELU>ELU>leaky ReLU（及其变体）>ReLU>tanh>logistic。如果网络的架构不能自归一化，那么ELU的性能可能会优于SELU（因为SELU在z=0时不平滑）。如果你非常关心运行时延迟，那么你可能更喜欢leaky ReLU。如果你不想调整其他超参数，则可以使用Keras使用的默认α值（例如，leaky ReLU为0.3）。如果你有空闲时间和计算能力，则可以使用交叉验证来评估其他激活函数，例如，如果网络过拟合，则为RReLU；如果你的训练集很大，则为PReLU。也就是说，由于ReLU是迄今为止最常用的激活函数，因此许多库和硬件加速器都提供了ReLU特定的优化。因此，如果你将速度放在首位，那么ReLU可能仍然是最佳选择。要使用leaky ReLU激活函数，创建一个LeakyReLU层，并将其添加到你想要应用它的层之后的模型中：

对于PReLU，将LeakyRelu（alpha=0.2）替换为PReLU()。Keras当前没有RReLU的官方实现，但是你可以轻松地实现自己的（要了解如何实现，请参阅第12章末尾的练习）。

对于SELU激活，在创建层时设置`activation="selu"`和`kernel_initializer="lecun_normal"`：

### 11.1.3 批量归一化

尽管将He初始化与ELU（或ReLU的任何变体）一起使用可以显著减少在训练开始时的梯度消失/梯度爆炸问题的危险，但这并不能保证它们在训练期间不会再出现。

在2015年的一篇论文中，Sergey Ioffe和Christian Szegedy提出了一种称为批量归一化（BN）的技术来解决这些问题。该技术包括在模型中的每个隐藏层的激活函数之前或之后添加一个操作。该操作对每个输入零中心并归一化，然后每层使用两个新的参数向量缩放和偏移其结果：一个用于缩放，另一个用于偏移。换句话说，该操作可以使模型学习各层输入的最佳缩放和均值。在许多情况下，如果你将BN层添加为神经网络的第一层，则无须归一化训练集（例如，使用StandardScaler）；BN层会为你完成此操作（因为它一次只能查看一个批次，它还可以重新缩放和偏移每个输入特征）。

为了使输入零中心并归一化，该算法**需要估计每个输入的均值和标准差**。通过评估当前小批次上的输入的均值和标准差（因此称为“批量归一化”）来做到这一点。公式11-3逐步总结了整个操作。

![fig06_公式11-3_批量归一化算法](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter11/figures/fig06_%E5%85%AC%E5%BC%8F11-3_%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%E7%AE%97%E6%B3%95.jpg)

此算法中：

- $\mu_B$是输入均值的向量，在整个小批量B上评估（每个输入包含一个均值）。

- $\sigma_B$是输入标准差的向量，也在整个小批量中进行评估（每个输入包含一个标准差）。

- $m_B$是小批量中的实例数量。

- $x_{(i)}$是实例i的零中心和归一化输入的向量。

- γ是该层的输出缩放参数向量（每个输入包含一个缩放参数）。

- ⊗表示逐元素乘法（每个输入乘以其相应的输出缩放参数）。

- β是层的输出移动（偏移）参数向量（每个输入包含一个偏移参数）。每个输入都通过其相应的移动参数进行偏移。

- ε是一个很小的数字以避免被零除（通常为10-5）。这称为平滑项。

- $z^{(i)}$是BN操作的输出。它是输入的缩放和偏移版本。

因此在训练期间，BN会归一化其输入，然后重新缩放并偏移它们。好！那在测试期间呢？这不那么简单。确实，我们可能需要对单个实例而不是成批次的实例做出预测：在这种情况下，我们无法计算每个输入的均值和标准差。而且，即使我们确实有一批次实例，它也可能太小，或者这些实例可能不是独立的和相同分布的，因此在这批实例上计算统计信息将是不可靠的。一种解决方法是等到训练结束，然后通过神经网络运行整个训练集，计算BN层每个输入的均值和标准差。然后，在进行预测时，可以使用这些“最终”的输入均值和标准差，而不是一个批次的输入均值和标准差。然而，大多数批量归一化的实现都是通过使用该层输入的均值和标准差的移动平均值来估计训练期间的最终统计信息。这是Keras在使用BatchNormalization层时自动执行的操作。综上所述，在每个批归一化层中学习了四个参数向量：通过常规反向传播学习γ（输出缩放向量）和β（输出偏移向量），和使用指数移动平均值估计的μ（最终的输入均值向量）和σ（最终输入标准差向量）。请注意，μ和σ是在训练期间估算的，但仅在训练后使用（以替换方程式11-3中的批量输入均值和标准差）。

Ioffe和Szegedy证明，批量归一化极大地改善了他们试验过的所有深度神经网络，从而极大地提高了ImageNet分类任务的性能（ImageNet是将图像分类为许多类的大型图像数据库，通常用于评估计算机视觉系统）。消失梯度的问题已大大减少，以至于它们可以使用饱和的激活函数，例如tanh甚至逻辑激活函数。网络对权重初始化也不太敏感。作者可以使用更大的学习率，大大加快了学习过程。它们特别指出：

批量归一化应用于最先进的图像分类模型，以少14倍的训练步骤即可达到相同的精度，在很大程度上击败了原始模型......使用批量归一化网络的集成，我们在ImageNet分类中改进了已发布的最好结果：前5位的验证错误达到了4.9％（和4.8％的测试错误），超过了人工评分者的准确性。

最后，就像不断赠送的礼物一样，批量归一化的作用就像正则化一样，减少了对其他正则化技术（如dropout，本章稍后将介绍）的需求。

但是，批量归一化确实增加了模型的复杂性（尽管它可以消除对输入数据进行归一化的需求，正如我们前面所讨论的）。此外，还有运行时间的损失：由于每一层都需要额外的计算，因此神经网络的预测速度较慢。幸运的是，经常可以在训练后将BN层与上一层融合，从而避免了运行时的损失。这是通过更新前一层的权重和偏置来完成的，以便它直接产生适当缩放和偏移的输出。例如，如果前一层计算$XW+b$，则BN层将计算$γ⊗(XW+b–μ)/σ+β$（忽略分母中的平滑项ε）。如果我们定义$W^′=γ⊗W/σ$和$b^′=γ⊗(b–μ)/σ+β$，则方程式可简化为$XW^′+b^′$。因此，如果我们用更新后的权重和偏置（$W^′$和$b^′$）替换前一层的权重和偏置（W和b），就可以去掉BN层（TFLite的优化器会自动执行此操作，请参阅第19章）。

你可能会发现训练相当慢，因为当你使用批量归一化时每个轮次要花费更多时间。通常情况下，这被BN的收敛速度要快得多的事实而抵消，因此达到相同性能所需的轮次更少，总而言之，墙上的时间通常会更短（这是墙上的时钟所测量的时间）。

#### 用Keras实现批量归一化

与使用Keras进行的大多数操作一样，实现批量归一化既简单又直观。只需在每个隐藏层的激活函数之前或之后添加一个BatchNormalization层，然后可选地在模型的第一层后添加一个BN层。例如，此模型在每个隐藏层之后作为模型的第一层（展平输入图像之后）应用BN：

```python
    model = keras.models.Sequential([
        keras.layers.Flatten(input_shape=[28, 28]),
        keras.layers.BatchNormalization(),
        keras.layers.Dense(300, activation="relu"),
        keras.layers.BatchNormalization(),
        keras.layers.Dense(100, activation="relu"),
        keras.layers.BatchNormalization(),
        keras.layers.Dense(10, activation="softmax")
    ])
```

就这样！在这个只有两个隐藏层的小示例中，批量归一化不可能产生非常积极的影响。但是对于更深层的网络，它可以带来巨大的改变。

如你所见，每个BN层的每个输入添加了四个参数：γ、β、μ和σ（例如，第一个BN层添加了3136个参数，即4×784）。最后两个参数μ和σ是移动平均值。它们不受反向传播的影响，因此Keras称其为“不可训练”（如果你计算BN参数的总数3136+1200+400，然后除以2，则得到2368，即此模型中不可训练参数的总数）。

让我们看一下第一个BN层的参数。两个是可训练的（通过反向传播），两个不是：

```python
    >>> [(var.name, var.trainable) for var in model.layers[1].variables]
    [('batch_normalization_v2/gamma:0', True),
    ('batch_normalization_v2/beta:0', True),
    ('batch_normalization_v2/moving_mean:0', False),
    ('batch_normalization_v2/moving_variance:0', False)]
```

现在，当你在Keras中创建BN层时，它还会创建两个操作，在训练期间的每次迭代中，Keras都会调用这两个操作。这些操作会更新移动平均值。由于我们使用的是TensorFlow后端，因此这些操作是TensorFlow操作（我们将在第12章中讨论TF操作）：

```python
    >>> model.layers[1].updates
    [<tf.Operation 'cond_2/Identity' type=Identity>,
    <tf.Operation 'cond_3/Identity' type=Identity>]
```

BN论文的作者主张在激活函数之前（而不是之后）添加BN层（就像我们刚才所做的那样）。关于此问题，存在一些争论，哪个更好取决于你的任务——你也可以对此进行试验，看看哪个选择最适合你的数据集。要在激活函数之前添加BN层，必须从隐藏层中删除激活函数，并将其作为单独的层添加到BN层之后。此外，由于批量归一化层的每个输入都包含一个偏移参数，因此你可以从上一层中删除偏置项（创建时只需传递use_bias=False即可）：

```python
    model = keras.models.Sequential([
        keras.layers.Flatten(input_shape=[28, 28]),
        keras.layers.BatchNormalization(),
        keras.layers.Dense(300, use_bias=False),
        keras.layers.BatchNormalization(),
        keras.layers.Activation("relu"),
        keras.layers.Dense(100, use_bias=False),
        keras.layers.BatchNormalization(),
        keras.layers.Activation("relu"),
        keras.layers.Dense(10, activation="softmax")
    ])
```

BatchNormalization类具有许多可以调整的超参数。默认值通常可以，但是你偶尔可能需要调整omentum。BatchNormalization层在更新指数移动平均值时使用此超参数。给定一个新值v（即在当前批次中计算的输入均值或标准差的新向量），该层使用以下公式来更新运行时平均$\hat v$：

$\hat v  \leftarrow \hat v \times momentum+v \times (1-momentum)$

一个良好的动量值通常接近1；例如0.9、0.99或0.999（对于较大的数据集和较小的批处理，你需要更多的9）。

另一个重要的超参数是axis：它确定哪个轴应该被归一化。默认为-1，这意味着默认情况下它将对最后一个轴进行归一化（使用跨其他轴计算得到的均值和标准差）。当输入批次为2D（即批次形状为[批次大小，特征]）时，这意味着将基于在批次中所有实例上计算得到的均值和标准差对每个输入特征进行归一化。例如，先前代码示例中的第一个BN层将独立地归一化（重新缩放和偏移）784个输入特征中的每一个。如果将第一个BN层移动到Flatten层之前，则输入批次将为3D，形状为[批次大小，高度，宽度]。因此，BN层将计算28个均值和28个标准差（每列像素1个，在批次中的所有实例以及在列中所有行之间计算），它将使用相同的均值和标准差对给定列中的所有像素进行归一化。也是只有28个缩放参数和28个偏移参数。相反，你如果仍然要独立的处理784个像素中的每一个，则应设置axis=[1，2]。

请注意，BN层在训练期间和训练后不会执行相同的计算：它在训练期间使用批处理统计信息，在训练后使用“最终”的统计信息（即移动平均的最终值）。让我们看一下这个类的源代码，看看如何处理：

```python
class BatchNormalization(keras.layers.Layer):
    [...]
    def call(self, inputs, training=None):
        [...]
```

如你所见，`call()`方法是执行计算的方法，它有一个额外的训练参数，默认情况下将其设置为None，但是在训练过程中`fit()`方法将其设置为1。如果你需要编写自定义层，它的行为在训练期间和测试期间必须有所不同，把一个训练参数添加到`call()`方法中，并在该方法中使用这个参数来决定要计算的内容（我们将在第12章中讨论自定义层）。

BatchNormalization已成为深度神经网络中最常用的层之一，以至于在图表中通常将其省略，因为假定在每层之后都添加了BN。但是Hongyi Zhang等人最近的论文可能会改变这一假设：通过使用一种新颖的fixed-update(fixup)权重初始化技术，作者设法训练了一个非常深的神经网络（10000层！），没有使用BN，在复杂的图像分类任务上实现了最先进的性能。但是，由于这是一项前沿研究，因此你在放弃批量归一化之前，可能需要等待其他研究来确认此发现。

### 11.1.4 梯度裁剪

缓解梯度爆炸问题的另一种流行技术是在反向传播期间裁剪梯度，使它们永远不会超过某个阈值。这称为梯度裁剪。这种技术最常用于循环神经网络，因为在RNN中难以使用批量归一化，正如我们将在第15章中看到的那样。对于其他类型的网络，BN通常就足够了。

在Keras中，实现梯度裁剪仅仅是一个在创建优化器时设置clipvalue或clipnorm参数的问题，例如：

```python
    optimizer = keras.optimizers.SGD(clipvalue=1.0)
    model.compile(loss="mse", optimizer=optimizer)
```

该优化器会将梯度向量的每个分量都裁剪为-1.0和1.0之间的值。这意味着所有损失的偏导数（相对于每个可训练的参数）将限制在-1.0和1.0之间。阈值是你可以调整的超参数。注意，它可能会改变梯度向量的方向。例如，如果原始梯度向量为[0.9，100.0]，则其大部分指向第二个轴的方向。但是按值裁剪后，将得到[0.9，1.0]，该点大致指向两个轴之间的对角线。实际上，这种方法行之有效。如果要确保“梯度裁剪”不更改梯度向量的方向，你应该通过设置clipnorm而不是clipvalue按照范数来裁剪。如果2范数大于你选择的阈值，则会裁剪整个梯度。例如，如果你设置clipnorm=1.0，则向量[0.9，100.0]将被裁剪为[0.00899964，0.9999595]，保留其方向，但几乎消除了第一个分量。如果你观察到了在训练过程中梯度爆炸（可以使用TensorBoard跟踪梯度的大小），可能要尝试使用两种方法（按值裁剪和按范数裁剪），看看哪个选择在验证集上表现更好。

## 11.2 重用预训练层

从头开始训练非常大的DNN通常不是一个好主意：相反，你应该总是试图找到一个现有的与你要解决的任务相似的神经网络（我们将在第14章讨论如何找到它们），然后重用该网络的较低层。此技术称为迁移学习。它不仅会大大加快训练速度，而且会大大减少训练数据。

假设你可以访问一个经过训练的DNN，将图片分成100个不同的类别，其中包括动物、植物、车辆和日常物品。你现在想训练DNN来对特定类型的车辆进行分类。这些任务非常相似，甚至部分有重叠，因此你应该尝试重用第一个网络的一部分（见图7）。

![fig07_重用预训练层](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter11/figures/fig07_%E9%87%8D%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E5%B1%82.jpg)

如果新任务的输入图片的大小与原始任务中使用的图片不同，通常必须添加预处理步骤将其调整为原始模型所需的大小。一般而言，当输入具有类似的低级特征时，迁移学习最有效。

通常应该替换掉原始模型的输出层，因为它对于新任务很有可能根本没有用，甚至对于新任务而言，可能没有正确数量的输出。

类似地，原始模型的上部分隐藏层不太可能像下部分那样有用，因为对新任务最有用的高级特征可能与对原始任务最有用的特征有很大的不同。你需要找到正确的层数来重用。

任务越相似，可重用的层越多（从较低的层开始）。对于非常相似的任务，请尝试保留所有的隐藏层和只是替换掉输出层。

首先尝试冻结所有可重复使用的层（使其权重不可训练，这样梯度下降就不会对其进行修改），训练模型并查看其表现。然后尝试解冻上部隐藏层中的一两层，使反向传播可以对其进行调整，再查看性能是否有所提高。你拥有的训练数据越多，可以解冻的层就越多。当解冻重用层时，降低学习率也很有用，可以避免破坏其已经调整好的权重。

如果你仍然无法获得良好的性能，并且你的训练数据很少，试着去掉顶部的隐藏层，然后再次冻结所有其余的隐藏层。你可以进行迭代，直到找到合适的可以重复使用的层数。如果你有大量的训练数据，则可以尝试替换顶部的隐藏层而不是去掉它们，你甚至可以添加更多的隐藏层。

### 11.2.1 用Keras进行迁移学习

让我们看一个示例。假设Fashion MNIST数据集仅包含8个类别，例如，除凉鞋和衬衫之外的所有类别。有人在该数据集上建立并训练了Keras模型，并获得了相当不错的性能（精度>90％）。我们将此模型称为A。你现在要处理另一项任务：你有凉鞋和衬衫的图像，想要训练一个二元分类器（正=衬衫，负=凉鞋）。你的数据集非常小，只有200张带标签的图像。当你使用与模型A相同的架构训练一个新模型（称为模型B）时，它的性能相当好（97.2％的精度）。但是由于这是一项容易得多的任务（只有两个类），所以你希望有更多。喝早茶时，你意识到你的任务与任务A非常相似，也许通过迁移学习可以有所帮助？让我们找出答案！

首先，你需要加载模型A并基于该模型的层创建一个新模型。让我们重用除输出层之外的所有层：

```python
    model_A = keras.models.load_model("my_model_A.h5")
    model_B_on_A = keras.models.Sequential(model_A.layers[:-1])
    model_B_on_A.add(keras.layers.Dense(1, activation="sigmoid"))
```

请注意，model_A和model_B_on_A现在共享一些层。当训练odel_B_on_A时，也会影响model_A。如果想避免这种情况，需要在重用model_A的层之前对其进行克隆。为此，请使用clone_model()来克隆模型A的架构，然后复制其权重（因为clone_model()不会克隆权重）

```python
    model_A_clone = keras.models.clone_model(model_A)
    model_A_clone.set_weights(model_A.get_weights())
```

现在你可以为任务B训练model_B_on_A，但是由于新的输出层是随机初始化的，它会产生较大的错误（至少在前几个轮次内），因此将存在较大的错误梯度，这可能会破坏重用的权重。为了避免这种情况，一种方法是在前几个轮次时冻结重用的层，给新层一些时间来学习合理的权重。为此，请将每一层的可训练属性设置为False并编译模型：

```python
    for layer in model_B_on_A.layers[:-1]:
        layer.trainable = False

    model_B_on_A.compile(loss="binary_crossentropy",
                        optimizer=keras.optimizers.SGD(lr=1e-3),
                        metrics=["accuracy"])
```

冻结或解冻层后，你必须总是要编译模型。

现在，你可以训练模型几个轮次，然后解冻重用的层（这需要再次编译模型），并继续进行训练来微调任务B的重用层。解冻重用层之后，降低学习率通常是个好主意，可以再次避免损坏重用的权重：

```python
        history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,
                            validation_data=(X_valid_B, y_valid_B))

    for layer in model_B_on_A.layers[:-1]:
        layer.trainable = True

    model_B_on_A.compile(loss="binary_crossentropy",
                        optimizer=keras.optimizers.SGD(lr=1e-3),
                        metrics=["accuracy"])
    history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,
                            validation_data=(X_valid_B, y_valid_B))
```

那么，最终结果是什么？好了，该模型的测试精度为99.25％，这意味着迁移学习将错误率从2.8％降低到了几乎0.7％！那是四倍的差距！

```python
    model_B_on_A.evaluate(X_test_B, y_test_B)
        >>> 63/63 [==============================] - 0s 667us/step - loss: 0.0684 - accuracy: 0.9935
    [0.06837236881256104, 0.9934999942779541]
```

你被说服了吗？你不应该：我作弊了！我尝试了许多配置，直到发现一个有明显改进的配置。如果你试着更改类别或随机种子，会发现改进通常会下降，甚至消失或反转。我所做的就是“折磨数据直到信服为止”。当论文看起来过于优秀时，你应该要怀疑：也许这个浮华的新技术实际上并没有多大帮助（事实上，它甚至可能降低性能），但作者尝试了许多变体，仅报告了最好的结果（这可能是由于运气所致），而没有提及他们在途中遇到了多少次失败。在大多数情况下，这根本不是恶意的，但这是造成如此多的科学结果永远无法复现的部分原因。

我为什么要作弊？事实证明，迁移学习在小型密集型网络中不能很好地工作，大概是因为小型网络学习的模式很少，密集网络学习的是非常特定的模式，这在其他任务中不是很有用。迁移学习最适合使用深度卷积神经网络，该神经网络倾向于学习更为通用的特征检测器（尤其是在较低层）。我们将在第14章中使用刚刚讨论的技术重新审视迁移学习（我保证，这一次不会作弊！）。

### 11.2.2 无监督预训练

假设你要处理一个没有太多标签训练数据的复杂任务，但是不幸的是，你找不到在类似任务上训练的模型。不要失去希望！首先，你应该尝试收集更多带有标签的训练数据，但如果做不到，你仍然可以执行无监督预训练（见图8）。确实，收集未标记的训练实例通常很便宜，但标记它们却很昂贵。如果你可以收集大量未标记的训练数据，则可以尝试使用它们来训练无监督模型，例如自动编码器或GAN（见第17章）。然后，你可以重用自动编码器的较低层或GAN判别器的较低层，在顶部为你的任务添加输出层，并使用有监督学习（即带有标记的训练实例）来微调最终的网络。

Geoffrey Hinton和他的团队在2006年使用的正是这种技术，它导致了神经网络的复兴以及深度学习的成功。直到2010年，无监督预训练——通常使用受限的Boltzmann机器（RBM，见附录E）——是深度网络的标准，只有在梯度消失问题得到解决后，使用有监督学习来训练DNN才变得更加普遍。当你要解决的任务很复杂，没有可重复使用的相似模型，标签的训练数据很少，但是无标签的训练数据很多时，无监督预训练（今天通常使用自动编码器或GAN而不是RBM）仍然是一个不错的选择。

请注意，在深度学习的早期很难训练深度模型，因此人们会使用一种称为**贪婪逐层预训练**（greedy layer-wise pretraining）的技术（如图8所示）。它们首先训练一个单层的无监督模型，通常是RBM，然后冻结该层并在其之上添加另一个层，然后再次训练模型（实际上只是训练新层），然后冻结新层并在其上添加另一层，再次训练模型，以此类推。如今，事情变得简单多了：人们通常只用一次就可以训练完整的无监督模型（即在图8中，直接从第三步开始），并使用自动编码器或GAN（不是RBM）。

![fig08_无监督预训练](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter11/figures/fig08_%E6%97%A0%E7%9B%91%E7%9D%A3%E9%A2%84%E8%AE%AD%E7%BB%83.jpg)

*在无监督训练中，使用一个无监督学习技术对无标记数据（或所有数据）进行训练，然后使用一个有监督学习技术对有标记数据进行最后任务的微调，无监督部分可以一次训练一层，如图所示，也可以直接训练整个模型。*

### 11.2.3 辅助任务的预训练

如果你没有太多标记的训练数据，最后一个选择是在辅助任务上训练第一个神经网络，你可以轻松地为其获得或生成标记的训练数据，然后对实际任务重用该网络的较低层。第一个神经网络的较低层将学习特征检测器，第二个神经网络可能会重用这些特征检测器。

例如，如果你要构建一个识别人脸的系统，每个人可能只有几张照片，显然不足以训练一个好的分类器。收集每个人的数百张图片不切实际。但是，你可以在网络上收集很多随机人物的图片，然后训练第一个神经网络来检测两个不同的图片是否是同一个人。这样的网络将会学习到很好的人脸特征检测器，因此重用其较低层可以使你用很少的训练数据来训练一个好的人脸分类器。

对于自然语言处理（NLP）应用，你可以下载数百万个文本文档的语料库并从中自动生成带标签的数据。例如，你可以随机屏蔽一些单词并训练模型来预测缺失的单词（例如，它应该能预测句子“What___you saying”中的缺失单词可能是“are”或者“were”）。如果你可以训练模型在此任务上达到良好的性能，那么它已经对语言有相当多的了解，你当然可以在实际任务中重用它并在带标签的数据上进行微调（我们将在第15章中讨论更多的预训练任务）。

自我监督学习是指你从数据本身自动生成标签，然后使用有监督学习技术在所得到的“标签”数据集上训练模型。由于此方法不需要任何人工标记，因此最好将其分类为无监督学习的一种形式。

## 11.3 更快的优化器

训练一个非常大的深度神经网络可能会非常缓慢。到目前为止，我们已经知道了四种加快训练速度（并获得了更好的解决方法）的方法：对连接权重应用一个良好的初始化策略，使用良好的激活函数，使用批量归一化，以及重用预训练网络的某些部分（可能建立在辅助任务上或使用无监督学习）。与常规的梯度下降优化器相比，使用更快的优化器也可以带来巨大的速度提升。在本节中，我们将介绍最受欢迎的算法：动量优化、Nesterov加速梯度、AdaGrad、RMSProp，最后是Adam和Nadam优化。

### 11.3.1 动量优化

想象一下，一个保龄球在光滑的表面上沿着平缓的坡度滚动：它开始时速度很慢，但很快会获得动量，直到最终达到终极速度（如果有摩擦或空气阻力）。这是Boris Polyak在1964年提出的动量优化背后的非常简单的想法。相比之下，常规的梯度下降法只是在斜坡上采取小的、常规的步骤，因此算法将花费更多的时间到达底部。

回想一下，梯度下降通过直接减去权重的成本函数$J(θ)$的梯度乘以学习率$η(∆_θJ(θ))$来更新权重θ。等式是：$θ\leftarrowθ–η∆_θJ(θ)$。它不关心较早的梯度是什么。如果局部梯度很小，则它会走得非常缓慢。

动量优化非常关心先前的梯度是什么：在每次迭代时，它都会从动量向量m（乘以学习率η）中减去局部梯度，并通过添加该动量向量来更新权重（见公式11-4）。换句话说，梯度是用于加速度而不是速度。为了模拟某种摩擦机制并防止动量变得过大，该算法引入了一个新的超参数β，称为动量，必须将其设置为0（高摩擦）和1（无摩擦）之间。典型的动量值为0.9。

![fig09_公式11-4_动量算法](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter11/figures/fig09_%E5%85%AC%E5%BC%8F11-4_%E5%8A%A8%E9%87%8F%E7%AE%97%E6%B3%95.jpg)

你可以轻松地验证，如果梯度保持恒定，则最终速度（即权重更新的最大大小）等于该梯度乘以学习率η再乘以1/(1–β)（忽略符号）。例如，如果β=0.9，则最终速度等于梯度乘以学习率的10倍，因此动量优化最终比梯度下降快10倍！这使动量优化比梯度下降要更快得地从平台逃脱。我们在第4章中看到，当输入的比例非常不同时，成本函数将看起来像一个拉长的碗（见图4-7）。梯度下降相当快地沿着陡峭的斜坡下降，但是沿着山谷下降需要很长时间。相反，动量优化将沿着山谷滚动得越来越快，直到达到谷底（最优解）。在不使用批量归一化的深层神经网络中，较高的层通常会得到比例不同的输入，因此使用动量优化会有所帮助。它还可以帮助绕过局部优化问题。

由于这种动量势头，优化器可能会稍微过调，然后又回来，再次过调，在稳定于最小点之前会多次振荡。这是在系统中有一些摩擦力的原因之一：它消除了这些振荡，从而加快了收敛速度。

在Keras中实现动量优化不费吹灰之力：只需要使用SGD优化器并设置其超参数momentum，然后就可躺下来看效果！

```python
    optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)
```

### 11.3.2 Nesterov加速梯度

Yurii Nesterov于1983年提出的动量优化的一个小变体几乎总是比原始动量优化要快。Nesterov加速梯度（Nesterov Accelerated Gradient，NAG）方法也称为Nesterov动量优化，它不是在局部位置θ，而是在θ+βm处沿动量方向稍微提前处测量成本函数的梯度（见公式11-5）。

![fig10_公式11-5_Nesterov加速梯度算法](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter11/figures/fig10_%E5%85%AC%E5%BC%8F11-5_Nesterov%E5%8A%A0%E9%80%9F%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95.jpg)

这种小的调整有效是因为通常动量向量会指向正确的方向（即朝向最优解），因此使用在该方向上测得的更远的梯度而不是原始位置上的梯度会稍微准确一些。如图11所示（其中$\nabla_1$代表在起点θ处测量的成本函数的梯度，$\nabla_2$代表在位于θ+βm点的梯度）。

![fig11_常规与Nesterov动量优化](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter11/figures/fig11_%E5%B8%B8%E8%A7%84%E4%B8%8ENesterov%E5%8A%A8%E9%87%8F%E4%BC%98%E5%8C%96.jpg)

正如你所看到的，Nesterov更新会最终稍微接近最优解。一段时间后，这些小的改进累加起来，NAG就比常规的动量优化要快得多。此外，请注意，当动量推动权重跨越谷底时，$\nabla_1$继续推动越过谷底，而$\nabla_2$推回谷底。这有助于减少振荡，因此NAG收敛更快。

NAG通常比常规动量优化更快。要使用它，只需在创建SGD优化器时设置nesterov=True即可：

```python
    optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)
```

### 11.3.3 AdaGrad

次考虑拉长的碗状问题：梯度下降从快速沿最陡的坡度下降开始，该坡度没有直接指向全局最优解，然后非常缓慢地下降到谷底。如果算法可以更早地纠正其方向，使它更多地指向全局最优解，那将是很好的。AdaGrad算法通过沿最陡峭的维度按比例缩小梯度向量（见公式11-6）来实现此校正。

![fig12_公式11-6_AdaGrad算法](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter11/figures/fig12_%E5%85%AC%E5%BC%8F11-6_AdaGrad%E7%AE%97%E6%B3%95.jpg)

第一步将梯度的平方累加到向量s中（请记住，⊗符号表示逐元素相乘）。此向量化形式等效于针对向量s中的每个元素$s_i$计算$s_i\leftarrow s_i+(∂J(θ)/∂θ_i)^2$。换句话说，每个$s_i$累加关于参数$θ_i$的成本函数偏导数的平方。如果成本函数沿第i个维度陡峭，则$s_i$将在每次迭代中变得越来越大。

第二步几乎与“梯度下降”相同，但有一个很大的区别：梯度向量按比例因子$\sqrt {s+ε}$缩小了（ 符号代表逐元素相除，而ε是避免除以零的平滑项，通常设置为$10^{-10}$）。此向量化形式等效于对所有参数$θ_i$同时计算$θ_i\leftarrow θ_i-\eta ∂J(\theta)/∂θ_i/\sqrt{s_i+ε}$ 。

简而言之，该算法会降低学习率，但是对于陡峭的维度，它的执行速度要比对缓慢下降的维度的执行速度要快。这称为自适应学习率。它有助于将结果更新更直接地指向全局最优解（见图13）。另一个好处是，它几乎不需要调整学习率超参数η。

![fig13_AdaGrad与梯度下降法](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter11/figures/fig13_AdaGrad%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.jpg)

对于简单的二次问题，AdaGrad经常表现良好，但是在训练神经网络时，它往往停止得太早。学习率被按比例缩小，以至于算法在最终达到全局最优解之前完全停止了。因此，即使Keras有Adagrad优化器，你也不应使用它来训练深度神经网络（不过，它对于诸如线性回归之类的简单任务可能是有效的）。尽管如此，了解AdaGrad仍有助于掌握其他自适应学习率优化器。

```python
    optimizer = keras.optimizers.Adagrad(lr=0.001)
```

### 11.3.4 RMSProp

正如我们所看到的，AdaGrad有下降太快，永远不会收敛到全局最优解的风险。RMSProp算法通过只是累加最近迭代中的梯度（而不是自训练开始以来的所有梯度）来解决这个问题。它通过在第一步中使用指数衰减来实现（见公式11-7）。

![fig14_公式11-7_RMSProp算法](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter11/figures/fig14_%E5%85%AC%E5%BC%8F11-7_RMSProp%E7%AE%97%E6%B3%95.jpg)

衰减率β通常设置为0.9。是的，它又是一个新的超参数，但是此默认值通常效果很好，因此你可能根本不需要调整它。

如你所料，Keras有RMSprop优化器

```python
    optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)
```

注意rho参数对应于公式11-7中的β，除了非常简单的问题外，该优化器几乎总是比AdaGrad表现更好。实际上，直到Adam优化出现之前，它一直是许多研究人员首选的优化算法。

### 11.3.5 Adam和Nadam优化

Adam代表自适应矩估计，结合了动量优化和RMSProp的思想：就像动量优化一样，它跟踪过去梯度的指数衰减平均值。就像RMSProp一样，它跟踪过去平方梯度的指数衰减平均值（见公式11-8）。

![fig15_公式11-8_Adam算法](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter11/figures/fig15_%E5%85%AC%E5%BC%8F11-8_Adam%E7%AE%97%E6%B3%95.jpg)

在此等式中，t表示迭代次数（从1开始）。

如果只看步骤1、2和5，你会发现Adam与动量优化和RMSProp非常相似。唯一的区别是步骤1计算的是指数衰减的平均值，而不是指数衰减的总和，但除了常数因子（衰减平均值是衰减总和的$1–β_1$倍）外，它们实际上是等效的。第3步和第4步在技术上有些细节：由于m和s初始化为0，因此在训练开始时它们会偏向0，这两个步骤将有助于在训练开始时提高m和s。

动量衰减超参数β1通常被初始化为0.9，而缩放衰减超参数$β_2$通常被初始化为0.999。如前所述，平滑项ε通常会初始化为一个很小的数字，例如$10^{-7$}。这些是Adam类的默认值（准确地说，epsilon的默认值为None，它告诉Keras使用keras.backend.epsilon()，默认值为$10^{-7$}。你可以使用keras.backend.set_epsilon()来改变）。这是使用Keras来创建
Adam优化器的方法：

```python
    optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)
```

由于Adam是一种自适应学习率算法（如AdaGrad和RMSProp），因此对学习率超参数η需要较少的调整。你通常可以使用默认值η=0.001，这使得Adam甚至比梯度下降更易于使用。

如果你开始对所有这些不同的技术感到不知所措，并且想知道如何为自己的任务选择合适的一个，请不要担心，本章末提供了一些实用的准则。

最后，Adam的两个变体值得一提：

#### AdaMax

请注意，在公式11-8的步骤2中，Adam累加了s中的梯度平方（对于最近的梯度，权重更大）。在第5步中，如果忽略ε和第3步和第4步（无论如何都是技术细节），Adam将以s的平方根按比例缩小参数更新。简而言之，Adam按时间衰减梯度的l2范数按比例缩小参数更新（请注意，l2范数是平方和的平方根）。与Adam在同一篇论文中介绍的AdaMax将l2范数替换为$l_∞$范数（一种表达最大值的新颖方法）。具体来说，它用$s\leftarrow max(β_2s,∆_θJ(θ))$替换公式11-8中的步骤2，它删除第4步，在第5步中，将梯度更新按比例s缩小，这是时间衰减梯度的最大值。实际上，这可以使AdaMax比Adam更稳定，但这确实取决于数据集，通常Adam的表现更好。因此，如果你用Adam在某些任务上遇到问题，这是你可以尝试的另一种优化器。

#### Nadam

Nadam优化是Adam优化加上Nesterov技巧，因此其收敛速度通常比Adam稍快。在他介绍这种技术的报告中，研究人员Timothy Dozat在各种任务上对许多不同的优化器进行了比较，发现Nadam总体上胜过Adam，但有时不如RMSProp。

自适应优化方法（包括RMSProp、Adam和Nadam优化）通常很棒，可以快速收敛到一个好的解决方案。然而，Ashia C.Wilson等人在2017年发表的论文，表明它们可以导致在某些数据集上泛化不佳的解决方案。因此，当你对模型的性能感到失望时，尝试改用普通的Nesterov加速梯度：你的数据集可能对自适应梯度过敏。还要检查最新的研究，因为它发展很快。

到目前为止讨论的所有优化技术都仅仅依赖于一阶偏导数（Jacobians）。一些优化文献还包含了基于二阶偏导数（Hessian，这是Jacobian的偏导数）的一些算法。不幸的是，这些算法很难应用于深度神经网络，因为每个输出有n2个Hessian（其中n是参数的数量），而不是每个输出只有n个Jacobian。由于DNN通常具有成千上万的参数，因此二阶优化算法甚至不适合存储在内存中，即使可以的话，计算Hessians也太慢了。

#### 训练稀疏模型

所有的优化算法都产生了密集模型，这意味着大多数参数都是非零的。如果你在运行时需要一个非常快的模型，或者需要占用更少的内存，那么你可能更喜欢使用一个稀疏模型。

实现这一点的一个简单方法是像往常一样训练模型，然后去掉很小的权重（将它们设置为零）。注意，这通常不会导致非常稀疏的模型，而且可能会降低模型的性能。一个更好的选择是在训练时使用强1正规化（我们在本章后面将看到怎么做），因为它会迫使优化器产生尽可能多的为零的权重（详见4.5.2节）。

如果这些技术仍然不够，请查看TensorFlow Model Optimization Toolkit（TF-MOT），它提供了一个剪枝API，能够根据连接的大小在训练期间迭代地删除连接。

表11-2比较了我们目前讨论的所有优化器（* 不好，** 平均，*** 好）。

表11-2：优化器比较

|类别|收敛速度|收敛质量|
|:---:|:---:|:---:|
|SGD| * | *** |
|SGD(momentum=...)| ** | *** |
|SGD(momentum=...,nesterov=True)| ** | *** |
|Adagrad| *** | *（停止太早） |
|RMSprop|*** | ** 或者 *** |
|Adam| *** | ** 或者 *** |
|Nadam| *** | ** 或者 *** |
|AdaMax| *** | ** 或者 *** |

### 11.3.6 学习率调度

找到一个好的学习率非常重要。如果你设置得太高，训练可能会发散（如4.2节所述）。如果你设置得太低，训练最终会收敛到最优解，但是这将花费很长时间。如果将它设置得稍微有点高，它一开始会很快，但是最终会围绕最优解震荡，不会真正稳定下来。如果你的计算力预算有限，则可能必须先中断训练，然后才能正确收敛，从而产生次优解决（见图16）。

![fig16_各种学习率η的学习曲线]()

正如我们在第10章中讨论的那样，你可以对模型进行数百次的迭代训练以找到一个良好的学习率，然后将学习率从很小的值呈指数级地增加到很大的值，再查看学习曲线并选择一个学习率略低于学习曲线开始回升的时候。然后，你可以重新初始化模型，并以该学习率对其进行训练。

但是，你可以做得比恒定学习率更好：如果你从一个较大的学习率开始，一旦训练没有取得进展后就降低它，那与恒定学习率相比，你就可以更快地找到一个最优解。有许多不同的策略可以降低训练期间的学习率。从低学习率开始，增加它，然后再降低它也是有好处的。这些策略称为学习率调度（我们在第4章中简要介绍了此概念）。以下这些是最常用的学习率调度：

#### 幂调度

将学习率设置为迭代次数t的函数：$η(t)=η_0/(1+t/s)^c$。初始学习率$η_0$、幂c（通常设置为1）和步骤s是超参数。学习率在每一步都会下降。在s个步骤之后，它下降到$η_0/2$。再在s个步骤之后，它下降到$η_0/3$，然后下降到$η_0/4$，然后是$η_0/5$，以此类推。如你所见，此调度开始迅速下降，然后越来越慢。当然，幂调度需要调整$η_0$和s（可能还有c）。

#### 指数调度

将学习速率设置为$η(t)=$η_0 0.1^{t/s}$。学习率每s步将逐渐下降10倍。幂调度越来越缓慢地降低学习率，而指数调度则使学习率每s步降低10倍。

#### 分段恒定调度

对一些轮次使用恒定的学习率（例如，对于5个轮次，$η_0=0.1$），对于另外一些轮次使用较小的学习率（例如，对于50个轮次，$η_0=0.001$），以此类推。尽管这个方法可以很好地工作，但仍需要仔细研究以找出正确的学习率顺序以及使用它们的轮次。

#### 性能调度

每N步测量一次验证误差（就像提前停止一样），并且当误差停止下降时，将学习率降低λ倍。

#### 1周期调度1
与其他方法相反，1周期调度（在Leslie Smith于2018年的论文中引入）从提高初始学习率$η_0$开始，在训练中途线性增长至$η_1$。然后，它在训练的后半部分将学习率再次线性降低到$η_0$，通过将学习率降低几个数量级（仍然是线性的）来完成最后几个轮次。使用与找到最优学习率相同的方法来选择最大学习率$η_1$，而初始学习率$η_0$大约要低10倍。当使用动量时，我们首先从高动量开始（例如0.95），然后在训练的前半部分将其降低到较低的动量（例如线性降低到0.85），然后在训练的后半部分将其恢复到最大值（例如0.95），并以该最大值结束最后几个轮次。史密斯做了很多实验，表明这种方法通常能够大大加快训练速度并达到更好的性能。例如，在流行的CIFAR10图像数据集上，此方法仅在100个轮次内就达到了91.9％的验证准确率，而不是通过标准方法（具有相同的神经网络架构）在800个轮次内达到的90.3％的准确率。

2013年，Andrew Senior等人撰写的论文比较了使用动量优化来训练深度神经网络进行语音识别时一些最受欢迎的学习率调度的性能。作者得出的结论是，在这种情况下，性能调度和指数调度都表现良好。他们喜欢指数调度，因为它易于微调，比最优解决方法收敛速度更快（他们还提到实现起来比性能调度更容易，但在Keras中，这两个选项都很简单）。也就是说，1周期方法的性能似乎更好。

在Keras中实现幂调度是最简单的选择：只需在创建优化器时设置超参数decay即可

```python
    optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)
```

decay是s（学习率除以多个数字单位所需要的步数）的倒数，Keras假定c等于1。

指数调度和分段调度也非常简单。你首先需要定义一个函数，该函数采用当前轮次并返回学习率。例如，让我们实现指数调度：

```python
    def exponential_decay_fn(epoch):
        return 0.01 * 0.1**(epoch / 20)
```

如果不想对$η_0$和s进行硬编码，则可以创建一个返回配置函数的函数：

```python
    def exponential_decay(lr0, s):
        def exponential_decay_fn(epoch):
            return lr0 * 0.1**(epoch / s)
        return exponential_decay_fn

    exponential_decay_fn = exponential_decay(lr0=0.01, s=20)
```

接下来，创建一个LearningRateScheduler回调函数，为其提供调度函数，然后将此回调函数传递给`fit()`方法：

```python
        lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)
    history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                        validation_data=(X_valid_scaled, y_valid),
                        callbacks=[lr_scheduler])
```

LearningRateScheduler将在每个轮次开始时更新优化器的learning_rate属性。通常每个轮次更新一次学习率就足够了，但是如果你希望更频繁地更新学习率（例如在每个步骤中），则可以编写自己的回调函数（请参阅notebook的“Exponential Scheduling”部分示例）。如果每个轮次有很多个步骤，则每一步都更新学习率是有意义的。另外，你可以使用keras.optimizers.schedules方法，这将在稍后进行介绍。

调度函数可以选择将当前学习率作为第二个参数。例如，以下调度函数将以前的学习率乘以$0.1^{1/20}$，这将导致相同的指数衰减（但现在衰减从轮次0开始而不是1开始）：

```python
    def exponential_decay_fn(epoch, lr):
        return lr * 0.1**(1 / 20)
```

此实现依赖于优化器的初始学习率（与先前的实现相反），因此请确保对其进行适当的设置。

保存模型时，优化器及其学习率也会随之保存。这意味着有了这个新的调度函数，你只需加载经过训练的模型，从中断的地方继续进行训练。但是，如果你的调度函数使用epoch参数，事情就变得不那么简单了：epoch不会被保存，并且每次你调用`fit()`方法时都会将其重置为0。如果你要继续训练一个中断的模型，则可能会导致一个很高的学习率，这可能会损害模型的权重。一种解决方法是手动设置`fit()`方法的initial_epoch参数，使epoch从正确的值开始。

对于分段恒定调度，可以使用以下调度函数（如前所述，可以根据需要定义一个更通用的函数。有关示例，请参见notebook的“Diecewise Lonstant Scheduling”部分），然后创建带有此函数的LearningRateScheduler回调函数，并将其传递给`fit()`方法，就像我们对指数调度所做的那样：

```python
    def piecewise_constant_fn(epoch):
        if epoch < 5:
            return 0.01
        elif epoch < 15:
            return 0.005
        else:
            return 0.001
```

对于性能调度，请使用ReduceLROnPlateau回调函数。例如，如果将以下回调函数传递给`fit()`方法，则每当连续5个轮次的最好验证损失都没有改善时，它将使学习率乘以0.5（有其他选项可用，请查看文档以获取更多详细信息）：

```python
    lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
```

最后，tf.keras提供了另一种实现学习率调度的方法：使用keras.optimizers.schedule中可以使用的调度之一来定义学习率，然后将该学习率传递给任意优化器。这种方法在每个步骤更新学习率而不是每个轮次。例如，以下是实现与我们先前定义的`exponential_decay_fn()`函数相同的指数调度的方法：

```python
    s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)
    learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)
    optimizer = keras.optimizers.SGD(learning_rate)
```

这很简单，而且当你保存模型时，学习率及其调度（包括其状态）也将被保存。但是，这种方法不是Keras API的一部分，它只适用于tf.keras。

对于1周期方法，实现起来没有特别的困难：只需创建一个每次迭代都能修改学习率的自定义回调函数即可（你可以通过更改self.model.optimizer.lr来更新优化器的学习率）。有关示例请参见notebook的“lycle scheduling”部分。

总而言之，指数衰减、性能调度和1周期都可以大大加快收敛速度，因此请尝试一下！

## 11.4 过正则化避免过拟合

拥有数千个参数，你可以拟合出整个动物园。深度神经网络通常具有数万个参数，有时甚至有数百万个。这给它们带来了难以置信的自由度，意味着它们可以拟合各种各样的复杂数据集。但是，这种巨大的灵活性也使网络易于过拟合训练集。我们需要正则化。我们已经在第10章中实现了最好的正则化技术之一：提前停止。而且，即使“批量归一化”被设计用来解决不稳定的梯度问题，它的作用也像一个很好的正则化。在本节中，我们将研究其他流行的神经网络正则化技术：$\ell_1$和$\ell_2$正则化、dropout和最大范数正则化。

### 11.4.1 $\ell_1$ 和 $\ell_2$ 正则化

就像在第4章中对简单线性模型所做的一样，可以使用$\ell_2$正则化来约束神经网络的连接权重，如果想要稀疏模型（许多权重等于0）则可以使用$\ell_1$正则化。以下是使用0.01的正则化因子将$\ell_2$正则化应用于Keras层的连接权重的方法：

```python
    layer = keras.layers.Dense(100, activation="elu",
                            kernel_initializer="he_normal",
                            kernel_regularizer=keras.regularizers.l2(0.01))
    # or l1(0.1) for ℓ1 regularization with a factor or 0.1
    # or l1_l2(0.1, 0.01) for both ℓ1 and ℓ2 regularization, with factors 0.1 and 0.01 respectively
```

`l2()`函数返回一个正则化函数，在训练过程中的每个步骤都将调用该正则化函数来计算正则化损失。然后将其添加到最终损失中。如你所料，如果需要$\ell_1$正则化，可以只使用`keras.regularizers.l1()`。如果你同时需要$\ell_1$和$\ell_2$正则化，请使用`keras.regularizers.l1_l2()`（同时指定两个正则化因子）。

由于你通常希望将相同的正则化函数应用于网络中的所有层，并在所有隐藏层中使用相同的激活函数和相同的初始化策略，因此你可能会发现自己重复了相同的参数。这使代码很难看且容易出错。为了避免这种情况，你可以尝试使用循环来重构代码。另一种选择是使用Python的`functools.partial()`函数，该函数可以使你为带有一些默认参数值的任何可调用对象创建一个小的包装函数：

```python
    model = keras.models.Sequential([
        keras.layers.Flatten(input_shape=[28, 28]),
        keras.layers.Dense(300, activation="elu",
                        kernel_initializer="he_normal",
                        kernel_regularizer=keras.regularizers.l2(0.01)),
        keras.layers.Dense(100, activation="elu",
                        kernel_initializer="he_normal",
                        kernel_regularizer=keras.regularizers.l2(0.01)),
        keras.layers.Dense(10, activation="softmax",
                        kernel_regularizer=keras.regularizers.l2(0.01))
    ])
    model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
    n_epochs = 2
    history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                        validation_data=(X_valid_scaled, y_valid))
```

### 11.4.2 dropout

对于深度神经网络，dropout是最受欢迎的正则化技术之一。它是由Geoffrey Hinton在2012年的论文中提出的，并且在Nitish Srivastava等人2014年的论文中得到了进一步的详述，已被证明是非常成功的：只需要增加dropout，即使最先进的神经网络也能获得1～2%的准确率提升。这听起来可能不算很多，但是当模型已经具有95％的准确率时，将准确率提高2％意味着将错误率降低了近40％（从5％的错误率降低到大约3％）。

这是一个非常简单的算法：在每个训练步骤中，每个神经元（包括输入神经元，但始终不包括输出神经元）都有暂时“删除”的概率p，这意味着在这个训练步骤中它被完全忽略，但在下一步中可能处于活动状态（见图17）。超参数p称为dropout率，通常设置为10％到50％：在循环神经网络中接近20～30％（见第15章），在卷积神经网络中接近40～50％（见第14章）。训练后，神经元不再被删除。这就是全部内容（除了一个技术细节，我们马上进行讨论）。

![fig17_使用dropout正则化]()

首先令人惊讶的是，这种破坏性技术完全有效。如果公司告诉员工每天早上扔硬币来决定是否去上班，公司的业绩会更好吗？好吧，谁知道呢？也许会！该公司将被迫调整其组织结构。它不能依靠任何一个人来操作咖啡机或执行任何其他关键任务，因此必须将这种专业知识分散到多个人中。员工必须学会与许多同事合作，而不仅仅是少数几个。该公司将变得更具弹性。如果一个人辞职，不会有太大的区别。尚不清楚这种想法是否真的适用于公司，但它确实适用于神经网络。经过dropout训练的神经元不能与其相邻的神经元相互适应，它们必须自己发挥最大的作用。它们也不能过分依赖少数输入神经元，它们必须注意每个输入神经元。它们最终对输入的微小变化不太敏感。最后，你将获得一个更有鲁棒性的网络，该网络有更好的泛化能力。

了解dropout能力的另一种方法是认识到在每个训练步骤中都会生成一个独特的神经网络。由于每个神经元都可以存在或不存在，因此共有2N个可能的网络（其中N是可以dropout神经元的总数）。这是一个巨大的数字，几乎不可能对同一神经网络进行两次采样。一旦你运行了10000个训练步骤，你实质上就已经训练了10000个不同的神经网络（每个神经网络只有一个训练实例）。这些神经网络显然不是独立的，因为它们共享许多权重，但是它们却是完全不同的。所得的神经网络可以看作是所有这些较小的神经网络的平均集成。

在实践中，你通常只可以对第一至第三层（不包括输出层）中的神经元应用
dropout。

有一个很小但很重要的技术细节。假设p=50％，在这种情况下，在测试过程中，一个神训练深度神经网络被连接到输入神经元的数量是训练期间（平均）的两倍。为了弥补这一事实，我们需要在训练后将每个神经元的输入连接权重乘以0.5。如果不这样做，每个神经元得到的总输入信号大约是网络训练时的两倍，表现可能不会很好。更一般而言，训练后我们需要将每个输入连接权重乘以保留概率（1-p）。或者，我们可以在训练过程中将每个神经元的输出除以保留概率（这些替代方法不是完全等效的，但效果一样好）。

要使用Keras实现dropout，可以使用keras.layers.Dropout层。在训练期间，它会随机丢弃一些输入（将它们设置为0），然后将其余输入除以保留概率。训练之后，它什么都不做，只是将输入传递到下一层。以下代码使用0.2的dropout率在每个Dense层之前应用dropout正则化：

```python
    model = keras.models.Sequential([
        keras.layers.Flatten(input_shape=[28, 28]),
        keras.layers.Dropout(rate=0.2),
        keras.layers.Dense(300, activation="elu", kernel_initializer="he_normal"),
        keras.layers.Dropout(rate=0.2),
        keras.layers.Dense(100, activation="elu", kernel_initializer="he_normal"),
        keras.layers.Dropout(rate=0.2),
        keras.layers.Dense(10, activation="softmax")
    ])
    model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
    n_epochs = 2
    history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                        validation_data=(X_valid_scaled, y_valid))
```

由于dropout仅在训练期间激活，因此比较训练损失和验证损失可能会产生误导。具体而言，模型可能会过拟合训练集，但仍具有相似的训练损失和验证损失。因此，请确保没有使用dropout来评估训练损失（例如，训练之后）。如果你发现模型过拟合，则可以提高dropout率。相反，如果模型欠拟合训练集，则应尝试降低dropout率。它还可以帮助增加较大层的dropout率，并减小较小层的dropout率。此外，许多最新的架构仅在最后一个隐藏层之后才使用dropout，因此如果完全dropout太强，你可以尝试一下此方法。

dropout确实会明显减慢收敛速度，但是如果正确微调，通常会导致更好的模型。因此，花额外的时间和精力是值得的。

如果要基于SELU激活函数（如前所述）对自归一化网络进行正则化，则应使用alpha dropout：这是dropout的一种变体，它保留了其输入的均值和标准差（在与SELU相同的论文中介绍，因为常规的dropout会破坏自归一化）。

### 11.4.3 蒙特卡罗（MC）Dropout

P356