# 第六章 决策树

与SVM一样，决策树是通用的机器学习算法，可以执行分类和回归任务，甚至多输出任务。它们是功能强大的算法，能够拟合复杂的数据集。例如，在第2章中，你在加州房屋数据集中训练了DecisionTreeRegressor模型，使其完全拟合（实际上是过拟合）。

决策树也是随机森林的基本组成部分（见第7章），它们是当今最强大的机器学习算法之一。

在本章中，我们将从讨论如何使用决策树进行训练、可视化和做出预测开始。然后，我们将了解Scikit-Learn使用的CART训练算法，并将讨论如何对树进行正则化并将其用于回归任务。最后，我们将讨论决策树的一些局限性。

## 6.1 训练和可视化决策树

为了理解决策树，让我们建立一个决策树，然后看看它是如何做出预测的。以下代码在鸢尾花数据集上训练了一个`DecisionTreeClassifier`（见第4章）:

```python
    from sklearn.datasets import load_iris
    from sklearn.tree import DecisionTreeClassifier

    iris = load_iris()
    X = iris.data[:, 2:] # petal length and width
    y = iris.target

    tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)
    tree_clf.fit(X, y)
```

要将决策树可视化，首先，使用`export_graphviz()`方法输出一个图形定义文件，命名为iris_tree.dot：

```python
    from graphviz import Source
    from sklearn.tree import export_graphviz

    export_graphviz(
        tree_clf,
        out_file = os.path.join(IMAGES_PATH, "iris_tree.dot"),
        feature_names = iris.feature_names[2:],
        class_names = iris.target_names,
        rounded = True,
        filed = True
    )

    Source.from_file(os.path.join(IMAGES_PATH, "iris_tree.dot"))
```

你的第一个决策树如图6-1所示：

![fig01_鸢尾花决策树](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter6/figures/fig01_%E9%B8%A2%E5%B0%BE%E8%8A%B1%E5%86%B3%E7%AD%96%E6%A0%91.jpg)

## 6.2 做出预测


让我们看看图1中的树是如何进行预测的。假设你找到一朵鸢尾花，要对其进行分类。你从根节点开始（深度为0，在顶部）：该节点询问花的花瓣长度是否小于2.45cm。如果是，则向下移动到根的左子节点（深度1，左）。在这种情况下，它是一片叶子节点（即它没有任何子节点），因此它不会提出任何问题：只需查看该节点的预测类，然后决策树就可以预测花朵是山鸢尾花（`class=setosa`）。

现在假设你发现了另一朵花，这次花瓣的长度大于2.45cm，你必须向下移动到根的右子节点（深度1，右），该子节点不是叶子节点，因此该节点会问另一个问题：花瓣宽度是否小于1.75cm？如果是，则你的花朵很可能是变色鸢尾花（深度2，左）。如果不是，则可能是维吉尼亚鸢尾花（深度2，右）。就是这么简单。

决策树的许多特质之一就是它们**几乎不需要数据准备**。实际上，它们**根本不需要特征缩放或居中**。

节点的samples属性统计它应用的训练实例数量。例如，有100个训练实例的花瓣长度大于2.45cm（深度1，右），其中54个花瓣宽度小于1.75cm（深度2，左）。节点的value属性说明了该节点上每个类别的训练实例数量。例如，右下节点应用在0个山鸢尾、1个变色鸢尾和45个维吉尼亚鸢尾实例上。最后，节点的gini属性衡量其**不纯度**（impurity）：如果应用的所有训练实例都属于同一个类别，那么节点就是“纯”的（gini=0）。例如，深度1左侧节点仅应用于山鸢尾花训练实例，所以它就是纯的，并且gini值为0。公式6-1说明了第i个节点的**基尼系数Gi**的计算方式。例如，深度2左侧节点，基尼系数等于1–(0/54)^2–(49/54)^2–(5/54)^2≈0.168。

![fig02_基尼不纯度](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter6/figures/fig02_%E5%9F%BA%E5%B0%BC%E4%B8%8D%E7%BA%AF%E5%BA%A6.jpg)

其中，$p_{i,k}$ 是第i个节点中训练实例之间的k类实例的比率。

Scikit-Learn使用的是CART算法，该算法仅生成二叉树：非叶节点永远只有两个子节点（即问题答案仅有是或否）。但是，其他算法（比如ID3生成的决策树），其节点可以拥有两个以上的子节点。

图3显示了决策树的决策边界。加粗直线表示根节点（深度0）的决策边界：花瓣长度=2.45cm。因为左侧区域是纯的（只有山鸢尾花），所以它不可再分。但是右侧区域是不纯的，所以深度1右侧的节点在花瓣宽度=1.75cm处（虚线所示）再次分裂。因为这里最大深度`max_depth`设置为2，所以决策树在此停止。但是如果你将`max_depth`设置为3，那么两个深度为2的节点将各自再产生一条决策边界（点线所示）。

![fig03_决策树决策边界](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter6/figures/fig03_%E5%86%B3%E7%AD%96%E6%A0%91%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C.jpg)

**白盒子与黑盒子**：决策树是直观的，其决策也易于解释。这种模型通常称为白盒模型。相反，正如我们将看到的，通常将随机森林或神经网络视为黑盒模型。它们做出了很好的预测，你可以轻松地检查它们为做出这些预测而执行的计算。但是，通常很难用简单的话语来解释为什么做出这样的预测。例如，如果神经网络说某个人出现在图片上，那么很难知道是什么因素促成了这一预测：该模型识别该人的眼睛、嘴、鼻子、鞋子，甚至他们坐的沙发？相反，决策树提供了很好的、简单的分类规则，如果需要的话，甚至可以手动应用这些规则（例如，用于花的分类）。

## 6.3 估计类概率

决策树同样可以估算某个实例属于特定类k的概率：首先，跟随决策树找到该实例的叶节点，然后返回该节点中类k的训练实例占比。例如，假设你发现一朵花，其花瓣长5cm，宽1.5cm。相应的叶节点为深度2左侧节点，因此决策树输出如下概率：山鸢尾花，0%（0/54）；变色鸢尾花，90.7%（49/54）；维吉尼亚鸢尾花，9.3%（5/54）。当然，如果你要求它预测类，那么它应该输出变色鸢尾花（类别1），因为它的概率最高。我们试一下：

```python
    tree_clf.predict_proba([[5, 1.5]])
    >>> array([[0.        , 0.90740741, 0.09259259]])
    tree_clf.predict([[5, 1.5]])
    >>> array([1])
```

完美！注意，在图6-2的右下角矩形中，任意点的估计概率都是相同的，例如，如果花瓣长6cm，宽1.5cm（即使看起来很可能是维吉尼亚鸢尾花）。

## 6.4 CART训练算法

Scikit-Learn使用**分类和回归树**（`Classification and Regression Tree`，CART）算法来训练决策树（也称为“增长树”）。该算法的工作原理是：首先使用单个特征k和阈值t<sub>k</sub>（例如，“花瓣长度”≤2.45cm”）将训练集分为两个子集。如何选择k和t<sub>k</sub>？它搜索产生最纯子集（按其大小加权）的一对(k，t<sub>k</sub>)。公式6-2给出了算法试图最小化的成本函数。

![fig04_CART分类成本函数]()

一旦CART算法成功地将训练集分为两部分，它就会使用相同的逻辑将子集进行分割，然后再分割子集，以此类推。一旦达到最大深度（由超参数`max_depth`定义），或者找不到可减少不纯度的分割，它将停止递归。其他一些超参数（稍后描述）可以控制其他一些停止条件(`min_samples_split`、`min_samples_leaf`、`min_weight_fraction_leaf`和 `max_leaf_nodes`)。

如你所见，**CART是一种贪婪算法**：从顶层开始搜索最优分裂，然后每层重复这个过程。几层分裂之后，它并不会检视这个分裂的不纯度是否为可能的最低值。**贪婪算法通常会产生一个相当不错的解，但是不能保证是最优解**。

而不幸的是，寻找最优树是一个已知的NP完全问题：需要的时间是O(exp(m))，所以即使是很小的训练集，也相当棘手。这就是为什么我们必须接受一个“相当不错”的解。

P是可以在多项式时间内解决的一组问题。NP是可以在多项式时间内验证其解的一组问题。NP难问题是可以在多项式时间内将任何NP问题减少的问题。一个NP完全问题是NP和NP难。一个主要的开放数学问题是P=NP是否相等。如果P≠NP（这似乎是可能的），那么不会找到针对任何NP完全问题的多项式算法（也许在量子计算机上除外）。

## 6.5 计算复杂度

进行预测需要从根到叶遍历决策树。决策树通常是近似平衡的，因此遍历决策树需要经过大约 $O(log_2(m))$ 个节点（注： $log_2$ 是二进制对数。它等于 $log_2(m) = log(m)/log(2)$ ）。由于每个节点仅需要检查一个特征值，因此总体预测复杂度为 $O(log_2(m))$ 。与特征数量无关。因此，即使处理大训练集，预测也非常快。

训练算法比较每个节点上所有样本上的所有特征（如果设置了`max_features`，则更少）。比较每个节点上所有样本的所有特征会导致训练复杂度为 $O(n×m log_2(m))$ 。对于小训练集（少于几千个实例），Scikit-Learn可以通过对数据进行预排序（设置`presort=True`）来加快训练速度，但是这样做会大大降低大训练集的训练速度。

## 6.6 基尼不纯度或熵

默认使用的是基尼不纯度来进行测量，但是，你可以将超参数`criterion`设置为`"entropy"`来选择熵作为不纯度的测量方式。熵的概念源于热力学，是一种分子混乱程度的度量：如果分子保持静止和良序，则熵接近于零。后来这个概念推广到各个领域，其中包括香农的信息理论，它衡量的是**一条信息的平均信息内容**（熵的减少通常称为信息增益）：如果所有的信息都相同，则熵为零。在机器学习中，它也经常被用作一种不纯度的测量方式：**如果数据集中仅包含一个类别的实例，其熵为零**。公式6-3显示了第i个节点的熵值的计算方式。例如，图6-1中
深度2左侧节点的熵值等于：$-\frac{49}{54}log_2(\frac{49}{54})-\frac{5}{54}log_2(\frac{5}{54})\approx 0.445$ 。

![fig05_熵]()

那么到底应该使用基尼不纯度还是熵呢？其实，大多数情况下，它们并没有什么大的不同，产生的树都很相似。基尼不纯度的计算速度略微快一些，所以它是个不错的默认选择。它们的不同在于，基尼不纯度倾向于从树枝中分裂出最常见的类别，而熵则倾向于生产更平衡的树。

## 6.7 正则化超参数

决策树极少对训练数据做出假设（比如线性模型就正好相反，它显然假设数据是线性的）。如果不加以限制，树的结构将跟随训练集变化，严密拟合，并且很可能过拟合。这种模型通常被称为**非参数模型**，这不是说它不包含任何参数（事实上它通常有很多参数），而是指在训练之前没有确定参数的数量，导致模型结构自由而紧密地贴近数据。相反，参数模型（比如线性模型）则有预先设定好的一部分参数，因此其自由度受限，从而降低了过拟合的风险（但是增加了欠拟合的风险）。

为避免过拟合，需要在训练过程中降低决策树的自由度。现在你应该知道，这个过程被称为**正则化**。正则化超参数的选择取决于使用的模型，但是通常来说，至少可以限制决策树的最大深度。在Scikit-Learn中，这由超参数`max_depth`控制（默认值为None，意味着无限制）。减小`max_depth`可使模型正则化，从而降低过拟合的风险。

`DecisionTreeClassifier`类还有一些其他的参数，同样可以限制决策树的形状：`min_samples_split`（分裂前节点必须有的最小样本数）、`min_samples_leaf`（叶节点必须有的最小样本数量）、`min_weight_fraction_leaf`（与`min_samples_leaf`一样，但表现为加权实例总数的占比）、`max_leaf_nodes`（最大叶节点数量），以及`max_features`（分裂每个节点评估的最大特征数量）。增大超参数`min_*`或减小`max_*`将使模型正则化。

还可以先不加约束地训练模型，然后再对不必要的节点进行剪枝（删除）。如果一个节点的子节点全部为叶节点，则该节点可被认为不必要，除非它所表示的纯度提升有重要的统计意义。标准统计测试（比如χ2测试）用来估算“提升纯粹是出于偶然”（被称为零假设）的概率。如果这个概率（称之为p值）高于一个给定阈值（通常是5%，由超参数控制），那么这个节点可被认为不必要，其子节点可被删除。直到所有不必要的节点都被
删除，剪枝过程结束。

图6显示了在卫星数据集上训练的两个决策树（在第5章中介绍）。左侧使用默认的超参数（即无限制）训练决策树，右侧使用`min_samples_leaf=4`进行训练。显然，左边的模型过拟合，右边的模型可能会更好地泛化。

![fig06_使用min_samples_leaf进行正则化]()

## 6.8 回归

决策树还能够执行回归任务。让我们使用Scikit-Learn的`DecisionTreeRegressor`类构建一个回归树，并用`max_depth=2`在一个有噪声的二次数据集上对其进行训练：

```python
    from sklearn.tree import DecisionTreeRegressor
    tree_reg = DecisionTreeRegressor(max_depth=2)
    tree_reg.fit(X, y)
```

生成的树如图7所示：

![fig07_回归决策树]()

这棵树看起来与之前建立的分类树很相似。主要差别在于，每个节点上不再预测一个类别而是预测一个值。例如，如果你想要对一个x1=0.6的新实例进行预测，那么从根节点开始遍历，最后到达预测value=0.111的叶节点。这个预测结果其实就是与这个叶节点关联的110个实例的平均目标值。在这110个实例上，预测产生的均方误差（MSE）等于0.015。

图8的左侧显示了该模型的预测。如果设置max_depth=3，将得到如图8右侧所示的预测。注意看，每个区域的预测值永远等于该区域内实例的目标平均值。算法分裂每个区域的方法就是使最多的训练实例尽可能接近这个预测值。

![fig08_两种决策树回归模型的预测]()

CART算法的工作原理与以前的方法大致相同，不同之处在于，它不再尝试以最小化不纯度的方式来拆分训练集，而是以最小化MSE的方式来拆分训练集。公式6-4给出了算法试图最小化的成本函数。

![fig09_CART回归成本函数]()

就像分类任务一样，决策树在处理回归任务时容易过拟合。如果不进行任何正则化（如使用默认的超参数），你会得到图10左侧的预测。这些预测显然非常过拟合训练集。只需设置`min_samples_leaf=10`就可以得到一个更合理的模型，如图10右侧所示：

![fig10_正则化一个回归决策树]()

## 6.9 不稳定性

希望到目前为止，你已经确信决策树有很多用处：它们易于理解和解释、易于使用、用途广泛且功能强大。但是，它们确实有一些局限性。首先，你可能已经注意到，决策树喜欢正交的决策边界（所有分割都垂直于轴），这使它们对训练集旋转敏感。例如，图11显示了一个简单的线性可分离数据集：在左侧，决策树可以轻松地对其进行拆分，而在右侧，将数据集旋转45度后，决策边界看起来复杂了（没有必要）。尽管两个决策树都非常拟合训练集，但右侧的模型可能无法很好地泛化。限制此问题的一种方法是使用主成分分析（见第8章），这通常会使训练数据的方向更好。

![fig11_对训练集旋转敏感]()

更概括地说，决策树的主要问题是**它们对训练数据中的小变化非常敏感**。例如，如果你从鸢尾花数据集中移除花瓣最宽的变色鸢尾花（花瓣长4.8cm，宽1.8cm），然后重新训练一个决策树，你可能得到如图12所示的模型。这跟之前图3的决策树看起来截然不同。事实上，由于Scikit-Learn所使用的算法是随机的，即使是在相同的训练数据上，你也可能得到完全不同的模型（除非你对超参数`random_state`进行设置）。

![fig12_对训练集细节敏感]()

随机森林可以通过对许多树进行平均预测来限制这种不稳定性，正如我们将在第7章中看到的那样。