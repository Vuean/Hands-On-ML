# 第九章 无监督学习技术

尽管今天机器学习的大多数应用都是基于有监督学习的（因此，这是大多数投资的方向），但是绝大多数可用数据都没有标签：我们具有输入特征X，但是没有标签y。计算机科学家Yann LeCun曾有句著名的话：“如果智能是蛋糕，无监督学习将是蛋糕本体，有
监督学习是蛋糕上的糖霜，强化学习是蛋糕上的樱桃。”换句话说，无监督学习具有巨大
的潜力，我们才刚刚开始研究。

假设你要创建一个系统，该系统将在制造生产线上为每个产品拍摄几张图片，并检测哪些产品有缺陷。你可以相当容易地创建一个自动拍照系统，这可能每天为你提供数千张图片。然后，你可以在几周内构建一个相当大的数据集。但是，等等，没有标签！如果你想训练一个常规的二元分类器来预测某件产品是否有缺陷，则需要将每张图片标记为“有缺陷”或“正常”。这通常需要人类专家坐下来并手动浏览所有图片。这是一项漫长、昂贵且烦琐的任务，因此通常只能在可用图片的一部分上完成。因此，标记的数据集将非常小，并且分类器的性能将令人失望。而且，公司每次对其产品进行任何更改时，都需要从头开始整个过程。如果该算法只需要利用未标记的数据而无须人工标记每张图片，那不是很好吗？让我们进入无监督学习。

在第8章中，我们研究了最常见的无监督学习任务：降维。在本章中，我们将研究其
他一些无监督的学习任务和算法：

- **聚类**

    目标是将相似的实例分组到集群中。聚类是很好的工具，用于数据分析、客户细分、推荐系统、搜索引擎、图像分割、半监督学习、降维等。

- **异常检测**

    目的是学习“正常”数据看起来是什么样的，然后将其用于检测异常情况，例如生产线上的缺陷产品或时间序列中的新趋势。

- **密度估算**

    这是**估计生成数据集的随机过程的概率密度函数**（PDF）的任务，密度估算通常用于异常检测：位于非常低密度区域的实例很可能是异常。它对于数据分析和可视化也很有用。

准备好蛋糕了吗？我们将从使用K-Means和DBSCAN进行聚类开始，然后讨论高斯混合模型，并了解如何将它们用于密度估计、聚类和异常检测。

## 9.1 聚类

你在山中徒步旅行时，偶然发现了从未见过的植物。你环顾四周，发现还有很多。它们并不完全相同，但是它们足够相似，你可能知道它们有可能属于同一物种（或至少属于同一属）。你可能需要植物学家告诉你什么是物种，但你当然不需要专家来识别外观相似的物体组。这称为聚类：**识别相似实例并将其分配给相似实例的集群或组**。

就像在分类中一样，每个实例都分配给一个组。但是与分类不同，**聚类是一项无监督任务**。考虑图1：左侧是鸢尾花数据集（在第4章中介绍），其中每个实例的种类（即类）用不同的标记表示。它是一个标记的数据集，非常适合使用逻辑回归、SVM或随机森林分类器等分类算法。右侧是相同的数据集，但是没有标签，因此你不能再使用分类算法。这就是聚类算法的引入之处，它们中的许多算法都可以轻松检测左下角的集群。肉眼也很容易看到，但是右上角的集群由两个不同的子集群组成，并不是很明显。也就是说，数据集具有两个附加特征（萼片长度和宽度），此处未表示，并且聚类算法可以很好地利用所有的特征，因此实际上它们可以很好地识别三个聚类（例如，使用高斯混合模型，在150个实例中，只有5个实例分配给错误的集群）。

![fig01_分类与聚类]()

聚类可用于各种应用程序，包括：

- 客户细分

    你可以根据客户的购买记录和他们在网站上的活动对客户进行聚类。这对于了解你的客户是谁以及他们的需求很有用，因此你可以针对每个细分客户调整产品和营销活动。例如，客户细分在推荐系统中可以很有用，可以推荐同一集群中其他用户喜欢的内容。

- 数据分析

    在分析新数据集时，运行聚类算法然后分别分析每个集群。

- 降维技术

    数据集聚类后，通常可以测量每个实例与每个集群的相似度（相似度是衡量一个实例和一个集群的相似程度）。然后可以将每个实例的特征向量x替换为其集群的向量。如果有k个集群，则此向量为k维。此向量的维度通常比原始特征向量低得多，但它可以保留足够的信息以进行进一步处理。

- 异常检测（也称为离群值检测）

    对所有集群具有低相似度的任何实例都可能是异常。例如，如果你已根据用户行为对网站用户进行了聚类，则可以检测到具有异常行为的用户，例如每秒的请求数量异常。异常检测在检测制造生产线中的缺陷或欺诈检测中特别有用。

- 半监督学习

    如果你只有几个标签，则可以执行聚类并将标签传播到同一集群中的所有实例。该技术可以大大增加可用于后续有监督学习算法的标签数量，从而提高其性能。

- 搜索引擎

    一些搜索引擎可让你搜索与参考图像相似的图像。要构建这样的系统，首先要对数据库中的所有图像应用聚类算法，相似的图像最终会出现在同一集群中。然后，当用户提供参考图像时，你需要做的就是使用训练好的聚类模型找到该图像的集群，然后可以简单地从该集群中返回所有的图像。

- 分割图像

    通过根据像素的颜色对像素进行聚类，然后用其聚类的平均颜色替换每个像素的颜色，可以显著减少图像中不同颜色的数量。图像分割用于许多物体检测和跟踪系统中，因为它可以更轻松地检测每个物体的轮廓。

关于聚类什么没有统一的定义，它实际上取决于上下文，并且不同的算法会得到不同种类的集群。一些算法会寻找围绕特定点（称为中心点）的实例。其他人则寻找密集实例的连续区域：这些集群可以呈现任何形状。一些算法是分层的，寻找集群中的集群。这样的示例不胜枚举。

在本节中，我们将研究两种流行的聚类算法——K-Means和DBSCAN，并探讨它们的一些应用，例如非线性降维、半监督学习和异常检测。

### 9.1.1 K-Means

考虑图2中所示的未标记数据集：你可以清楚地看到5组实例。K-Means算法是一种简单的算法，能够非常快速、高效地对此类数据集进行聚类，通常只需几次迭代即可。它是由贝尔实验室的Stuart Lloyd在1957年提出的，用于脉冲编码调制，但直到1982年才对外发布。1965年，Edward W.Forgy发布了相同的算法，因此K-Means有时被称为Lloyd–Forgy。

![fig02_由5组实例集群组成的未标记数据集]()

让我们在该数据集上训练一个K-Means聚类器。它将尝试找到每个集群的中心，并将每个实例分配给最近的集群：

```python
    from sklearn.cluster import KMeans

    k = 5
    kmeans = KMeans(n_clusters=k, random_state=42)
    y_pred = kmeans.fit_predict(X)
```

请注意，你必须指定这个算法必须要找到的集群数k。在此例中，通过查看数据可以明显看出k应该设置为5，但总的来说并不是那么容易。我们会简短讨论这个问题。

每个实例都会分配给5个集群之一。在聚类的上下文中，实例的标签是该实例被算法分配给该集群的索引：不要与分类中的类标签相混淆（请记住，聚类是无监督学习任务）。KMeans实例保留了经过训练的实例的标签副本，可通过labels_实例变量得到该副本：

```python
    y_pred
    >>> array([4, 1, 0, ..., 3, 0, 1])
    y_pred is kmeans.labels_
    >>> True
```

我们还可以看一下算法发现的5个中心点：

```python
    kmeans.cluster_centers_
    >>> array([[ 0.20876306,  2.25551336],
       [-2.80389616,  1.80117999],
       [-1.46679593,  2.28585348],
       [-2.79290307,  2.79641063],
       [-2.80037642,  1.30082566]])
```

可以很容易地将新实例分配给中心点最接近的集群：

```python
    X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])
    kmeans.predict(X_new)
    >>> array([0, 0, 3, 3])
```

如果绘制集群的边界，则会得到Voronoi图（参见图3，其中每个中心点都用×表示）。

![fig03_K-Means决策边界]()

