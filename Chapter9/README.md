# 第九章 无监督学习技术

尽管今天机器学习的大多数应用都是基于有监督学习的（因此，这是大多数投资的方向），但是绝大多数可用数据都没有标签：我们具有输入特征X，但是没有标签y。计算机科学家Yann LeCun曾有句著名的话：“如果智能是蛋糕，无监督学习将是蛋糕本体，有
监督学习是蛋糕上的糖霜，强化学习是蛋糕上的樱桃。”换句话说，无监督学习具有巨大
的潜力，我们才刚刚开始研究。

假设你要创建一个系统，该系统将在制造生产线上为每个产品拍摄几张图片，并检测哪些产品有缺陷。你可以相当容易地创建一个自动拍照系统，这可能每天为你提供数千张图片。然后，你可以在几周内构建一个相当大的数据集。但是，等等，没有标签！如果你想训练一个常规的二元分类器来预测某件产品是否有缺陷，则需要将每张图片标记为“有缺陷”或“正常”。这通常需要人类专家坐下来并手动浏览所有图片。这是一项漫长、昂贵且烦琐的任务，因此通常只能在可用图片的一部分上完成。因此，标记的数据集将非常小，并且分类器的性能将令人失望。而且，公司每次对其产品进行任何更改时，都需要从头开始整个过程。如果该算法只需要利用未标记的数据而无须人工标记每张图片，那不是很好吗？让我们进入无监督学习。

在第8章中，我们研究了最常见的无监督学习任务：降维。在本章中，我们将研究其
他一些无监督的学习任务和算法：

- **聚类**

    目标是将相似的实例分组到集群中。聚类是很好的工具，用于数据分析、客户细分、推荐系统、搜索引擎、图像分割、半监督学习、降维等。

- **异常检测**

    目的是学习“正常”数据看起来是什么样的，然后将其用于检测异常情况，例如生产线上的缺陷产品或时间序列中的新趋势。

- **密度估算**

    这是**估计生成数据集的随机过程的概率密度函数**（PDF）的任务，密度估算通常用于异常检测：位于非常低密度区域的实例很可能是异常。它对于数据分析和可视化也很有用。

准备好蛋糕了吗？我们将从使用K-Means和DBSCAN进行聚类开始，然后讨论高斯混合模型，并了解如何将它们用于密度估计、聚类和异常检测。

## 9.1 聚类

你在山中徒步旅行时，偶然发现了从未见过的植物。你环顾四周，发现还有很多。它们并不完全相同，但是它们足够相似，你可能知道它们有可能属于同一物种（或至少属于同一属）。你可能需要植物学家告诉你什么是物种，但你当然不需要专家来识别外观相似的物体组。这称为聚类：**识别相似实例并将其分配给相似实例的集群或组**。

就像在分类中一样，每个实例都分配给一个组。但是与分类不同，**聚类是一项无监督任务**。考虑图1：左侧是鸢尾花数据集（在第4章中介绍），其中每个实例的种类（即类）用不同的标记表示。它是一个标记的数据集，非常适合使用逻辑回归、SVM或随机森林分类器等分类算法。右侧是相同的数据集，但是没有标签，因此你不能再使用分类算法。这就是聚类算法的引入之处，它们中的许多算法都可以轻松检测左下角的集群。肉眼也很容易看到，但是右上角的集群由两个不同的子集群组成，并不是很明显。也就是说，数据集具有两个附加特征（萼片长度和宽度），此处未表示，并且聚类算法可以很好地利用所有的特征，因此实际上它们可以很好地识别三个聚类（例如，使用高斯混合模型，在150个实例中，只有5个实例分配给错误的集群）。

![fig01_分类与聚类](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig01_%E5%88%86%E7%B1%BB%E4%B8%8E%E8%81%9A%E7%B1%BB.jpg)

聚类可用于各种应用程序，包括：

- 客户细分

    你可以根据客户的购买记录和他们在网站上的活动对客户进行聚类。这对于了解你的客户是谁以及他们的需求很有用，因此你可以针对每个细分客户调整产品和营销活动。例如，客户细分在推荐系统中可以很有用，可以推荐同一集群中其他用户喜欢的内容。

- 数据分析

    在分析新数据集时，运行聚类算法然后分别分析每个集群。

- 降维技术

    数据集聚类后，通常可以测量每个实例与每个集群的相似度（相似度是衡量一个实例和一个集群的相似程度）。然后可以将每个实例的特征向量x替换为其集群的向量。如果有k个集群，则此向量为k维。此向量的维度通常比原始特征向量低得多，但它可以保留足够的信息以进行进一步处理。

- 异常检测（也称为离群值检测）

    对所有集群具有低相似度的任何实例都可能是异常。例如，如果你已根据用户行为对网站用户进行了聚类，则可以检测到具有异常行为的用户，例如每秒的请求数量异常。异常检测在检测制造生产线中的缺陷或欺诈检测中特别有用。

- 半监督学习

    如果你只有几个标签，则可以执行聚类并将标签传播到同一集群中的所有实例。该技术可以大大增加可用于后续有监督学习算法的标签数量，从而提高其性能。

- 搜索引擎

    一些搜索引擎可让你搜索与参考图像相似的图像。要构建这样的系统，首先要对数据库中的所有图像应用聚类算法，相似的图像最终会出现在同一集群中。然后，当用户提供参考图像时，你需要做的就是使用训练好的聚类模型找到该图像的集群，然后可以简单地从该集群中返回所有的图像。

- 分割图像

    通过根据像素的颜色对像素进行聚类，然后用其聚类的平均颜色替换每个像素的颜色，可以显著减少图像中不同颜色的数量。图像分割用于许多物体检测和跟踪系统中，因为它可以更轻松地检测每个物体的轮廓。

关于聚类什么没有统一的定义，它实际上取决于上下文，并且不同的算法会得到不同种类的集群。一些算法会寻找围绕特定点（称为中心点）的实例。其他人则寻找密集实例的连续区域：这些集群可以呈现任何形状。一些算法是分层的，寻找集群中的集群。这样的示例不胜枚举。

在本节中，我们将研究两种流行的聚类算法——K-Means和DBSCAN，并探讨它们的一些应用，例如非线性降维、半监督学习和异常检测。

### 9.1.1 K-Means

考虑图2中所示的未标记数据集：你可以清楚地看到5组实例。K-Means算法是一种简单的算法，能够非常快速、高效地对此类数据集进行聚类，通常只需几次迭代即可。它是由贝尔实验室的Stuart Lloyd在1957年提出的，用于脉冲编码调制，但直到1982年才对外发布。1965年，Edward W.Forgy发布了相同的算法，因此K-Means有时被称为Lloyd–Forgy。

![fig02_由5组实例集群组成的未标记数据集](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig02_%E7%94%B15%E7%BB%84%E5%AE%9E%E4%BE%8B%E9%9B%86%E7%BE%A4%E7%BB%84%E6%88%90%E7%9A%84%E6%9C%AA%E6%A0%87%E8%AE%B0%E6%95%B0%E6%8D%AE%E9%9B%86.jpg)

让我们在该数据集上训练一个K-Means聚类器。它将尝试找到每个集群的中心，并将每个实例分配给最近的集群：

```python
    from sklearn.cluster import KMeans

    k = 5
    kmeans = KMeans(n_clusters=k, random_state=42)
    y_pred = kmeans.fit_predict(X)
```

请注意，你**必须指定这个算法必须要找到的集群数k**。在此例中，通过查看数据可以明显看出k应该设置为5，但总的来说并不是那么容易。我们会简短讨论这个问题。

每个实例都会分配给5个集群之一。在聚类的上下文中，实例的标签是该实例被算法分配给该集群的索引：不要与分类中的类标签相混淆（请记住，聚类是无监督学习任务）。KMeans实例保留了经过训练的实例的标签副本，可通过labels_实例变量得到该副本：

```python
    y_pred
    >>> array([4, 1, 0, ..., 3, 0, 1])
    y_pred is kmeans.labels_
    >>> True
```

我们还可以看一下算法发现的5个中心点：

```python
    kmeans.cluster_centers_
    >>> array([[ 0.20876306,  2.25551336],
       [-2.80389616,  1.80117999],
       [-1.46679593,  2.28585348],
       [-2.79290307,  2.79641063],
       [-2.80037642,  1.30082566]])
```

可以很容易地将新实例分配给中心点最接近的集群：

```python
    X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])
    kmeans.predict(X_new)
    >>> array([0, 0, 3, 3])
```

如果绘制集群的边界，则会得到Voronoi图（参见图3，其中每个中心点都用×表示）。

![fig03_K-Means决策边界](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig03_K-Means%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C.jpg)

绝大多数实例已正确分配给适当的集群，但少数实例可能贴错了标签（尤其是在左上集群和中央集群之间的边界附近）。的确，当集群具有不同的直径时，K-Means算法的性能不是很好，**因为将实例分配给某个集群时，它所关心的只是与中心点的距离**。

与其**将每个实例分配给一个单独的集群**（称为**硬聚类**），不如**为每个实例赋予每个集群一个分数**（称为**软聚类**）会更有用。分数可以是实例与中心点之间的距离。相反，它可以是相似性分数（或远近程度），例如高斯径向基函数（在第5章中介绍）。在KMeans类中，`transform()`方法测量每个实例到每个中心点的距离：

```python
    kmeans.transform(X_new)
    >>> array([[0.32995317, 2.81093633, 1.49439034, 2.9042344 , 2.88633901],
       [2.80290755, 5.80730058, 4.4759332 , 5.84739223, 5.84236351],
       [3.29399768, 1.21475352, 1.69136631, 0.29040966, 1.71086031],
       [3.21806371, 0.72581411, 1.54808703, 0.36159148, 1.21567622]])
```

在此例中，X_new中的第一个实例与第一个中心点的距离为2.81，与第二个中心点的距离为0.33，与第三个中心点的距离为2.90，与第四个中心点的距离为1.49，与第五个中心点的距离为2.89。如果你有一个高维数据集并以这种方式进行转换，那么最终将得到一个k维数据集：这种转换是一种非常有效的非线性降维技术。

#### K-Means算法

那么，该算法如何工作？好吧，假设你得到了中心点。你可以通过将每个实例分配给中心点最接近的集群来标记数据集中的所有实例。相反，如果你有了所有实例的标签，则**可以通过计算每个集群的实例平均值来定位所有的中心点**。但是你既没有标签也没有中心点，那么如何进行呢？好吧，只要从随机放置中心点开始（例如，随机选择k个实例并将其位置用作中心点）。然后标记实例，更新中心点，标记实例，更新中心点，以此类推，直到中心点停止移动。这个算法能保证在有限数量的步骤内收敛（通常很小），不会永远振荡。

你可以在图4中看到正在使用的算法：中心点被随机初始化（左上），然后实例被标记（右上），中心点被更新（左中心），实例被重新标记（右中），以此类推。如你所看到的，仅仅三个迭代，该算法就达到了近似于最佳状态的聚类。

![fig04_K-Means算法](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig04_K-Means%E7%AE%97%E6%B3%95.jpg)

该算法的计算复杂度在实例数m，集群数k和维度n方面通常是线性的。但是，仅当数据具有聚类结构时才如此。如果不是这样，那么在最坏的情况下，复杂度会随着实例数量的增加而呈指数增加。实际上，这种情况很少发生，并且K-Means通常是最快的聚类算法之一。

尽管算法保证会收敛，但它可能不会收敛到正确解（即可能会收敛到局部最优值）：这取决于中心点的初始化。图5显示了两个次优解，如果你不幸地使用随机初始化步骤，算法会收敛到它们。

![fig05_由于中心点初始化不好而导致的次优解](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig05_%E7%94%B1%E4%BA%8E%E4%B8%AD%E5%BF%83%E7%82%B9%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8D%E5%A5%BD%E8%80%8C%E5%AF%BC%E8%87%B4%E7%9A%84%E6%AC%A1%E4%BC%98%E8%A7%A3.jpg)

让我们看一下通过改善中心点初始化来减低这种风险的几种方法。

#### 中心点初始化方法

如果你碰巧知道了中心点应该在哪里（例如，你之前运行了另一种聚类算法），则可以将超参数`init`设置为包含中心点列表的NumPy数组，并将`n_init`设置为1：

```python
    good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])
    kmeans = KMeans(n_clusters=5, init=good_init, n_init=1
```

另一种解决方案是使用不同的随机初始化多次运行算法，并保留最优解。随机初始化的数量由超参数`n_init`控制：默认情况下，它等于10，这意味着当你调用`fit()`时，前述算法运行10次，Scikit-Learn会保留最优解。但是，如何确切知道哪种解答是最优解呢？它使用性能指标！这个指标称为模型的惯性，即每个实例与其最接近的中心点之间的均方距离。对于图5中左边的模型，它大约等于223.3；对于图5中右边的模型，它等于237.5；对于图3中的模型，它等于211.6。KMeans类运行`n_init`次算法，并保留模型的最低惯性。在此例中，将选择图3中的模型（除非我们非常不幸地使用`n_init`连续随机初始化）。如果你有意，可以通过`inertia_instance`变量来访问模型的惯性：

```python
    kmeans.inertia_
    >>> 211.59853725816828
```

`score()`方法返回负惯性。为什么是负的？因为预测器的`score()`方法必须始终遵循Scikit-Learn的“越大越好”的规则，即如果一个预测器比另一个更好，则其`score()`方法应该返回更大的数：

```python
    kmeans.score(X)
    -211.5985372581683
```

David Arthur和Sergei Vassilvitskii在2006年的论文中提出了对K-Means算法的重要改进——K-Means++。他们引入了一种更智能的初始化步骤，该步骤倾向于选择彼此相距较远的中心点，这一改进使得K-Means算法收敛到次优解的可能性很小。他们表明，更智能的初始化步骤所需的额外计算量是值得的，因为它可以大大减少寻找最优解所需运行算法的次数。以下是K-Means++的初始化算法：

1. 取一个中心点 $c^{(1)}$，从数据集中随机选择一个中心点。

2. 取一个新中心点 $c^{(i)}$，选择一个概率为 $D(x^{(i)})^2 / \sum^m_{j=1}D(x^{(j)})^2$ 的实例 $x^{(i)}$，其中 $D(x^{(i)})$ 是实例 $x^{(i)}$ 与已经选择的最远中心点的距离。这种概率分布确保距离已选择的中心点较远的实例有更大可能性被选择为中心点。

3. 重复上一步，直到选择了所有k个中心点。

默认情况下，KMeans类使用这种初始化方法。如果要强制使用原始方法（即随机选择k个实例来初始化中心点），则可以将超参数`init`设置为"random"。你很少需要这样做。

#### 加速的K-Means和小批量K-Means

Charles Elkan在2003年的一篇论文中提出了对K-Means算法的另一个重要改进。该改进后的算法通过避免许多不必要的距离计算，大大加快了算法的速度。Elkan通过利用三角不等式（即一条直线是两点之间的最短距离）并通过跟踪实例和中心点之间的上下限距离来实现这一目的。这是KMeans类默认使用的算法（你可以通过将超参数algorithm设置为"full"来强制使用原始算法，尽管你可能永远也不需要）。

David Sculley在2010年的一篇论文中提出了K-Means算法的另一个重要变体。该算法能够在每次迭代中使用小批量K-Means稍微移动中心点，而不是在每次迭代中使用完整的数据集。这将算法的速度提高了3到4倍，并且可以对不容纳内存的大数据集进行聚类。Scikit-Learn在MiniBatchKMeans类中实现了此算法。你可以像KMeans类一样使用此类：

```python
    from sklearn.cluster import MiniBatchKMeans
    minibatch_kmeans = MiniBatchKMeans(n_clusters=5)
    minibatch_kmeans.fit(X)
```

如果数据集无法容纳在内存中，则最简单的选择是使用`memmap`类，就像我们在第8章中对增量PCA所做的那样。或者你可以一次将一个小批量传给`partial_fit()`方法，但这需要更多的工作，因为你需要执行多次初始化并选择一个最好的初始化（有关示例请参阅notebook的小批量处理K-Means部分）。

尽管小批量K-Means算法比常规K-Means算法快得多，但其惯性通常略差一些，尤其是随着集群的增加。你可以在图6中看到这一点：左侧的图比较了小批量K-Means和常规K-Means模型的惯性，这是在先前数据集上使用各种不同k训练而来的。两条曲线之间的差异保持相当恒定，但是随着k的增加，该差异变得越来越大，因为惯性变得越来越小。在右边的图中，你可以看到小批量K-Means比常规K-Means快得多，并且该差异随k的增加而增加。

![fig06_小批量K-Means](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig06_%E5%B0%8F%E6%89%B9%E9%87%8FK-Means.jpg)

#### 寻找最佳聚类数

到目前为止，我们将集群数k设置为5，因为通过查看数据可以明显看出这是正确的集群数。但是总的来说，知道如何设置k并不容易，如果将k设置为错误值，结果可能会很糟糕。如图7所示，将k设置为3或8会导致非常糟糕的模型。

![fig07_集群数量的错误选择](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig07_%E9%9B%86%E7%BE%A4%E6%95%B0%E9%87%8F%E7%9A%84%E9%94%99%E8%AF%AF%E9%80%89%E6%8B%A9.jpg)

你可能会认为我们可以选择惯性最低的模型，对吗？不是那么简单！k=3的惯性是
653.2，比k=5（211.6）大得多。但是在k=8的情况下，惯性仅为119.1。尝试选择k时，惯性不是一个好的性能指标，因为随着k的增加，惯性会不断降低。实际上，集群越多，每个实例将越接近其最接近的中心点，因此惯性将越低。让我们将惯性作为k的函数作图（见图8）。

![fig08_集群k的函数曲线](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig08_%E9%9B%86%E7%BE%A4k%E7%9A%84%E5%87%BD%E6%95%B0%E6%9B%B2%E7%BA%BF.jpg)

如你所见，随着k增大到4，惯性下降得很快，但随着k继续增大，惯性下降得很慢。该曲线大致像手臂形状，在k=4处有一个“肘”形。因此，如果我们不知道更好的选择，则4是一个不错的选择：**较低的值变化比较大，而较高的值没有多大帮助，我们有时可以把集群再细分成两半**。

为集群数选择最佳值的技术相当粗糙。一种更精确的方法（但也需要更多的计算）是使用**轮廓分数**，它是所有实例的**平均轮廓系数**。实例的轮廓系数等于(b-a)/max(a,b)，其中a是与同一集群中其他实例的平均距离（即集群内平均距离），b是平均最近集群距离（即到下一个最近集群实例的平均距离，定义为使b最小的那个，不包括实例自身的集群）。轮廓系数可以在-1和+1之间变化。接近+1的系数表示该实例很好地位于其自身的集群中，并且远离其他集群，而接近0的系数表示该实例接近一个集群的边界，最后一个接近-1的系数意味着该实例可能已分配给错误的集群。

要计算轮廓分数，可以使用Scikit-Learn的`silhouette_score()`函数，为它输入数据集中的所有实例以及为其分配的标签：

```python
    >>> from sklearn.metrics import silhouette_score
    >>> silhouette_score(X, kmeans.labels_)
    0.65551764257282
```

让我们比较不同集群数量的轮廓分数（见图9）。

![fig09_使用轮廓分数选择集群数k](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig09_%E4%BD%BF%E7%94%A8%E8%BD%AE%E5%BB%93%E5%88%86%E6%95%B0%E9%80%89%E6%8B%A9%E9%9B%86%E7%BE%A4%E6%95%B0k.jpg)

如你所见，这个可视化效果比上一个更加丰富：尽管它确认k=4是一个很好的选择，但它也说明一个事实，即k=5也非常好，并且比k=6或者k=7要好得多。这个在比较惯性时看不到。

当你绘制每个实例的轮廓系数时，可以获得更加丰富的可视化效果，这些轮廓系数按它们分配的集群和系数值进行排序，这称为轮廓图（见图10）。每个图包含一个刀形的聚类。形状的高度表示集群包含的实例数，宽度表示集群中实例的排序轮廓系数（越宽越好）。虚线表示平均轮廓系数。

![fig10_各种K值的轮廓图分析](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig10_%E5%90%84%E7%A7%8DK%E5%80%BC%E7%9A%84%E8%BD%AE%E5%BB%93%E5%9B%BE%E5%88%86%E6%9E%90.jpg)

垂直虚线表示每个集群的轮廓分数。当集群中的大多数实例的系数均低于此分数时（如果许多实例在虚线附近停止，在其左侧结束），则该集群比较糟糕，因为这意味着其实例太接近其他集群了。可以看到，当k=3或者k=6时，我们得到了不好的集群。但是当k=4或k=5时，集群看起来很好：大多数实例都超出虚线，向右延伸并接近1.0。当k=4时，索引为1（从顶部开始的第三个）的集群很大。当k=5时，所有集群的大小都相似。因此，即使k=4的整体轮廓得分略大于k=5的轮廓得分，使用k=5来获得相似大小的集群似乎也是一个好主意。

### 9.1.2 K-Means的局限

尽管K-Means有许多优点，尤其是快速且可扩展，但它并不完美。如我们所见，必须多次运行该算法才能避免次优解，此外，你还需要指定集群数，这很麻烦。此外，当集群具有不同的大小、不同的密度或非球形时，K-Means的表现也不佳。例如，图11显示了K-Means如何对包含三个不同尺寸、密度和方向的椭圆形集群的数据集进行聚类。

![fig11_K-Means无法正确聚类这些椭圆形集群](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig11_K-Means%E6%97%A0%E6%B3%95%E6%AD%A3%E7%A1%AE%E8%81%9A%E7%B1%BB%E8%BF%99%E4%BA%9B%E6%A4%AD%E5%9C%86%E5%BD%A2%E9%9B%86%E7%BE%A4.jpg)

如你所见，这些聚类的结果都不好。左边的结果好一点，但是它仍然砍掉了中间集群的25%，并将其分配给右边的集群。即使其惯性较低，右侧的结果也很糟糕。因此，根据数据，不同的聚类算法可能会表现好一点。在这种类型的椭圆簇上，高斯混合模型非常有效。

在运行K-Means之前，请先缩放输入特征，否则集群可能会变形，这样K-Means的性能会很差。缩放特征并不能保证所有的集群都很好，但是通常可以改善结果。

现在让我们看一些可以从聚类中受益的方法。我们将使用K-Means，但可以随时试验其他的聚类算法。

### 9.1.3 使用聚类进行图像分割

**图像分割是将图像分成多个分割的任务**。在语义分割中，属于同一对象类型的所有像素均被分配给同一分割。例如，在无人驾驶汽车的视觉系统中，可能会将行人图像中的所有像素都分配给“行人”（会有一个包含所有行人的分割）。在实例分割中，属于同一单个对象的所有像素都分配给同一分割。在这种情况下，每个行人会有不同的分割。如今，使用最新技术的基于卷积神经网络的复杂结构可以实现语义分割或实例分割（见第14章）。在这里，我们做一些简单的事情：**颜色分割**。如果像素具有相似的颜色，我们将简单地将它们分配给同一分割。在某些应用中，这可能就足够了。例如，如果要分析卫星图像以测量某个区域中有多少森林总面积，则颜色分割就很好了。

首先，使用Matplotlib的`imread()`函数加载图像（见图12的左上方图像）：

```python
    from matplotlib.image import imread
    image = imread(os.path.join(images_path, filename))
    image.shape
```

**图像表示为3D数组**。第一维的大小是高度，第二个是宽度，第三个是颜色通道的数量，在这种情况下为红色、绿色和蓝色（RGB）。换句话说，对于每个像素都有一个3D向量，包含红色、绿色和蓝色的强度，强度在0.0和1.0之间（如果使用`imageio.imread()`，则在0和255之间）。某些图像可能具有较少的通道，例如灰度图像（一个通道）。某些图像可能具有更多的通道，例如具有透明度的alpha通道的图像或卫星图像，这些图像通常包含许多光频率（例如红外）的通道。以下代码对数组进行重构以得到RGB颜色的长列表，然后使用K-Means将这些颜色聚类：

```python
    X = image.reshape(-1, 3)
    kmeans = KMeans(n_clusters=8, random_state=42).fit(X)
    segmented_img = kmeans.cluster_centers_[kmeans.labels_]
    segmented_img = segmented_img.reshape(image.shape)
```

例如，它可以识别所有绿色阴影的颜色聚类。接下来，对于每种颜色（例如深绿色），它会寻找像素颜色集群的平均颜色。例如，所有绿色阴影都可以用相同的浅绿色代替（假设绿色集群的平均颜色是浅绿色）。最后，它会重新调整颜色的长列表，使其形状与原始图像相同。我们完成了！

这样就输出了图12右上方所示的图像。你可以尝试各种数量的集群，如图12所示。当你使用少于8个集群时，请注意，瓢虫的闪亮红色不能聚成单独的一类，它将与环境中的颜色合并。这是因为K-Means更喜欢相似大小的集群。瓢虫很小，比图像的其余部分要小得多，所以即使它的颜色是鲜明的，K-Means也不能为它指定一个集群。

![fig12_使用各种数量的颜色聚类K-Means进行图像分割](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig12_%E4%BD%BF%E7%94%A8%E5%90%84%E7%A7%8D%E6%95%B0%E9%87%8F%E7%9A%84%E9%A2%9C%E8%89%B2%E8%81%9A%E7%B1%BBK-Means%E8%BF%9B%E8%A1%8C%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2.jpg)

不是很难，不是吗？现在让我们看一下聚类的另一个应用：预处理。

### 9.1.4 使用聚类进行预处理

聚类是一种有效的降维方法，特别是**作为有监督学习算法之前的预处理步骤**。作为使用聚类进行降维的示例，让我们处理数字数据集，它是一个类似于MNIST的简单数据集，其中包含了表示从0到9数字的1797个灰度8×8图像。首先，加载数据集：

```python
    from sklearn.datasets import load_digits

    X_digits, y_digits = load_digits(return_X_y=True)
```

将其分为训练集和测试集：

```python
    from sklearn.model_selection import train_test_split

    X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=42)
```

拟合一个逻辑回归模型：

```python
    from sklearn.linear_model import LogisticRegression

    log_reg = LogisticRegression(multi_class="ovr", solver="lbfgs", max_iter=5000, random_state=42)
    log_reg.fit(X_train, y_train)
```

在测试集上评估其精度：

```python
    log_reg_score = log_reg.score(X_test, y_test)
    log_reg_score
    >>> 0.9688888888888889
```

这就是我们的基准：准确率为96.9%。让我们看看通过使用K-Means作为预处理是否可以做得更好。我们创建一个流水线，该流水线首先将训练集聚类为50个集群，并将图像替换为其与这50个集群的距离，然后应用逻辑回归模型：

```python
    from sklearn.pipeline import Pipeline

    pipeline = Pipeline([
        ("kmeans", KMeans(n_clusters=50, random_state=42)),
        ("log_reg", LogisticRegression(multi_class="ovr", solver="lbfgs", max_iter=5000, random_state=42)),
    ])
    pipeline.fit(X_train, y_train)
```

由于有10个不同的数字，因此很容易将集群数量设置为10。但是，每个数字可以用几种不同的方式写入，因此最好使用更大的集群数量，例如50。评估这个分类流水线：

```python
    pipeline_score = pipeline.score(X_test, y_test)
    pipeline_score
    >>> 0.9777777777777777
```

怎么样？我们把错误率降低了近30%（从大约3.1%降低到大约2.2%）！

但是我们是任意选择集群k，我们当然可以做得更好。由于K-Means只是分类流水线中的预处理步骤，因此为k找到一个好的值要比以前简单得多。无须执行轮廓分析或最小化惯性。k的最优解是在交叉验证过程中的最佳分类性能的值。我们可以使用`GridSearchCV`来找到集群的最优解：

```python
    from sklearn.model_selection import GridSearchCV

    param_grid = dict(kmeans__n_clusters=range(2,100))
    grid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)
    grid_clf.fit(X_train, y_train)
```

让我们看一下k的最优解和流水线的性能：

```python
    grid_clf.best_params_
    >>>
    grid_clf.score(X_test, y_test)
    >>>
```

在k=99个聚类的情况下，我们获得了显著的精度提升，在测试集上达到了98.22%的精度。你可以继续探索更高值的k，因为99是我们探索范围内的最大值。

### 9.1.5 使用聚类进行半监督学习

聚类的另一个应用是在半监督学习中，我们有很多未标记的实例，而很少带标签的实例。让我们在digits数据集的50个带标签实例的样本上训练逻辑回归模型：

```python
    n_labeled = 50
    log_reg = LogisticRegression(multi_class="ovr", solver="lbfgs", random_state=42)
    log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])
    log_reg.score(X_test, y_test)
    >>> 0.8333333333333334
```

精度仅为83.3%。毫不奇怪，这比我们在完整的训练集上训练模型时要低得多。让我们看看如何能做得更好。首先，我们将训练集聚类为50个集群。然后，对于每个集群，找到最接近中心点的图像。我们将这些图像称为代表性图像：

```python
    k = 50
    kmeans = KMeans(n_clusters=k, random_state=42)
    X_digits_dist = kmeans.fit_transform(X_train)
    representative_digit_idx = np.argmin(X_digits_dist, axis=0)
    X_representative_digits = X_train[representative_digit_idx]
```

图13显示了这50张代表性图像。

![fig13_50个代表性数字图像](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig13_50%E4%B8%AA%E4%BB%A3%E8%A1%A8%E6%80%A7%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F.jpg)

让我们看一下每个图像并手动标记它：

```python
    y_representative_digits = np.array([
        0, 1, 3, 2, 7, 6, 4, 6, 9, 5,
        1, 2, 9, 5, 2, 7, 8, 1, 8, 6,
        3, 2, 5, 4, 5, 4, 0, 3, 2, 6,
        1, 7, 7, 9, 1, 8, 6, 5, 4, 8,
        5, 3, 3, 6, 7, 9, 7, 8, 4, 9])
```

现在，我们有一个只有50个带标签实例的数据集，但是它们不是随机实例，每个实例都是其集群的代表图像。让我们看看性能是否更好：

```python
    log_reg = LogisticRegression(multi_class="ovr", solver="lbfgs", max_iter=5000, random_state=42)
    log_reg.fit(X_representative_digits, y_representative_digits)
    log_reg.score(X_test, y_test)
    >>> 0.9222222222222223
```

哇！尽管我们仍然仅在50个实例上训练模型，但准确性从83.3%跃升至92.2%。由于标记实例通常很昂贵且很痛苦，尤其是当必须由专家手动完成时，标记代表性实例（而不是随机实例）是一个好主意。

但是也许我们可以更进一步：如果将标签传播到同一集群中的所有其他实例，会怎么样？这称为标签传播：

```python
    y_train_propagated = np.empty(len(X_train), dtype=np.int32)
    for i in range(k):
        y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]

    log_reg = LogisticRegression(multi_class="ovr", solver="lbfgs", max_iter=5000, random_state=42)
    log_reg.fit(X_train, y_train_propagated)
    log_reg.score(X_test, y_test)
    >>> 0.9333333333333333
```

我们获得了合理的精度提升，这不让人惊奇。问题在于，我们把每个代表性实例的标签传播到同一集群中的所有实例，包括位于集群边界附近的实例，这些实例更容易被错误标记。让我们看看如果仅将标签传播到最接近中心点的20%的实例，会发生什么情况：

```python
    percentile_closest = 75

    X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]
    for i in range(k):
        in_cluster = (kmeans.labels_ == i)
        cluster_dist = X_cluster_dist[in_cluster]
        cutoff_distance = np.percentile(cluster_dist, percentile_closest)
        above_cutoff = (X_cluster_dist > cutoff_distance)
        X_cluster_dist[in_cluster & above_cutoff] = -1

    partially_propagated = (X_cluster_dist != -1)
    X_train_partially_propagated = X_train[partially_propagated]
    y_train_partially_propagated = y_train_propagated[partially_propagated]
```

在这个部分传播的数据集上再次训练模型：

```python
    log_reg = LogisticRegression(multi_class="ovr", solver="lbfgs", max_iter=5000, random_state=42)
    log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)

    log_reg.score(X_test, y_test)
    >>> 0.9355555555555556
```

很好！仅使用50个带标签的实例（平均每个类只有5个实例！），我们获得了93.5%的准确率，与整个带标签的数字数据集上逻辑回归的性能非常接近（96.9%）。之所以如此出色，是因为传播的标签实际上非常好，它们的准确率非常接近99%，如以下代码所示：

```python
    np.mean(y_train_partially_propagated == y_train[partially_propagated])
```

**主动学习**：

为了继续改进模型和训练集，下一步可能是进行几轮主动学习，即当人类专家与学习算法进行交互时，在算法要求时为特定实例提供标签。主动学习有许多不同的策略，但最常见的策略之一称为不确定性采样。下面是它的工作原理：

1. 该模型是在目前为止收集到的带标签的实例上进行训练，并且该模型用于对所有未标记实例进行预测。

2. 把模型预测最不确定的实例（当估计的概率最低时）提供给专家来做标记。

3. 重复此过程，直到性能改进到不值得做标记为止。

其他策略包括标记那些会导致模型最大变化的实例或模型验证错误下降最大的实例，或不同模型不一致的实例（例如SVM或随机森林）。

在进入高斯混合模型之前，让我们看一下DBSCAN，这是另一种流行的聚类算法，它说明了一种基于局部密度估计的不同的方法。这种方法允许算法识别任意形状的聚类。

### 9.1.6 DBSCAN

该算法将集群定义为高密度的连续区域。下面是它的工作原理：

- 对于每个实例，该算法都会计算在距它一小段距离ε内有多少个实例。该区域称为实例的**ε-邻域**。

- 如果一个实例在其ε邻域中至少包含min sam_ples个实例（包括自身），则该实例被视为核心实例。换句话说，核心实例是位于密集区域中的实例。

- 核心实例附近的所有实例都属于同一集群。这个邻域可能包括其他核心实例。因此，一长串相邻的核心实例形成一个集群。

- 任何不是核心实例且邻居中没有实例的实例都被视为异常。

如果所有集群足够密集并且被低密度区域很好地分隔开，则该算法效果很好。Scikit-Learn中的DBSCAN类就像你期望的那样易于使用。让我们在第5章介绍的moons数据集上进行测试：

```python
    from sklearn.datasets import make_moons
    X, y = make_moons(n_samples=1000, noise=0.05, random_state=42)
    from sklearn.cluster import DBSCAN
    dbscan = DBSCAN(eps=0.05, min_samples=5)
    dbscan.fit(X)
```

在labels_实例变量中可以得到所有实例的标签：

```python
    dbscan.labels_[:10]
    >>> array([ 0,  2, -1, -1,  1,  0,  0,  0,  2,  5], dtype=int64)
```

请注意，某些实例的集群索引等于-1，这意味着该算法将它们视为异常。`core_sample_indices_`变量可以得到核心实例的索引，`components_`变量可以得到核心实例本身：

```python
    len(dbscan.core_sample_indices_)
    >>> 808
    dbscan.core_sample_indices_[:10]
    >>> array([ 0,  4,  5,  6,  7,  8, 10, 11, 12, 13], dtype=int64)
    dbscan.components_[:3]
    >>> array([[-0.02137124,  0.40618608],
       [-0.84192557,  0.53058695],
       [ 0.58930337, -0.32137599]])
```

这种聚类在图14的左侧图中表示。如你所见，它识别出很多异常以及7个不同的集群。很令人失望！幸运的是，如果通过将eps增大到0.2来扩大每个实例的邻域，则可以得到右边的聚类，看起来很完美。让我们继续这个模型。

![fig14_使用两个不同邻域半径的DBSCAN聚类](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig14_%E4%BD%BF%E7%94%A8%E4%B8%A4%E4%B8%AA%E4%B8%8D%E5%90%8C%E9%82%BB%E5%9F%9F%E5%8D%8A%E5%BE%84%E7%9A%84DBSCAN%E8%81%9A%E7%B1%BB.jpg)

出乎意料的是，尽管DBSCAN类具有`fit_predict()`方法，但它没有`predict()`方法。换句话说，它无法预测新实例属于哪个集群。之所以做出这个决定是因为不同的分类算法可以更好地完成不同的任务，因此作者决定让用户选择使用哪种算法。而且实现起来并不难。例如，让我们训练一个`KNeighborsClassifier`：

```python
    from sklearn.neighbors import KNeighborsClassifier
    knn = KNeighborsClassifier(n_neighbors=50)
    knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])
```

现在，给出一些新实例，我们可以预测它们最有可能属于哪个集群，甚至可以估计每个集群的概率：

```python
    X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])
    knn.predict(X_new)
    >>> array([1, 0, 1, 0], dtype=int64)
    knn.predict_proba(X_new)
    >>> array([[0.18, 0.82],
       [1.  , 0.  ],
       [0.12, 0.88],
       [1.  , 0.  ]])
```

请注意，我们仅在核心实例上训练了分类器，但是我们也可以选择在所有实例或除异常之外的所有实例上训练分类器，这取决于你的任务。

决策边界如图15所示（十字表示X_new中的四个实例）。注意，由于训练集中没有异常，分类器总是选择一个集群，即使该集群很远也是如此。直接引入最大距离的概念，在这种情况下，距离两个集群均较远的两个实例被归类为异常。为此，请使用`KNeighborsClassifier的kneighbors()`方法。给定一组实例，它返回训练集中k个最近邻居的距离和索引（两个矩阵，每个有k列）：

```python
    y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)
    y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]
    y_pred[y_dist > 0.2] = -1
    y_pred.ravel()
    >>> array([-1,  0,  1, -1], dtype=int64)
```

![fig15_两个聚类之间的决策边界]()

简而言之，DBSCAN是一种非常简单但功能强大的算法，能够识别任何数量的任何形状的集群。它对异常值具有鲁棒性，并且只有两个超参数（`eps`和`min_samples`）。但是，如果密度在整个集群中变化很大，则可能无法正确识别所有的集群。它的计算复杂度大约为`O(mlogm)`，使其在实例数量上非常接近线性，但是如果`eps`大，Scikit-Learn的实现可能需要多达`O(m^2)的内存。

### 9.1.7 其他聚类算法

Scikit-Learn还实现了另外几种你应该了解的聚类算法。我们无法在此处详细介绍所有内容，以下是一个简要概述：

1. 聚集聚类

    集群的层次结构是自下而上构建的。想想许多微小的气泡漂浮在水上并逐渐相互附着在一起，直到有一大群气泡。同样，在每次迭代中，聚集聚类连接最近的一对集群（从单个实例开始）。如果为每对合并的集群绘制一个带有分支的树，将得到一个二叉树集群，其中叶子是各个实例。这种方法可以很好地扩展到大量实例或集群。它可以识别各种形状的集群，生成灵活且信息丰富的集群树，而不强迫你选择特定的集群规模，并且可以与任何成对的距离一起使用。如果你有一个连通矩阵，它可以很好地扩展到大量实例，该矩阵是一个稀疏m×m矩阵，它表明哪些实例对是相邻的（例如，由`sklearn.neighbors.kneighbors_graph()`返回）。没有连通矩阵，该算法就无法很好地扩展到大型数据集。

2. BIRCH

    BIRCH（使用层次结构的平衡迭代约简和聚类）算法是专门为非常大的数据集设计的，并且只要特征数量不太大（<20），它可以比批量处理的K-Means更快，结果相似。在训练期间，它构建了一个树结构，该树结构仅包含足以将每个新实例快速分配给集群的信息，而不必将所有实例存储在树中，这种方法使它在处理大型数据集时使用有限的内存。

3. 均值漂移

    此算法从开始在每个实例上居中放置一个圆圈。然后，对于每个圆，它会计算位于其中的所有实例的均值，然后移动圆，使其以均值为中心。接下来，迭代此均值移动步骤，直到所有圆都停止移动（直到每个圆都以其包含的实例的均值为中心）为止。均值漂移将圆向较高密度的方向移动，直到每个圆都找到了局部最大密度。最后，所有圆已经落在同一位置（或足够近）的实例都分配给同一集群。均值漂移具有与DBSCAN相同的特性，例如如何找到具有任何形状的任意数量的集群，它的超参数很少（仅一个圆的半径，称为带宽），并且依赖于局部密度估计。但是，与DBSCAN不同，均值漂移在有内部密度变化时，集群会分裂成小块。不幸的是，其计算复杂度为`O(m^2)`，因此不适用于大型数据集。

4. 相似性传播
    
    此算法使用投票机制，实例对相似的实例进行投票将其作为代表，一旦该算法收敛，每个代表及其投票者将组成一个集群。相似性传播可以检测任意数量的不同大小的集群。不幸的是，该算法的计算复杂度为`O(m^2)`，因此也不适用于大型数据集。

5. 谱聚类

    该算法采用实例之间的相似度矩阵，并由其创建低维嵌入（即降维），然后在该低维空间中使用另一种聚类算法（Scikit-Learn使用K-Means实现）。谱聚类可以识别复杂的集群结构，也可以用于分割图（例如，识别社交网络上的朋友集群）。它不能很好地扩展到大量实例，并且当集群的大小不同时，它的效果也不好。

## 9.2 高斯混合模型

**高斯混合模型（GMM）是一种概率模型**，它假定实例是由多个参数未知的高斯分布的混合生成的。从单个高斯分布生成的所有实例都形成一个集群，通常看起来像一个椭圆。每个集群可以具有不同的椭圆形状、大小、密度和方向，如图11所示。当你观察到一个实例时，知道它是从一个高斯分布中生成的，但是不知道是哪个高斯分布，并且你也不知道这些分布的参数是什么。

几种GMM变体。在GaussianMixture类中实现的最简单变体中，你必须事先知道高斯分布的数量k。假定数据集X是通过以下概率过程生成的：

- 对于每个实例，从k个集群中随机选择一个集群。选择第j个集群的概率由集群的权重 $φ^{(j)}$ 定义。第i个实例选择的集群的索引记为 $z^{(i)}$。

- 如果 $z^{(i)}=j$ ，表示第i个实例已分配给第j个集群，则从高斯分布中随机采样该实例的位置 $x^{(i)}$，其中均值 $μ^{(j)}$ 和协方差矩阵 $Σ^{(j)}$。这记为 $x^{(i)}~N(μ^{(j)},$Σ^{(j)}$)$。

该生成过程可以表示为图模型。图16表示随机变量之间的条件依存关系的结构，其中结构包括其参数（正方形）、随机变量（圆形）及其条件依存关系（实线箭头）。

![fig16_高斯混合模型的图形表示](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig16_%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9B%BE%E5%BD%A2%E8%A1%A8%E7%A4%BA.jpg)

这是该图的解释：

- 圆圈代表随机变量。

- 正方形代表固定值（即模型的参数）。

- 大矩形称为平板。它们表示其内容重复了几次。

- 每个平板右下角的数字表示其内容重复了多少次。因此，有m个随机变量 $z^{(i)}$ （从 $z^{(1)}$ 到 $z^{(m)}$）和m个随机变量 $x^{(i)}$，还有k个均值 $μ^{(j)}$ 和k个协方差矩阵 $Σ^{(j)}$。最后，只有一个权重向量φ（包含所有权重 $φ^{(1)}$ 到 $φ^{(k)}$ ）。

- 每个变量 $z^{(i)}$ 从具有权重φ的分类分布中得出。每个变量 $x^{(i)}$ 从正态分布中得出，均值和协方差矩阵由其集群 $z^{(i)}$ 定义。

- 实线箭头表示条件依赖性。例如，每个随机变量 $z^{(i)}$ 的概率分布取决于权重向量φ。请注意，当箭头越过平板边界时，这意味着它适用于该平板的所有重复。例如，权重向量φ确定所有随机变量 $x^{(1)}$ 到 $x^{(m)}$ 的概率分布。

- 从 $z^{(i)}$ 到 $x^{(i)}$ 的弯曲箭头表示一个切换：根据 $z^{(i)}$ 的值，将从不同的高斯分布中采样实例 $x^{(i)}$ 。例如，如果 $z^{(i)}=j$ ，则 $x^{(i)}~(μ^{(j)},Σ^{(j)})$。

- 阴影节点表示该值是已知的。因此，在这种情况下，只有随机变量 $x^{(i)}$ 具有已知值，它们被称为已观察变量。未知的随机变量 $z^{(i)}$ 称为潜在变量。

那么，你可以用这种模型做什么？好吧，给定数据集X，你通常希望首先估计权重φ以及所有分布参数 $φ^{(1)}$ 至 $φ^{(k)}$ 和 $Σ^{(1)}$ 至 $Σ^{(k)}$ 。Scikit-Learn的`GaussianMixture`类使这个超级简单：

```python
    from sklearn.mixture import GaussianMixture
    gm = GaussianMixture(n_components=3, n_init=10, random_state=42)
    gm.fit(X)
```

算法估计的参数：

```python
    gm.weights_
    >>> array([0.39025715, 0.40007391, 0.20966893])
    gm.means_
    >>> array([[ 0.05131611,  0.07521837],
       [-1.40763156,  1.42708225],
       [ 3.39893794,  1.05928897]])
    gm.covariances_
    >>> array([[[ 0.68799922,  0.79606357],
        [ 0.79606357,  1.21236106]],

       [[ 0.63479409,  0.72970799],
        [ 0.72970799,  1.1610351 ]],

       [[ 1.14833585, -0.03256179],
        [-0.03256179,  0.95490931]]])
```

效果很好！实际上，用于生成数据的权重分别为0.2、0.4和0.4。同样，均值和协方差矩阵与该算法发现的非常接近。但这是怎么做到的？这个类依赖于期望最大化（Expectation-Maximization，EM）算法，该算法与K-Means算法有很多相似之处：它随机初始化集群参数，然后重复两个步骤直到收敛，首先将实例分配给集群（这称为期望步骤），然后更新集群（这称为最大化步骤）。听起来很熟悉，对吧？在聚类的上下文中，你可以将EM看作是K-Means的推广，它不仅可以找到集群中心（$μ^{(1)}$ 到 $μ^{(k)}$ ），而且可以找到它们的大小、形状和方向（$Σ^{(1)}$ 到 $Σ^{(k)}$ ）以及它们的相对权重（ $φ^{(1)}$ 到 $φ^{(k)}$ ）。但是与K-Means不同，EM使用软集群分配而不是硬分配。对于每个实例，在期望步骤中，该算法（基于当前的集群参数）估计它属于每个集群的概率。然后，在最大化步骤中，使用数据集中的所有实例来更新每个聚类，并通过每个实例属于该集群的估计概率对其进行加权。这些概率称为实例集群的职责。在最大化步骤中，每个集群的更新主要受到其最负责的实例的影响。

不幸的是，就像K-Means一样，EM最终可能会收敛到较差的解，因此它需要运行几次，仅仅保留最优解。这就是我们将n_init设置为10的原因。请注意，默认情况下，`n_init`设置为1。

检查算法是否收敛以及迭代次数：

```python
    gm.converged_
    >>> True
    gm.n_iter_
    >>> 4
```

现在，你已经估算了每个集群的位置、大小、形状、方向和相对权重，该模型可以容易地将每个实例分配给最可能的集群（硬聚类）或估算它属于特定集群的概率（软聚类）。只需对硬聚类使用`predict()`方法，对软聚类使用`predict_proba()`方法：

```python
    gm.predict(X)
    >>> array([0, 0, 1, ..., 2, 2, 2], dtype=int64)
    gm.predict_proba(X)
    >>> array([[9.76741808e-01, 6.78581203e-07, 2.32575136e-02],
       [9.82832955e-01, 6.76173663e-04, 1.64908714e-02],
       [7.46494398e-05, 9.99923327e-01, 2.02398402e-06],
       ...,
       [4.26050456e-07, 2.15512941e-26, 9.99999574e-01],
       [5.04987704e-16, 1.48083217e-41, 1.00000000e+00],
       [2.24602826e-15, 8.11457779e-41, 1.00000000e+00]])
```

高斯混合模型是一个生成模型，这意味着你可以从中采样新实例（请注意，它们按集群索引排序）：

```python
    X_new, y_new = gm.sample(6)
    X_new
    >>> array([[-0.86944074, -0.32767626],
       [ 0.29836051,  0.28297011],
       [-2.8014927 , -0.09047309],
       [ 3.98203732,  1.49951491],
       [ 3.81677148,  0.53095244],
       [ 2.84104923, -0.73858639]])
    y_new
    >>> array([0, 0, 1, 2, 2, 2])
```

也可以在任何给定位置估计模型的密度。这可以使用`score_samples()`方法实现的：对于每个给定的实例，该方法估算该位置的概率密度函数（PDF）的对数。分数越高，密度越高：

```python
    gm.score_samples(X)
    >>> array([-2.60768954, -3.57110232, -3.32987086, ..., -3.51347241,
       -4.39798588, -3.80746532])
```

如果你计算这些分数的指数，可以在给定实例的位置得到PDF的值。这些不是概率，而是概率密度，它们可以取任何正值（不只是0到1之间的值）。要估计实例落入特定区域的概率，你必须在该区域上积分PDF（如果你在可能的实例位置的整个空间中执行此操作，结果将为1）。

图17显示了该模型的集群均值、决策边界（虚线）和密度等值线。

![fig17_训练过的高斯混合模型的均值、决策边界和密度等值线](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig17_%E8%AE%AD%E7%BB%83%E8%BF%87%E7%9A%84%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9D%87%E5%80%BC%E3%80%81%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C%E5%92%8C%E5%AF%86%E5%BA%A6%E7%AD%89%E5%80%BC%E7%BA%BF.jpg)

很好！该算法显然找到了一个很好的解答。当然，我们通过使用一组二维高斯分布生成数据来简化其任务（不幸的是，现实生活中的数据并不总是那么高斯和低维的）。我们还为算法提供了正确数量的集群。当存在多个维度、多个集群或很少实例时，EM可能难以收敛到最佳解决方案。你可能需要通过限制算法必须学习的参数数量来降低任务的难度。一种方法是限制集群可能具有的形状和方向的范围，这可以通过对协方差矩阵施加约束来实现。为此，将`covariance_type`超参数设置为以下值之一：

"spherical"：所有集群都必须是球形的，但它们可以具有不同的直径（即不同的方差）。

"diag"：集群可以采用任何大小的任意椭圆形，但是椭圆形的轴必须平行于坐标轴（即协方差矩阵必须是对角线）。

"tied"：所有集群必须具有相同的椭圆形状、大小和方向（即所有集群共享相同的协方差矩阵）。

默认情况下，`covariance_type`等于"full"，意味着每个集群可以采用任何形状、大小和方向（它具有自己无约束的协方差矩阵）。图18绘制了当`covariance_type`设置为"tied"或"spherical"时，通过EM算法找到的解。

![fig18_束状集群和球形集群的高斯混合](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig18_%E6%9D%9F%E7%8A%B6%E9%9B%86%E7%BE%A4%E5%92%8C%E7%90%83%E5%BD%A2%E9%9B%86%E7%BE%A4%E7%9A%84%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88.jpg)

训练高斯混合模型的计算复杂度取决于实例数m、维度n、集群数k和协方差矩阵的约束。如果`covariance_type`为"spherical"或"diag"，假定数据具有聚类结构，则为`O(kmn)`；如果`cova riance_type`为"tied"或"full"，则为`O(kmn2+kn3)`，因此不会扩展到具有大量特征。

高斯混合模型也可以用于异常检测。让我们看看它是如何做的。

### 9.2.1 使用高斯混合进行异常检测

