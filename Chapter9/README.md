# 第九章 无监督学习技术

尽管今天机器学习的大多数应用都是基于有监督学习的（因此，这是大多数投资的方向），但是绝大多数可用数据都没有标签：我们具有输入特征X，但是没有标签y。计算机科学家Yann LeCun曾有句著名的话：“如果智能是蛋糕，无监督学习将是蛋糕本体，有
监督学习是蛋糕上的糖霜，强化学习是蛋糕上的樱桃。”换句话说，无监督学习具有巨大
的潜力，我们才刚刚开始研究。

假设你要创建一个系统，该系统将在制造生产线上为每个产品拍摄几张图片，并检测哪些产品有缺陷。你可以相当容易地创建一个自动拍照系统，这可能每天为你提供数千张图片。然后，你可以在几周内构建一个相当大的数据集。但是，等等，没有标签！如果你想训练一个常规的二元分类器来预测某件产品是否有缺陷，则需要将每张图片标记为“有缺陷”或“正常”。这通常需要人类专家坐下来并手动浏览所有图片。这是一项漫长、昂贵且烦琐的任务，因此通常只能在可用图片的一部分上完成。因此，标记的数据集将非常小，并且分类器的性能将令人失望。而且，公司每次对其产品进行任何更改时，都需要从头开始整个过程。如果该算法只需要利用未标记的数据而无须人工标记每张图片，那不是很好吗？让我们进入无监督学习。

在第8章中，我们研究了最常见的无监督学习任务：降维。在本章中，我们将研究其
他一些无监督的学习任务和算法：

- **聚类**

    目标是将相似的实例分组到集群中。聚类是很好的工具，用于数据分析、客户细分、推荐系统、搜索引擎、图像分割、半监督学习、降维等。

- **异常检测**

    目的是学习“正常”数据看起来是什么样的，然后将其用于检测异常情况，例如生产线上的缺陷产品或时间序列中的新趋势。

- **密度估算**

    这是**估计生成数据集的随机过程的概率密度函数**（PDF）的任务，密度估算通常用于异常检测：位于非常低密度区域的实例很可能是异常。它对于数据分析和可视化也很有用。

准备好蛋糕了吗？我们将从使用K-Means和DBSCAN进行聚类开始，然后讨论高斯混合模型，并了解如何将它们用于密度估计、聚类和异常检测。

## 9.1 聚类

你在山中徒步旅行时，偶然发现了从未见过的植物。你环顾四周，发现还有很多。它们并不完全相同，但是它们足够相似，你可能知道它们有可能属于同一物种（或至少属于同一属）。你可能需要植物学家告诉你什么是物种，但你当然不需要专家来识别外观相似的物体组。这称为聚类：**识别相似实例并将其分配给相似实例的集群或组**。

就像在分类中一样，每个实例都分配给一个组。但是与分类不同，**聚类是一项无监督任务**。考虑图1：左侧是鸢尾花数据集（在第4章中介绍），其中每个实例的种类（即类）用不同的标记表示。它是一个标记的数据集，非常适合使用逻辑回归、SVM或随机森林分类器等分类算法。右侧是相同的数据集，但是没有标签，因此你不能再使用分类算法。这就是聚类算法的引入之处，它们中的许多算法都可以轻松检测左下角的集群。肉眼也很容易看到，但是右上角的集群由两个不同的子集群组成，并不是很明显。也就是说，数据集具有两个附加特征（萼片长度和宽度），此处未表示，并且聚类算法可以很好地利用所有的特征，因此实际上它们可以很好地识别三个聚类（例如，使用高斯混合模型，在150个实例中，只有5个实例分配给错误的集群）。

![fig01_分类与聚类](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig01_%E5%88%86%E7%B1%BB%E4%B8%8E%E8%81%9A%E7%B1%BB.jpg)

聚类可用于各种应用程序，包括：

- 客户细分

    你可以根据客户的购买记录和他们在网站上的活动对客户进行聚类。这对于了解你的客户是谁以及他们的需求很有用，因此你可以针对每个细分客户调整产品和营销活动。例如，客户细分在推荐系统中可以很有用，可以推荐同一集群中其他用户喜欢的内容。

- 数据分析

    在分析新数据集时，运行聚类算法然后分别分析每个集群。

- 降维技术

    数据集聚类后，通常可以测量每个实例与每个集群的相似度（相似度是衡量一个实例和一个集群的相似程度）。然后可以将每个实例的特征向量x替换为其集群的向量。如果有k个集群，则此向量为k维。此向量的维度通常比原始特征向量低得多，但它可以保留足够的信息以进行进一步处理。

- 异常检测（也称为离群值检测）

    对所有集群具有低相似度的任何实例都可能是异常。例如，如果你已根据用户行为对网站用户进行了聚类，则可以检测到具有异常行为的用户，例如每秒的请求数量异常。异常检测在检测制造生产线中的缺陷或欺诈检测中特别有用。

- 半监督学习

    如果你只有几个标签，则可以执行聚类并将标签传播到同一集群中的所有实例。该技术可以大大增加可用于后续有监督学习算法的标签数量，从而提高其性能。

- 搜索引擎

    一些搜索引擎可让你搜索与参考图像相似的图像。要构建这样的系统，首先要对数据库中的所有图像应用聚类算法，相似的图像最终会出现在同一集群中。然后，当用户提供参考图像时，你需要做的就是使用训练好的聚类模型找到该图像的集群，然后可以简单地从该集群中返回所有的图像。

- 分割图像

    通过根据像素的颜色对像素进行聚类，然后用其聚类的平均颜色替换每个像素的颜色，可以显著减少图像中不同颜色的数量。图像分割用于许多物体检测和跟踪系统中，因为它可以更轻松地检测每个物体的轮廓。

关于聚类什么没有统一的定义，它实际上取决于上下文，并且不同的算法会得到不同种类的集群。一些算法会寻找围绕特定点（称为中心点）的实例。其他人则寻找密集实例的连续区域：这些集群可以呈现任何形状。一些算法是分层的，寻找集群中的集群。这样的示例不胜枚举。

在本节中，我们将研究两种流行的聚类算法——K-Means和DBSCAN，并探讨它们的一些应用，例如非线性降维、半监督学习和异常检测。

### 9.1.1 K-Means

考虑图2中所示的未标记数据集：你可以清楚地看到5组实例。K-Means算法是一种简单的算法，能够非常快速、高效地对此类数据集进行聚类，通常只需几次迭代即可。它是由贝尔实验室的Stuart Lloyd在1957年提出的，用于脉冲编码调制，但直到1982年才对外发布。1965年，Edward W.Forgy发布了相同的算法，因此K-Means有时被称为Lloyd–Forgy。

![fig02_由5组实例集群组成的未标记数据集](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig02_%E7%94%B15%E7%BB%84%E5%AE%9E%E4%BE%8B%E9%9B%86%E7%BE%A4%E7%BB%84%E6%88%90%E7%9A%84%E6%9C%AA%E6%A0%87%E8%AE%B0%E6%95%B0%E6%8D%AE%E9%9B%86.jpg)

让我们在该数据集上训练一个K-Means聚类器。它将尝试找到每个集群的中心，并将每个实例分配给最近的集群：

```python
    from sklearn.cluster import KMeans

    k = 5
    kmeans = KMeans(n_clusters=k, random_state=42)
    y_pred = kmeans.fit_predict(X)
```

请注意，你**必须指定这个算法必须要找到的集群数k**。在此例中，通过查看数据可以明显看出k应该设置为5，但总的来说并不是那么容易。我们会简短讨论这个问题。

每个实例都会分配给5个集群之一。在聚类的上下文中，实例的标签是该实例被算法分配给该集群的索引：不要与分类中的类标签相混淆（请记住，聚类是无监督学习任务）。KMeans实例保留了经过训练的实例的标签副本，可通过labels_实例变量得到该副本：

```python
    y_pred
    >>> array([4, 1, 0, ..., 3, 0, 1])
    y_pred is kmeans.labels_
    >>> True
```

我们还可以看一下算法发现的5个中心点：

```python
    kmeans.cluster_centers_
    >>> array([[ 0.20876306,  2.25551336],
       [-2.80389616,  1.80117999],
       [-1.46679593,  2.28585348],
       [-2.79290307,  2.79641063],
       [-2.80037642,  1.30082566]])
```

可以很容易地将新实例分配给中心点最接近的集群：

```python
    X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])
    kmeans.predict(X_new)
    >>> array([0, 0, 3, 3])
```

如果绘制集群的边界，则会得到Voronoi图（参见图3，其中每个中心点都用×表示）。

![fig03_K-Means决策边界](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter9/figures/fig03_K-Means%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C.jpg)

绝大多数实例已正确分配给适当的集群，但少数实例可能贴错了标签（尤其是在左上集群和中央集群之间的边界附近）。的确，当集群具有不同的直径时，K-Means算法的性能不是很好，**因为将实例分配给某个集群时，它所关心的只是与中心点的距离**。

与其**将每个实例分配给一个单独的集群**（称为**硬聚类**），不如**为每个实例赋予每个集群一个分数**（称为**软聚类**）会更有用。分数可以是实例与中心点之间的距离。相反，它可以是相似性分数（或远近程度），例如高斯径向基函数（在第5章中介绍）。在KMeans类中，`transform()`方法测量每个实例到每个中心点的距离：

```python
    kmeans.transform(X_new)
    >>> array([[0.32995317, 2.81093633, 1.49439034, 2.9042344 , 2.88633901],
       [2.80290755, 5.80730058, 4.4759332 , 5.84739223, 5.84236351],
       [3.29399768, 1.21475352, 1.69136631, 0.29040966, 1.71086031],
       [3.21806371, 0.72581411, 1.54808703, 0.36159148, 1.21567622]])
```

在此例中，X_new中的第一个实例与第一个中心点的距离为2.81，与第二个中心点的距离为0.33，与第三个中心点的距离为2.90，与第四个中心点的距离为1.49，与第五个中心点的距离为2.89。如果你有一个高维数据集并以这种方式进行转换，那么最终将得到一个k维数据集：这种转换是一种非常有效的非线性降维技术。

#### K-Means算法

那么，该算法如何工作？好吧，假设你得到了中心点。你可以通过将每个实例分配给中心点最接近的集群来标记数据集中的所有实例。相反，如果你有了所有实例的标签，则**可以通过计算每个集群的实例平均值来定位所有的中心点**。但是你既没有标签也没有中心点，那么如何进行呢？好吧，只要从随机放置中心点开始（例如，随机选择k个实例并将其位置用作中心点）。然后标记实例，更新中心点，标记实例，更新中心点，以此类推，直到中心点停止移动。这个算法能保证在有限数量的步骤内收敛（通常很小），不会永远振荡。

你可以在图4中看到正在使用的算法：中心点被随机初始化（左上），然后实例被标记（右上），中心点被更新（左中心），实例被重新标记（右中），以此类推。如你所看到的，仅仅三个迭代，该算法就达到了近似于最佳状态的聚类。

![fig04_K-Means算法]()

该算法的计算复杂度在实例数m，集群数k和维度n方面通常是线性的。但是，仅当数据具有聚类结构时才如此。如果不是这样，那么在最坏的情况下，复杂度会随着实例数量的增加而呈指数增加。实际上，这种情况很少发生，并且K-Means通常是最快的聚类算法之一。

尽管算法保证会收敛，但它可能不会收敛到正确解（即可能会收敛到局部最优值）：这取决于中心点的初始化。图5显示了两个次优解，如果你不幸地使用随机初始化步骤，算法会收敛到它们。

![fig05_由于中心点初始化不好而导致的次优解]()

让我们看一下通过改善中心点初始化来减低这种风险的几种方法。

#### 中心点初始化方法

如果你碰巧知道了中心点应该在哪里（例如，你之前运行了另一种聚类算法），则可以将超参数`init`设置为包含中心点列表的NumPy数组，并将`n_init`设置为1：

```python
    good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])
    kmeans = KMeans(n_clusters=5, init=good_init, n_init=1
```

另一种解决方案是使用不同的随机初始化多次运行算法，并保留最优解。随机初始化的数量由超参数`n_init`控制：默认情况下，它等于10，这意味着当你调用`fit()`时，前述算法运行10次，Scikit-Learn会保留最优解。但是，如何确切知道哪种解答是最优解呢？它使用性能指标！这个指标称为模型的惯性，即每个实例与其最接近的中心点之间的均方距离。对于图5中左边的模型，它大约等于223.3；对于图5中右边的模型，它等于237.5；对于图3中的模型，它等于211.6。KMeans类运行`n_init`次算法，并保留模型的最低惯性。在此例中，将选择图3中的模型（除非我们非常不幸地使用`n_init`连续随机初始化）。如果你有意，可以通过`inertia_instance`变量来访问模型的惯性：

```python
    kmeans.inertia_
    >>> 211.59853725816828
```

`score()`方法返回负惯性。为什么是负的？因为预测器的`score()`方法必须始终遵循Scikit-Learn的“越大越好”的规则，即如果一个预测器比另一个更好，则其`score()`方法应该返回更大的数：

```python
    kmeans.score(X)
    -211.5985372581683
```

David Arthur和Sergei Vassilvitskii在2006年的论文中提出了对K-Means算法的重要改进——K-Means++。他们引入了一种更智能的初始化步骤，该步骤倾向于选择彼此相距较远的中心点，这一改进使得K-Means算法收敛到次优解的可能性很小。他们表明，更智能的初始化步骤所需的额外计算量是值得的，因为它可以大大减少寻找最优解所需运行算法的次数。以下是K-Means++的初始化算法：

1. 取一个中心点 $c^{(1)}$，从数据集中随机选择一个中心点。

2. 取一个新中心点 $c^{(i)}$，选择一个概率为 $D(x^{(i)})^2 / \sum^m_{j=1}D(x^{(j)})^2$ 的实例 $x^{(i)}$，其中 $D(x^{(i)})$ 是实例 $x^{(i)}$ 与已经选择的最远中心点的距离。这种概率分布确保距离已选择的中心点较远的实例有更大可能性被选择为中心点。

3. 重复上一步，直到选择了所有k个中心点。

默认情况下，KMeans类使用这种初始化方法。如果要强制使用原始方法（即随机选择k个实例来初始化中心点），则可以将超参数`init`设置为"random"。你很少需要这样做。

#### 加速的K-Means和小批量K-Means

Charles Elkan在2003年的一篇论文中提出了对K-Means算法的另一个重要改进。该改进后的算法通过避免许多不必要的距离计算，大大加快了算法的速度。Elkan通过利用三角不等式（即一条直线是两点之间的最短距离）并通过跟踪实例和中心点之间的上下限距离来实现这一目的。这是KMeans类默认使用的算法（你可以通过将超参数algorithm设置为"full"来强制使用原始算法，尽管你可能永远也不需要）。

David Sculley在2010年的一篇论文中提出了K-Means算法的另一个重要变体。该算法能够在每次迭代中使用小批量K-Means稍微移动中心点，而不是在每次迭代中使用完整的数据集。这将算法的速度提高了3到4倍，并且可以对不容纳内存的大数据集进行聚类。Scikit-Learn在MiniBatchKMeans类中实现了此算法。你可以像KMeans类一样使用此类：

```python
    from sklearn.cluster import MiniBatchKMeans
    minibatch_kmeans = MiniBatchKMeans(n_clusters=5)
    minibatch_kmeans.fit(X)
```

如果数据集无法容纳在内存中，则最简单的选择是使用`memmap`类，就像我们在第8章中对增量PCA所做的那样。或者你可以一次将一个小批量传给`partial_fit()`方法，但这需要更多的工作，因为你需要执行多次初始化并选择一个最好的初始化（有关示例请参阅notebook的小批量处理K-Means部分）。

尽管小批量K-Means算法比常规K-Means算法快得多，但其惯性通常略差一些，尤其是随着集群的增加。你可以在图6中看到这一点：左侧的图比较了小批量K-Means和常规K-Means模型的惯性，这是在先前数据集上使用各种不同k训练而来的。两条曲线之间的差异保持相当恒定，但是随着k的增加，该差异变得越来越大，因为惯性变得越来越小。在右边的图中，你可以看到小批量K-Means比常规K-Means快得多，并且该差异随k的增加而增加。

![fig06_小批量K-Means]()

#### 寻找最佳聚类数

到目前为止，我们将集群数k设置为5，因为通过查看数据可以明显看出这是正确的集群数。但是总的来说，知道如何设置k并不容易，如果将k设置为错误值，结果可能会很糟糕。如图7所示，将k设置为3或8会导致非常糟糕的模型。

![fig07_集群数量的错误选择]()

你可能会认为我们可以选择惯性最低的模型，对吗？不是那么简单！k=3的惯性是
653.2，比k=5（211.6）大得多。但是在k=8的情况下，惯性仅为119.1。尝试选择k时，惯性不是一个好的性能指标，因为随着k的增加，惯性会不断降低。实际上，集群越多，每个实例将越接近其最接近的中心点，因此惯性将越低。让我们将惯性作为k的函数作图（见图8）。

![fig08_集群k的函数曲线]()

如你所见，随着k增大到4，惯性下降得很快，但随着k继续增大，惯性下降得很慢。该曲线大致像手臂形状，在k=4处有一个“肘”形。因此，如果我们不知道更好的选择，则4是一个不错的选择：**较低的值变化比较大，而较高的值没有多大帮助，我们有时可以把集群再细分成两半**。

为集群数选择最佳值的技术相当粗糙。一种更精确的方法（但也需要更多的计算）是使用**轮廓分数**，它是所有实例的**平均轮廓系数**。实例的轮廓系数等于(b-a)/max(a,b)，其中a是与同一集群中其他实例的平均距离（即集群内平均距离），b是平均最近集群距离（即到下一个最近集群实例的平均距离，定义为使b最小的那个，不包括实例自身的集群）。轮廓系数可以在-1和+1之间变化。接近+1的系数表示该实例很好地位于其自身的集群中，并且远离其他集群，而接近0的系数表示该实例接近一个集群的边界，最后一个接近-1的系数意味着该实例可能已分配给错误的集群。

要计算轮廓分数，可以使用Scikit-Learn的`silhouette_score()`函数，为它输入数据集中的所有实例以及为其分配的标签：

```python
    >>> from sklearn.metrics import silhouette_score
    >>> silhouette_score(X, kmeans.labels_)
    0.65551764257282
```

让我们比较不同集群数量的轮廓分数（见图9）。

![fig09_使用轮廓分数选择集群数k]()

如你所见，这个可视化效果比上一个更加丰富：尽管它确认k=4是一个很好的选择，但它也说明一个事实，即k=5也非常好，并且比k=6或者k=7要好得多。这个在比较惯性时看不到。

当你绘制每个实例的轮廓系数时，可以获得更加丰富的可视化效果，这些轮廓系数按它们分配的集群和系数值进行排序，这称为轮廓图（见图10）。每个图包含一个刀形的聚类。形状的高度表示集群包含的实例数，宽度表示集群中实例的排序轮廓系数（越宽越好）。虚线表示平均轮廓系数。

![fig10_各种K值的轮廓图分析]()

垂直虚线表示每个集群的轮廓分数。当集群中的大多数实例的系数均低于此分数时（如果许多实例在虚线附近停止，在其左侧结束），则该集群比较糟糕，因为这意味着其实例太接近其他集群了。可以看到，当k=3或者k=6时，我们得到了不好的集群。但是当k=4或k=5时，集群看起来很好：大多数实例都超出虚线，向右延伸并接近1.0。当k=4时，索引为1（从顶部开始的第三个）的集群很大。当k=5时，所有集群的大小都相似。因此，即使k=4的整体轮廓得分略大于k=5的轮廓得分，使用k=5来获得相似大小的集群似乎也是一个好主意。

### 9.1.2 K-Means的局限

尽管K-Means有许多优点，尤其是快速且可扩展，但它并不完美。如我们所见，必须多次运行该算法才能避免次优解，此外，你还需要指定集群数，这很麻烦。此外，当集群具有不同的大小、不同的密度或非球形时，K-Means的表现也不佳。例如，图11显示了K-Means如何对包含三个不同尺寸、密度和方向的椭圆形集群的数据集进行聚类。

![fig11_K-Means无法正确聚类这些椭圆形集群]()

如你所见，这些聚类的结果都不好。左边的结果好一点，但是它仍然砍掉了中间集群的25%，并将其分配给右边的集群。即使其惯性较低，右侧的结果也很糟糕。因此，根据数据，不同的聚类算法可能会表现好一点。在这种类型的椭圆簇上，高斯混合模型非常有效。

在运行K-Means之前，请先缩放输入特征，否则集群可能会变形，这样K-Means的性能会很差。缩放特征并不能保证所有的集群都很好，但是通常可以改善结果。

现在让我们看一些可以从聚类中受益的方法。我们将使用K-Means，但可以随时试验其他的聚类算法。

### 9.1.3 使用聚类进行图像分割

**图像分割是将图像分成多个分割的任务**。在语义分割中，属于同一对象类型的所有像素均被分配给同一分割。例如，在无人驾驶汽车的视觉系统中，可能会将行人图像中的所有像素都分配给“行人”（会有一个包含所有行人的分割）。在实例分割中，属于同一单个对象的所有像素都分配给同一分割。在这种情况下，每个行人会有不同的分割。如今，使用最新技术的基于卷积神经网络的复杂结构可以实现语义分割或实例分割（见第14章）。在这里，我们做一些简单的事情：**颜色分割**。如果像素具有相似的颜色，我们将简单地将它们分配给同一分割。在某些应用中，这可能就足够了。例如，如果要分析卫星图像以测量某个区域中有多少森林总面积，则颜色分割就很好了。

首先，使用Matplotlib的`imread()`函数加载图像（见图12的左上方图像）：

```python
    from matplotlib.image import imread
    image = imread(os.path.join(images_path, filename))
    image.shape
```

**图像表示为3D数组**。第一维的大小是高度，第二个是宽度，第三个是颜色通道的数量，在这种情况下为红色、绿色和蓝色（RGB）。换句话说，对于每个像素都有一个3D向量，包含红色、绿色和蓝色的强度，强度在0.0和1.0之间（如果使用`imageio.imread()`，则在0和255之间）。某些图像可能具有较少的通道，例如灰度图像（一个通道）。某些图像可能具有更多的通道，例如具有透明度的alpha通道的图像或卫星图像，这些图像通常包含许多光频率（例如红外）的通道。以下代码对数组进行重构以得到RGB颜色的长列表，然后使用K-Means将这些颜色聚类：

```python
    X = image.reshape(-1, 3)
    kmeans = KMeans(n_clusters=8, random_state=42).fit(X)
    segmented_img = kmeans.cluster_centers_[kmeans.labels_]
    segmented_img = segmented_img.reshape(image.shape)
```

例如，它可以识别所有绿色阴影的颜色聚类。接下来，对于每种颜色（例如深绿色），它会寻找像素颜色集群的平均颜色。例如，所有绿色阴影都可以用相同的浅绿色代替（假设绿色集群的平均颜色是浅绿色）。最后，它会重新调整颜色的长列表，使其形状与原始图像相同。我们完成了！

这样就输出了图12右上方所示的图像。你可以尝试各种数量的集群，如图12所示。当你使用少于8个集群时，请注意，瓢虫的闪亮红色不能聚成单独的一类，它将与环境中的颜色合并。这是因为K-Means更喜欢相似大小的集群。瓢虫很小，比图像的其余部分要小得多，所以即使它的颜色是鲜明的，K-Means也不能为它指定一个集群。

![fig12_使用各种数量的颜色聚类K-Means进行图像分割]()

不是很难，不是吗？现在让我们看一下聚类的另一个应用：预处理。

### 9.1.4 使用聚类进行预处理

聚类是一种有效的降维方法，特别是**作为有监督学习算法之前的预处理步骤**。作为使用聚类进行降维的示例，让我们处理数字数据集，它是一个类似于MNIST的简单数据集，其中包含了表示从0到9数字的1797个灰度8×8图像。首先，加载数据集：

```python
    from sklearn.datasets import load_digits

    X_digits, y_digits = load_digits(return_X_y=True)
```

将其分为训练集和测试集：

```python
    from sklearn.model_selection import train_test_split

    X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=42)
```

拟合一个逻辑回归模型：

```python
    from sklearn.linear_model import LogisticRegression

    log_reg = LogisticRegression(multi_class="ovr", solver="lbfgs", max_iter=5000, random_state=42)
    log_reg.fit(X_train, y_train)
```

在测试集上评估其精度：

```python
    log_reg_score = log_reg.score(X_test, y_test)
    log_reg_score
    >>> 0.9688888888888889
```

这就是我们的基准：准确率为96.9%。让我们看看通过使用K-Means作为预处理是否可以做得更好。我们创建一个流水线，该流水线首先将训练集聚类为50个集群，并将图像替换为其与这50个集群的距离，然后应用逻辑回归模型：

```python
    from sklearn.pipeline import Pipeline

    pipeline = Pipeline([
        ("kmeans", KMeans(n_clusters=50, random_state=42)),
        ("log_reg", LogisticRegression(multi_class="ovr", solver="lbfgs", max_iter=5000, random_state=42)),
    ])
    pipeline.fit(X_train, y_train)
```

由于有10个不同的数字，因此很容易将集群数量设置为10。但是，每个数字可以用几种不同的方式写入，因此最好使用更大的集群数量，例如50。评估这个分类流水线：

```python
    pipeline_score = pipeline.score(X_test, y_test)
    pipeline_score
    >>> 0.9777777777777777
```

怎么样？我们把错误率降低了近30%（从大约3.1%降低到大约2.2%）！

但是我们是任意选择集群k，我们当然可以做得更好。由于K-Means只是分类流水线中的预处理步骤，因此为k找到一个好的值要比以前简单得多。无须执行轮廓分析或最小化惯性。k的最优解是在交叉验证过程中的最佳分类性能的值。我们可以使用`GridSearchCV`来找到集群的最优解：

```python
    from sklearn.model_selection import GridSearchCV

    param_grid = dict(kmeans__n_clusters=range(2,100))
    grid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)
    grid_clf.fit(X_train, y_train)
```

让我们看一下k的最优解和流水线的性能：

```python
    grid_clf.best_params_
    >>>
    grid_clf.score(X_test, y_test)
    >>>
```

在k=99个聚类的情况下，我们获得了显著的精度提升，在测试集上达到了98.22%的精度。你可以继续探索更高值的k，因为99是我们探索范围内的最大值。

### 9.1.5 使用聚类进行半监督学习

聚类的另一个应用是在半监督学习中，我们有很多未标记的实例，而很少带标签的实例。让我们在digits数据集的50个带标签实例的样本上训练逻辑回归模型：

```python
    n_labeled = 50
    log_reg = LogisticRegression(multi_class="ovr", solver="lbfgs", random_state=42)
    log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])
    log_reg.score(X_test, y_test)
    >>> 0.8333333333333334
```

精度仅为83.3%。毫不奇怪，这比我们在完整的训练集上训练模型时要低得多。让我们看看如何能做得更好。首先，我们将训练集聚类为50个集群。然后，对于每个集群，找到最接近中心点的图像。我们将这些图像称为代表性图像：

```python
    k = 50
    kmeans = KMeans(n_clusters=k, random_state=42)
    X_digits_dist = kmeans.fit_transform(X_train)
    representative_digit_idx = np.argmin(X_digits_dist, axis=0)
    X_representative_digits = X_train[representative_digit_idx]
```

图13显示了这50张代表性图像。

![fig13_50个代表性数字图像]()

让我们看一下每个图像并手动标记它：

```python
    y_representative_digits = np.array([
        0, 1, 3, 2, 7, 6, 4, 6, 9, 5,
        1, 2, 9, 5, 2, 7, 8, 1, 8, 6,
        3, 2, 5, 4, 5, 4, 0, 3, 2, 6,
        1, 7, 7, 9, 1, 8, 6, 5, 4, 8,
        5, 3, 3, 6, 7, 9, 7, 8, 4, 9])
```

现在，我们有一个只有50个带标签实例的数据集，但是它们不是随机实例，每个实例都是其集群的代表图像。让我们看看性能是否更好：

```python
    log_reg = LogisticRegression(multi_class="ovr", solver="lbfgs", max_iter=5000, random_state=42)
    log_reg.fit(X_representative_digits, y_representative_digits)
    log_reg.score(X_test, y_test)
    >>> 0.9222222222222223
```

哇！尽管我们仍然仅在50个实例上训练模型，但准确性从83.3%跃升至92.2%。由于标记实例通常很昂贵且很痛苦，尤其是当必须由专家手动完成时，标记代表性实例（而不是随机实例）是一个好主意。

但是也许我们可以更进一步：如果将标签传播到同一集群中的所有其他实例，会怎么样？这称为标签传播：

```python
    y_train_propagated = np.empty(len(X_train), dtype=np.int32)
    for i in range(k):
        y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]

    log_reg = LogisticRegression(multi_class="ovr", solver="lbfgs", max_iter=5000, random_state=42)
    log_reg.fit(X_train, y_train_propagated)
    log_reg.score(X_test, y_test)
    >>> 0.9333333333333333
```

我们获得了合理的精度提升，这不让人惊奇。问题在于，我们把每个代表性实例的标签传播到同一集群中的所有实例，包括位于集群边界附近的实例，这些实例更容易被错误标记。让我们看看如果仅将标签传播到最接近中心点的20%的实例，会发生什么情况：

```python
    percentile_closest = 75

    X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]
    for i in range(k):
        in_cluster = (kmeans.labels_ == i)
        cluster_dist = X_cluster_dist[in_cluster]
        cutoff_distance = np.percentile(cluster_dist, percentile_closest)
        above_cutoff = (X_cluster_dist > cutoff_distance)
        X_cluster_dist[in_cluster & above_cutoff] = -1

    partially_propagated = (X_cluster_dist != -1)
    X_train_partially_propagated = X_train[partially_propagated]
    y_train_partially_propagated = y_train_propagated[partially_propagated]
```

在这个部分传播的数据集上再次训练模型：

```python
    log_reg = LogisticRegression(multi_class="ovr", solver="lbfgs", max_iter=5000, random_state=42)
    log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)

    log_reg.score(X_test, y_test)
    >>> 0.9355555555555556
```

很好！仅使用50个带标签的实例（平均每个类只有5个实例！），我们获得了93.5%的准确率，与整个带标签的数字数据集上逻辑回归的性能非常接近（96.9%）。之所以如此出色，是因为传播的标签实际上非常好，它们的准确率非常接近99%，如以下代码所示：

```python
    np.mean(y_train_partially_propagated == y_train[partially_propagated])
```

**主动学习**：

为了继续改进模型和训练集，下一步可能是进行几轮主动学习，即当人类专家与学习算法进行交互时，在算法要求时为特定实例提供标签。主动学习有许多不同的策略，但最常见的策略之一称为不确定性采样。下面是它的工作原理：

1. 该模型是在目前为止收集到的带标签的实例上进行训练，并且该模型用于对所有未标记实例进行预测。

2. 把模型预测最不确定的实例（当估计的概率最低时）提供给专家来做标记。

3. 重复此过程，直到性能改进到不值得做标记为止。

其他策略包括标记那些会导致模型最大变化的实例或模型验证错误下降最大的实例，或不同模型不一致的实例（例如SVM或随机森林）。

在进入高斯混合模型之前，让我们看一下DBSCAN，这是另一种流行的聚类算法，它说明了一种基于局部密度估计的不同的方法。这种方法允许算法识别任意形状的聚类。

### 9.1.6 DBSCAN

该算法将集群定义为高密度的连续区域。下面是它的工作原理：
