# 第四章 训练模型

本章我们将从最简单的模型之一——**线性回归模型**，开始介绍两种非常不同的训练模型的方法：

- 通过“闭式”方程，直接计算出最拟合训练集的模型参数（也就是使训练集上的成本函数最小化的模型参数）。

- 使用迭代优化的方法，即梯度下降（GD），逐渐调整模型参数直至训练集上的成本函数调至最低，最终趋同于第一种方法计算出来的模型参数。我们还会研究几个梯度下降的变体，包括**批量梯度下降**、**小批量梯度下降**以及**随机梯度下降**。

接着我们将会进入**多项式回归的讨论**，这是一个更为复杂的模型，更适合非线性数据集。由于该模型的参数比线性模型更多，因此更容易造成对训练数据过拟合，我们将使用**学习曲线**来分辨这种情况是否发生。然后，再介绍几种**正则化技巧**，降低过拟合训练数据的风险。

最后，我们将学习两种经常用于分类任务的模型：`Logistic回归`和`Softmax回归`。

## 4.1 线性回归

线性模型就是对输入特征加权求和，再加上一个我们称为**偏置项**（也称为截距项）的常数，以此进行预测，如公式4-1所示：

![图01_线性回归模型预测公式]()

其中，y是预测值；n是特征数量；xi是第i个特征值；θj是第j个模型参数（包括偏差项θ0和特征值θ1，θ2，...,θn）；

可以使用向量化的形式更简洁地表示：

![图02_线性回归模型预测(向量化形式)]()

其中，θ是模型的参数向量，其中包含偏差项θ0和特征值θ1至θn；x是实例的特征向量，包含从x0至xn，x0始终是1；θ·x是向量θ和x的点积，它当然等于θ0x0+θ1x1+θ2x2+...+θnxn；hθ是假设函数，使用模型参数θ。

在机器学习中，向量通常表示为列向量，是有单一列的二维数组。如果θ和x为列向量，则预测为y^=θ<sup>T</sup>x，其中θ<sup>T</sup>为θ（行向量而不是列向量）的转置，且θ<sup>T</sup>x为θ<sup>T</sup>和x的矩阵乘积。当然这是相同的预测，除了现在是以单一矩阵表示而不是一个标量值。在本书中，我将使用这种表示法来避免在点积和矩阵乘法之间切换。

这就是线性回归模型，我们该怎样训练线性回归模型呢？回想一下，**训练模型就是设置模型参数直到模型最拟合训练集的过程**。为此，我们首先需要知道怎么测量模型对训练数据的拟合程度是好还是差。在第2章中，我们了解到回归模型最常见的性能指标是**均方根误差（RMSE）**（见公式2-1）。因此，在训练线性回归模型时，你需要找到最小化RMSE的θ值。在实践中，将均方误差（MSE）最小化比最小化RMSE更为简单，二者效果相同（因为使函数最小化的值，同样也使其平方根最小）。

在训练集X上，使用公式4-3计算训练集X上线性回归的MSE，h<sub>θ</sub>为假设函数。

线性回归模型的MSE成本函数：

![图03_线性回归模型的MSE成本函数]()

唯一的区别是h换成了h<sub>θ</sub>，以便清楚地表明模型被向量θ参数化。为了简化符号，我们将MSE(X, h<sub>θ</sub>)直接写作MSE(θ)。

### 4.1.1 标准方程

为了得到使成本函数最小的θ值，有一个闭式解方法——也就是一个直接得出结果的数学方程，即**标准方程**：

![图04_标准方程]()

其中，θ^是使成本函数最小的θ值；y是包含y<sup>(1)</sup>到y<sup>(m)</sup>的目标值向量。

我们生成一些线性数据来测试这个公式：

```python
    import numpy as np

    X = 2 * np.random.rand(100, 1)
    y = 4 + 3 * X + np.random.rand(100, 1)

    plt.plot(X, y, "b.")
    plt.xlabel("$x_1$", fontsize=18)
    plt.ylabel("$y$", rotation=0, fontsize=18)
    plt.axis([0, 2, 0, 15])
```

![图05_随机生成的线性数据集]()

使用标准方程来计算θ^。使用NumPy的线性代数模块（`np.linalg`）中的`inv()`函数来对矩阵求逆，并用`dot()`方法计算矩阵的内积：

```python
    X_b = np.c_[np.ones((100, 1)), X] # add x0=1 to each instance
    theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
```

我们实际用来生成数据的函数是y=4+3x<sub>1</sub>+高斯噪声。来看看公式的结果：

```python
    theta_best
    >>>array([[4.51359766],
       [2.98323418]])
```

我们期待的是θ<sub>0</sub>=4，θ<sub>1</sub>=3得到的是θ<sub>0</sub>=4.215，θ<sub>1</sub>=2.770。非常接近，噪声的存在使其不可能完全还原为原本的函数。

现在可以用θ^做出预测：

```python
    X_new = np.array([[0], [2]])
    X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0=1 to each instance
    y_predict = X_new_b.dot(theta_best)
    y_predict
```

绘制模型的预测结果：

```python
    plt.plot(X_new, y_predict, "r-")
    plt.plot(X, y, "b.")
    plt.axis([0, 2, 0, 15])
    plt.show()
```

![图06_线性回归模型预测]()

使用Scikit-Learn执行线性回归很简单：

```python
    from sklearn.linear_model import LinearRegression
    lin_reg = LinearRegression()
    lin_reg.fit(X, y)
    lin_reg.intercept_, lin_reg.coef_
    >>> (array([4.51359766]), array([[2.98323418]]))
    lin_reg.predict(X_new)
    >>>array([[ 4.51359766],
       [10.48006601]])
```

`LinearRegression`类基于`scipy.linalg.lstsq()`函数（名称代表“最小二乘”），你可以直接调用它：

```python
    theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)
    theta_best_svd
```

此函数计算 $\mathbf{X}^+\mathbf{y}$，其中 $\mathbf{X}^{+}$ 是 $\mathbf{X}$ 的伪逆（具体说是Moore-Penrose 逆）。可以使用`np.linalg.pinv()`来直接计算这个逆：

```python
    np.linalg.pinv(X_b).dot(y)
    >>>array([[4.51359766],
       [2.98323418]])
```

伪逆本身是使用被称为**奇异值分解**（**Singular Value Decomposition，SVD**）的标准矩阵分解技术来计算的，可以将训练集矩阵 $\mathbf{X}$ 分解为三个矩阵 $\mathbf{UΣV}^T$  的乘积（请参阅`numpy.linalg.svd()`）。T伪逆的计算公式为 $\mathbf{X}^+\mathbf{=VΣ}^+\mathbf{U}^T$ 。为了计算矩阵 $\mathbf{Σ}^+$ ，该算法取  $\mathbf{Σ}$ 并将所有小于一个小阈值的值设置为零，然后将所有非零值替换成它们的倒数，最后把结果矩阵转置。这种方法比计算标准方程更有效，再加上它可以很好地处理边缘情况：的确，如果矩阵   $\mathbf{X}^T\mathbf{X}$  是不可逆的（即奇异的），标准方程可能没有解，例如m < n或某些特征是多余的，但伪逆总是有定义的。

### 4.1.2 计算复杂度

