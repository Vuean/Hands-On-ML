# 第四章 训练模型

本章我们将从最简单的模型之一——**线性回归模型**，开始介绍两种非常不同的训练模型的方法：

- 通过“闭式”方程，直接计算出最拟合训练集的模型参数（也就是使训练集上的成本函数最小化的模型参数）。

- 使用迭代优化的方法，即梯度下降（GD），逐渐调整模型参数直至训练集上的成本函数调至最低，最终趋同于第一种方法计算出来的模型参数。我们还会研究几个梯度下降的变体，包括**批量梯度下降**、**小批量梯度下降**以及**随机梯度下降**。

接着我们将会进入**多项式回归的讨论**，这是一个更为复杂的模型，更适合非线性数据集。由于该模型的参数比线性模型更多，因此更容易造成对训练数据过拟合，我们将使用**学习曲线**来分辨这种情况是否发生。然后，再介绍几种**正则化技巧**，降低过拟合训练数据的风险。

最后，我们将学习两种经常用于分类任务的模型：`Logistic回归`和`Softmax回归`。

## 4.1 线性回归

线性模型就是对输入特征加权求和，再加上一个我们称为**偏置项**（也称为截距项）的常数，以此进行预测，如公式4-1所示：

![图01_线性回归模型预测公式](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter4/%E5%9B%BE01_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E5%85%AC%E5%BC%8F.png)

其中，y是预测值；n是特征数量；xi是第i个特征值；θj是第j个模型参数（包括偏差项θ0和特征值θ1，θ2，...,θn）；

可以使用向量化的形式更简洁地表示：

![图02_线性回归模型预测(向量化形式)](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter4/%E5%9B%BE02_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B(%E5%90%91%E9%87%8F%E5%8C%96%E5%BD%A2%E5%BC%8F).png)

其中，θ是模型的参数向量，其中包含偏差项θ0和特征值θ1至θn；x是实例的特征向量，包含从x0至xn，x0始终是1；θ·x是向量θ和x的点积，它当然等于θ0x0+θ1x1+θ2x2+...+θnxn；hθ是假设函数，使用模型参数θ。

在机器学习中，向量通常表示为列向量，是有单一列的二维数组。如果θ和x为列向量，则预测为y^=θ<sup>T</sup>x，其中θ<sup>T</sup>为θ（行向量而不是列向量）的转置，且θ<sup>T</sup>x为θ<sup>T</sup>和x的矩阵乘积。当然这是相同的预测，除了现在是以单一矩阵表示而不是一个标量值。在本书中，我将使用这种表示法来避免在点积和矩阵乘法之间切换。

这就是线性回归模型，我们该怎样训练线性回归模型呢？回想一下，**训练模型就是设置模型参数直到模型最拟合训练集的过程**。为此，我们首先需要知道怎么测量模型对训练数据的拟合程度是好还是差。在第2章中，我们了解到回归模型最常见的性能指标是**均方根误差（RMSE）**（见公式2-1）。因此，在训练线性回归模型时，你需要找到最小化RMSE的θ值。在实践中，将均方误差（MSE）最小化比最小化RMSE更为简单，二者效果相同（因为使函数最小化的值，同样也使其平方根最小）。

在训练集X上，使用公式4-3计算训练集X上线性回归的MSE，h<sub>θ</sub>为假设函数。

线性回归模型的MSE成本函数：

![图03_线性回归模型的MSE成本函数](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter4/%E5%9B%BE03_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84MSE%E6%88%90%E6%9C%AC%E5%87%BD%E6%95%B0.png)

唯一的区别是h换成了h<sub>θ</sub>，以便清楚地表明模型被向量θ参数化。为了简化符号，我们将MSE(X, h<sub>θ</sub>)直接写作MSE(θ)。

### 4.1.1 标准方程

为了得到使成本函数最小的θ值，有一个闭式解方法——也就是一个直接得出结果的数学方程，即**标准方程**：

![图04_标准方程](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter4/%E5%9B%BE04_%E6%A0%87%E5%87%86%E6%96%B9%E7%A8%8B.png)

其中，θ^是使成本函数最小的θ值；y是包含y<sup>(1)</sup>到y<sup>(m)</sup>的目标值向量。

我们生成一些线性数据来测试这个公式：

```python
    import numpy as np

    X = 2 * np.random.rand(100, 1)
    y = 4 + 3 * X + np.random.rand(100, 1)

    plt.plot(X, y, "b.")
    plt.xlabel("$x_1$", fontsize=18)
    plt.ylabel("$y$", rotation=0, fontsize=18)
    plt.axis([0, 2, 0, 15])
```

![图05_随机生成的线性数据集](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter4/%E5%9B%BE05_%E9%9A%8F%E6%9C%BA%E7%94%9F%E6%88%90%E7%9A%84%E7%BA%BF%E6%80%A7%E6%95%B0%E6%8D%AE%E9%9B%86.jpg)

使用标准方程来计算θ^。使用NumPy的线性代数模块（`np.linalg`）中的`inv()`函数来对矩阵求逆，并用`dot()`方法计算矩阵的内积：

```python
    X_b = np.c_[np.ones((100, 1)), X] # add x0=1 to each instance
    theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
```

我们实际用来生成数据的函数是y=4+3x<sub>1</sub>+高斯噪声。来看看公式的结果：

```python
    theta_best
    >>>array([[4.51359766],
       [2.98323418]])
```

我们期待的是θ<sub>0</sub>=4，θ<sub>1</sub>=3得到的是θ<sub>0</sub>=4.215，θ<sub>1</sub>=2.770。非常接近，噪声的存在使其不可能完全还原为原本的函数。

现在可以用θ^做出预测：

```python
    X_new = np.array([[0], [2]])
    X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0=1 to each instance
    y_predict = X_new_b.dot(theta_best)
    y_predict
```

绘制模型的预测结果：

```python
    plt.plot(X_new, y_predict, "r-")
    plt.plot(X, y, "b.")
    plt.axis([0, 2, 0, 15])
    plt.show()
```

![图06_线性回归模型预测](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter4/%E5%9B%BE06_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B.jpg)

使用Scikit-Learn执行线性回归很简单：

```python
    from sklearn.linear_model import LinearRegression
    lin_reg = LinearRegression()
    lin_reg.fit(X, y)
    lin_reg.intercept_, lin_reg.coef_
    >>> (array([4.51359766]), array([[2.98323418]]))
    lin_reg.predict(X_new)
    >>>array([[ 4.51359766],
       [10.48006601]])
```

`LinearRegression`类基于`scipy.linalg.lstsq()`函数（名称代表“最小二乘”），你可以直接调用它：

```python
    theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)
    theta_best_svd
```

此函数计算 $\mathbf{X}^+\mathbf{y}$，其中 $\mathbf{X}^{+}$ 是 $\mathbf{X}$ 的伪逆（具体说是Moore-Penrose 逆）。可以使用`np.linalg.pinv()`来直接计算这个逆：

```python
    np.linalg.pinv(X_b).dot(y)
    >>>array([[4.51359766],
       [2.98323418]])
```

伪逆本身是使用被称为**奇异值分解**（**Singular Value Decomposition，SVD**）的标准矩阵分解技术来计算的，可以将训练集矩阵 $\mathbf{X}$ 分解为三个矩阵 $\mathbf{UΣV}^T$  的乘积（请参阅`numpy.linalg.svd()`）。T伪逆的计算公式为 $\mathbf{X}^+\mathbf{=VΣ}^+\mathbf{U}^T$ 。为了计算矩阵 $\mathbf{Σ}^+$ ，该算法取  $\mathbf{Σ}$ 并将所有小于一个小阈值的值设置为零，然后将所有非零值替换成它们的倒数，最后把结果矩阵转置。这种方法比计算标准方程更有效，再加上它可以很好地处理边缘情况：的确，如果矩阵   $\mathbf{X}^T\mathbf{X}$  是不可逆的（即奇异的），标准方程可能没有解，例如m < n或某些特征是多余的，但伪逆总是有定义的。

### 4.1.2 计算复杂度

标准方程计算X<sup>T</sup>X的逆，X<sup>T</sup>X是一个（n+1）×（n+1）的矩阵（n是特征数量）。对这种矩阵求逆的计算复杂度通常为O(n^2.4)到O(n^3)之间，取决于具体实现。换句话说，如果将特征数量翻倍，那么计算时间将乘以大约2^2.4=5.3倍到2^3=8倍之间。

Scikit-Learn的`LinearRegression`类使用的SVD方法的复杂度约为O(n^2)。如果你将特征数量加倍，那计算时间大约是原来的4倍。

特征数量比较大（例如100000）时，标准方程和SVD的计算将极其缓慢。好的一面是，相对于训练集中的实例数量（O(m)）来说，两个都是线性的，所以能够有效地处理大量的训练集，只要内存足够。

同样，线性回归模型一经训练（不论是标准方程还是其他算法），预测就非常快速：因为计算复杂度相对于想要预测的实例数量和特征数量来说都是线性的。换句话说，对两倍的实例（或者是两倍的特征数）进行预测，大概需要两倍的时间。

现在，我们再看几个截然不同的线性回归模型的训练方法，这些方法**更适合特征数或者训练实例数量大到内存无法满足要求的场景**。

## 4.2 梯度下降

梯度下降是一种非常通用的优化算法，能够为大范围的问题找到最优解。梯度下降的中心思想就是**迭代地调整参数从而使成本函数最小化**。

假设你迷失在山上的浓雾之中，你能感觉到的只有你脚下路面的坡度。**快速到达山脚的一个策略就是沿着最陡的方向下坡**。这就是梯度下降的做法：**通过测量参数向量θ相关的误差函数的局部梯度，并不断沿着降低梯度的方向调整，直到梯度降为0，到达最小值**！

具体来说，首先使用一个随机的θ值（这被称为**随机初始化**），然后逐步改进，每次踏出一步，每一步都尝试降低一点成本函数（如MSE），直到算法收敛出一个最小值，如下图所示：

![图07_梯度下降示意图](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter4/%E5%9B%BE07_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg)

在梯度下降的描述中，模型参数被随机初始化并反复调整使成本函数最小化。学习步长与成本函数的斜率成正比，因此，当参数接近最小值时，步长逐渐变小。

梯度下降中一个重要参数是每一步的步长，这取决于**超参数学习率**。如果学习率太低，算法需要经过大量迭代才能收敛，这将耗费很长时间，如下图所示：

![图08_梯度下降_学习率太小](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter4/%E5%9B%BE08_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D_%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%A4%AA%E5%B0%8F.jpg)

反过来说，如果学习率太高，那你可能会越过山谷直接到达另一边，甚至有可能比之前的起点还要高。这会导致算法发散，值越来越大，最后无法找到好的解决方案，如下图所示：

![图09_梯度下降_学习率太大](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter4/%E5%9B%BE09_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D_%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%A4%AA%E5%A4%A7.jpg)

最后，并不是所有的成本函数看起来都像一个漂亮的碗。有的可能看着像洞、山脉、高原或者各种不规则的地形，导致很难收敛到最小值。下图显示了梯度下降的两个主要挑战：如果随机初始化，算法从左侧起步，那么会收敛到一个局部最小值，而不是全局最小值。如果算法从右侧起步，那么需要经过很长时间才能越过整片高原，如果你停下得太早，将永远达不到全局最小值。

![图10_梯度下降陷阱](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter4/%E5%9B%BE10_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E9%99%B7%E9%98%B1.jpg)

幸好，**线性回归模型的MSE成本函数恰好是个凸函数**，这意味着连接曲线上任意两点的线段永远不会跟曲线相交。也就是说，不存在局部最小值，只有一个全局最小值。它同时也是一个连续函数，所以斜率不会产生陡峭的变化。这两点保证的结论是：即便是乱走，梯度下降都可以趋近到全局最小值（只要等待时间足够长，学习率也不是太高）。

成本函数虽然是碗状的，但如果不同特征的尺寸差别巨大，那它可能是一个非常细长的碗。如下图所示的梯度下降，左边的训练集上特征1和特征2具有相同的数值规模，而右边的训练集上，特征1的值则比特征2要小得多（注：因为特征1的值较小，所以θ1需要更大的变化来影响成本函数，这就是为什么碗形会沿着θ1轴拉长。）

![图11_有（左）和没有（右）特征缩放的梯度下降](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter4/%E5%9B%BE11_%E6%9C%89%EF%BC%88%E5%B7%A6%EF%BC%89%E5%92%8C%E6%B2%A1%E6%9C%89%EF%BC%88%E5%8F%B3%EF%BC%89%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.jpg)

正如你所见，左图的梯度下降算法直接走向最小值，可以快速到达。而在右图中，先是沿着与全局最小值方向近乎垂直的方向前进，接下来是一段几乎平坦的长长的山谷。最终还是会抵达最小值，但是这需要花费大量的时间。

**应用梯度下降时，需要保证所有特征值的大小比例都差不多（比如使用Scikit-Learn的StandardScaler类），否则收敛的时间会长很多**。

同时上图也说明，**训练模型也就是搜寻使成本函数（在训练集上）最小化的参数组合**。这是模型参数空间层面上的搜索：模型的参数越多，这个空间的维度就越多，搜索就越难。同样是在干草堆里寻找一根针，在一个三百维的空间里就比在一个三维空间里要棘手得多。幸运的是，线性回归模型的成本函数是凸函数，针就躺在碗底。

### 4.2.1 批量梯度下降

要实现梯度下降，你需要计算每个模型关于参数θ<sub>j</sub>的成本函数的梯度。换言之，你需要计算的是如果改变θ<sub>j</sub>，成本函数会改变多少。这被称为偏导数。这就好比是在问“如果我面向东，我脚下的坡度斜率是多少？”然后面向北问同样的问题（如果你想象超过三个维度的宇宙，对于其他的维度以此类推）。公式4-5计算了关于参数θ<sub>j</sub>的成本函数的偏导数，计作：

![图12_成本函数的偏导数1](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter4/%E5%9B%BE12_%E6%88%90%E6%9C%AC%E5%87%BD%E6%95%B0%E7%9A%84%E5%81%8F%E5%AF%BC%E6%95%B01.png)

成本函数的偏导数：

![图13_成本函数的偏导数2](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter4/%E5%9B%BE13_%E6%88%90%E6%9C%AC%E5%87%BD%E6%95%B0%E7%9A%84%E5%81%8F%E5%AF%BC%E6%95%B02.png)

如果不想单独计算这些偏导数，可以使用下式对其进行一次性计算。度向量记作▽<sub>θ</sub>MSE(θ)，包含所有成本函数（每个模型参数一个）的偏导数：

![图14_成本函数的梯度向量](https://github.com/Vuean/Hands-On-ML/blob/main/Chapter4/%E5%9B%BE14_%E6%88%90%E6%9C%AC%E5%87%BD%E6%95%B0%E7%9A%84%E6%A2%AF%E5%BA%A6%E5%90%91%E9%87%8F.png)

请注意，在计算梯度下降的每一步时，都是**基于完整的训练集X的**。这就是为什么该算法会被称为批量梯度下降：每一步都使用整批训练数据（实际上，全梯度下降可能是个更好的名字）。因此，面对非常庞大的训练集时，算法会变得极慢（不过我们即将看到快得多的梯度下降算法）。但是，梯度下降算法随特征数量扩展的表现比较好。如果要训练的线性模型拥有几十万个特征，使用梯度下降比标准方程或者SVD要快得多。

一旦有了梯度向量，哪个点向上，就朝反方向下坡。也就是从θ中减去▽<sub>θ</sub>MSE(θ)。这时**学习率η**就发挥作用了：用梯度向量乘以η确定下坡步长的大小：

![图15_梯度下降步骤]()

让我们看一下该算法的快速实现：

```python
    eta = 0.1       # learning rate
    n_iterations = 1000
    m = 100

    theta = np.random.randn(2, 1)   # random initialization

    for iterator in range(n_iterations):
        gradients = 2 / m * X_b.T.dot(X_b.dot(theta) - y)
        theta = theta - eta * gradients

    theta
    >>>array([[4.51359766],
        [2.98323418]])
```

嘿，这不正是标准方程的发现么！梯度下降表现完美。如果使用了其他的学习率eta呢？下图展现了分别使用三种不同的学习率时，梯度下降的前十步（虚线表示起点）。

![图16_各种学习率的梯度下降]()

左图的学习率太低：算法最终还是能找到解决方法，就是需要太长时间。中间图的学习率看起来非常棒：几次迭代就收敛出了最终解。而右图的学习率太高：算法发散，直接跳过了数据区域，并且每一步都离实际解决方案越来越远。

要找到合适的学习率，可以使用网格搜索（见第2章）。但是你可能需要限制迭代次数，这样网格搜索可以淘汰掉那些收敛耗时太长的模型。

你可能会问，要怎么限制迭代次数呢？如果设置太低，算法可能在离最优解还很远时就停了。但是如果设置得太高，模型达到最优解后，继续迭代则参数不再变化，又会浪费时间。一个简单的办法是在开始时设置一个非常大的迭代次数，但是当梯度向量的值变得很微小时中断算法——也就是当它的范数变得低于（称为容差）时，因为这时梯度下降已经（几乎）到达了最小值。

**收敛速度**：成本函数为凸函数，并且斜率没有陡峭的变化时（如MSE成本函数），具有固定学习率的批量梯度下降最终会收敛到最佳解，但是你需要等待一段时间：它可以进行O（1/∈）次迭代以在∈的范围内达到最佳值，具体取决于成本函数的形状。换句话说，如果将容差缩小为原来的1/10（以得到更精确的解），算法将不得不运行10倍的时间。

### 4.2.2 随机梯度下降

